[
  {
    "objectID": "PS/PS01.html",
    "href": "PS/PS01.html",
    "title": "Problem Set 01",
    "section": "",
    "text": "Name:\nField that you’re interested in applying TS and forecasting to (ecology, econ, weather, etc):\n\n\nObtain a CSV of time series data\n\nBy whatever means, get a .csv file of time series data relating to any topic: ecological, financial, etc.\nThere should be at least three variables of data\nIf you download from the web, include a link. If you use ChatGPT, include a link to your shared search\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport requests\nimport pandas as pd\n\n\n\n1. Time series plots\n\nPlot the raw time series data\nIdentify any obvious patterns in your time series\nIdentify, if any, interesting insights these patterns suggest\n\n\n\n2. Scatterplots\n\nPlot a 3x3 grid of all pairwise scatterplots\nIdentify, if any, interesting trends or insight\n\n\n\n3. Lag & autocorrelation plots\n\nPlot a lag plot of any variable of your choice to visualize its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nPlot an autocorrelation plot of this same variable to quantify its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nIdentify, if any, interesting insight from these lag plots"
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": "Manipulating Time Series Data in Python",
    "section": "",
    "text": "import pandas as pd\n\n# Create the range of dates here\nseven_days = pd.date_range(start = '2017-1-1', periods=7, freq='D')\n\n# Iterate over the dates and print the number and name of the weekday\nfor day in seven_days:\n    print(day.dayofweek, day.day_name())\n\n6 Sunday\n0 Monday\n1 Tuesday\n2 Wednesday\n3 Thursday\n4 Friday\n5 Saturday"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nDiscuss slido responses"
  },
  {
    "objectID": "index.html#lecture",
    "href": "index.html#lecture",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nTheory covered in “Time Series Analysis in Python” DataCamp course"
  },
  {
    "objectID": "index.html#problem-set-2",
    "href": "index.html#problem-set-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Problem Set 2",
    "text": "Problem Set 2"
  },
  {
    "objectID": "index.html#announcements-1",
    "href": "index.html#announcements-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "index.html#lecture-1",
    "href": "index.html#lecture-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n\nI will make groups at random using code above. I will join a group if there is an odd person left out.\nWork together for 20 minutes to come up with single .ipynb\nI will pick two teams at random to present to the class\nI will present my work to the class. click here\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form"
  },
  {
    "objectID": "index.html#ps01-groups",
    "href": "index.html#ps01-groups",
    "title": "SDS390: Ecological Forecasting",
    "section": "PS01 Groups",
    "text": "PS01 Groups\n 1 Yang           Christy        1\n 2 O'Meara        Abbey          1\n 3 Slosser        Tillie         2 ***\n 4 Murray         Kiera          2 ***\n 5 Khan           Nubraz         3\n 6 Ding           Jenny          3\n 7 Anesko         Greta          4\n 8 Martin         Teddy          4\n 9 Xu             Xiaoman        5\n10 An             Rachael        5\n11 Kogalovski     Aleksandra     6\n12 Qiu            Chi            6\n13 Knecht         Beata          7 ***\n14 Basnet Chettri Charavee       7 ***\n15 Tha Ra Wun     Tint           8\n16 Huang          Juniper        8\n17 Pu             Betty          9"
  },
  {
    "objectID": "index.html#announcements-2",
    "href": "index.html#announcements-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS1 mini-presentations on Thursday\nPS1 submission format posted below"
  },
  {
    "objectID": "index.html#lecture-2",
    "href": "index.html#lecture-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nLec 4.2 on FPP 3.4 Classical Decomposition, second attempt\n\nWhat is \\(m\\) used in example?\nHow is seasonal component \\(S_t\\) computed? What is assumed seasonality?\nCode over code block below\n\nFFP 3.5 Briefly discuss other decomposition methods used\nFFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of \\(m\\)-MA smoothing like for classical decomposition \n\n\nlibrary(fpp3)\n\n# Code block 1: Recreate Fig 3.13 ----\n# 1.a) Get data\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\n# Note index is 1M = monthly. This sets m=12 in m-MA method below\nus_retail_employment\n\n# 1.b) Recreate Fig 3.13\nus_retail_employment %&gt;%\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n\n\n# Code block 2: Recreate all 4 subplots in Fig 3.13 ----\n# Get data frame with full breakdown of decomposition\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() %&gt;% \n  # Convert from tsibble to regular tibble for data wrangling\n  as_tibble()\n\n# 2.a) Top plot: Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# 2.b) 2nd plot: trend-cycle via MA average method\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# 2.c) Extra: detrended plot\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Detrended data Original data minus trend-cycle component\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n# 2.d) 3rd plot: seasonality (values repeat every 12 months)\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"Seasonality\")\n\n# 2.e) 4th plot: Compute remainders\nggplot(full_decomposition, aes(x=Month, y = random)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n\n\n# Code block 3:  Compute full breakdown for one row: 1990 July ----\n# 3.a) LOOK at data\n# IMO most important function in RStudio: View()\n# Focus on Row 7 1990 July: where do all these values come from?\nView(full_decomposition)\n\n# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA\n# Compute mean of first 12 values\nfull_decomposition %&gt;% \n  slice(1:12) %&gt;% \n  summarize(mean = mean(Employed))\n# Compute mean of next 12 values\nfull_decomposition %&gt;% \n  slice(2:13) %&gt;% \n  summarize(mean = mean(Employed))\n# Now do second averaging of 2 values\n(13186 + 13170)/2\n\n# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625\nfull_decomposition &lt;- full_decomposition %&gt;% \n  mutate(detrended = Employed - trend)\n\n# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661\nfull_decomposition %&gt;% \n  mutate(month_num = month(Month)) %&gt;% \n  group_by(month_num) %&gt;% \n  summarize(St = mean(detrended, na.rm = TRUE))\n\n# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610\n# y_t - T_hat_t - S_hat_t\n13170.1 - 13177.76 - (-13.311661)"
  },
  {
    "objectID": "index.html#announcements-3",
    "href": "index.html#announcements-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nComment on the generalizability of everything I say"
  },
  {
    "objectID": "index.html#lecture-3",
    "href": "index.html#lecture-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\n\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Modified version of code to produce FPP Fig 3.10\naus_exports &lt;- global_economy |&gt;\n  filter(Country == \"Australia\") \n\n# Note number of rows\naus_exports\n\n# Set m and plot\nm &lt;- 28\naus_exports |&gt;\n  mutate(\n    `m-MA` = slider::slide_dbl(Exports, mean,\n                .before = m, .after = m, .complete = TRUE)\n  ) |&gt;\n  autoplot(Exports) +\n  geom_line(aes(y = `m-MA`), colour = \"#D55E00\") +\n  labs(y = \"% of GDP\",\n       title = \"Total Australian exports\") +\n  guides(colour = guide_legend(title = \"series\"))\n\n\n# Code block 2 ----\n# Classical decomposition breakdown\n\n# Plot TS data in questions\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\nus_retail_employment\n\n# Code to create full Fig 3.13\nus_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n# Get data frame with all the decomposition parts\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components()\n\nfull_decomposition\n\n# Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# Step 2: Subtract trend-cycle from original data\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Original data minus trend-cycle component\")\n\n# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"For each season compute average\")\n\n# Step 4: Compute remainders. 4th plot of Fig 3.13\nggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\")"
  },
  {
    "objectID": "index.html#announcements-4",
    "href": "index.html#announcements-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nThe next course “Time Series Analysis in Python” due Tue 10/3 Thu 10/5 9:25am.\nI’m keeping up with screencasts, still need to finish Chapter 4.\nSlido responses:\n\n“I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I’m doing a small portion of a data analysis. I find myself”going through the motions” and feeling it is tedious because I don’t think I’m really comprehending the importance of each step.”\n“Making stupid mistakes on the syntax and kind of confused about the difference between [] and . when calling an attribute”\n“Worried I won’t retain my understanding”\n“I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it”\n\nMain tip: “Optimal frustration”"
  },
  {
    "objectID": "index.html#lecture-4",
    "href": "index.html#lecture-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 3.1 Transformations. \\(\\log10\\) transformations:\n\nWhat are \\(\\log\\) (base \\(e\\)) and \\(\\log10\\) (base 10) tranformations? Example table\nEffect on visualizations: Example figure\n\nFPP 3.2 Time series decompositions"
  },
  {
    "objectID": "index.html#problem-set-1",
    "href": "index.html#problem-set-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Problem Set 1",
    "text": "Problem Set 1\n\nPosted on Slack under #questions\nIndividual PS01.ipynb files\n\nDue Thu 10/5 9:25am on moodle (see Moodle link on top right of page)\nSubmit both a PS01.ipynb where all code is reproducible and a .csv file of your data\n\nIn-class on Thu 10/5: “Think, Pair, Share” exercise\n\nI will randomly create teams of pairs. Any remaining odd number student will be paired with me.\nYou will show each other your code and prepare a single mini-presentation .ipynb\nI will pick 2-3 pairs at random to present their work in front of the class\nYou will rate your peer’s preparation using this Google Form\n\n\nClarifications added afterwards:\n\nDataset should be at least 100 rows\nFor mini-presentation, you will have to choose one of the two datasets\nDo problem set in python"
  },
  {
    "objectID": "index.html#announcements-5",
    "href": "index.html#announcements-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nFirst problem set assigned on Tue, which will build into first mini-presentation\nMountain Day recap\nOriginally assigned course “Manipulating Time Series Data in Python” due next Tue 9/26 before class\nGo to Roster Google Sheet (top right of page) and fill DC columns"
  },
  {
    "objectID": "index.html#lecture-5",
    "href": "index.html#lecture-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nDataCamp\n\nPoll class on sli.do about DC\nDC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.\nProf. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3\nScreencasts location\nContinuing Time Series with Python skill track, the next course “Time Series Analysis in Python” due Tue 10/3. If there is an Exercise you’d like me to do in class, let me know.\n\nFinish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Lag plots: relationship of a TS variable with itself in the past\n# Create regular TS plot of data in Fig 2.19 Beer production over time\nrecent_production &lt;- aus_production |&gt;\n  filter(year(Quarter) &gt;= 2000)\n\n# Note time index meta-data = 1Q = quarter\nrecent_production\n\n# Note patterns\nrecent_production |&gt; autoplot(Beer)"
  },
  {
    "objectID": "PS/PS01_albert.html#get-btc-data",
    "href": "PS/PS01_albert.html#get-btc-data",
    "title": "Problem Set 01",
    "section": "Get BTC data",
    "text": "Get BTC data\n\n# ChatGPT Code from https://chat.openai.com/share/572f7333-82a9-411f-95df-4f543fd3ae96\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nbtc_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(btc_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nbtc_df = df.set_index('Timestamp')\nbtc_df.rename(columns={\"Price\": \"BTC\"}, inplace = True)\n\nbtc_df.info()\nbtc_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:09:11\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nBTC\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n27968.128047\n\n\n2023-04-10\n28351.236994\n\n\n2023-04-11\n29657.974137\n\n\n2023-04-12\n30260.936109\n\n\n2023-04-13\n29904.138695"
  },
  {
    "objectID": "PS/PS01_albert.html#get-eth-data",
    "href": "PS/PS01_albert.html#get-eth-data",
    "title": "Problem Set 01",
    "section": "Get ETH data",
    "text": "Get ETH data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\neth_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(eth_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\neth_df = df.set_index('Timestamp')\neth_df.rename(columns={\"Price\": \"ETH\"}, inplace = True)\n\neth_df.info()\neth_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 05:28:41\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ETH     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nETH\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n1851.050671\n\n\n2023-04-10\n1859.940387\n\n\n2023-04-11\n1909.882061\n\n\n2023-04-12\n1892.938911\n\n\n2023-04-13\n1920.223031"
  },
  {
    "objectID": "PS/PS01_albert.html#get-sol-data",
    "href": "PS/PS01_albert.html#get-sol-data",
    "title": "Problem Set 01",
    "section": "Get SOL data",
    "text": "Get SOL data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/solana/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nsol_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(sol_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nsol_df = df.set_index('Timestamp')\nsol_df.rename(columns={\"Price\": \"SOL\"}, inplace = True)\n\nsol_df.info()\nsol_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:22:21\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   SOL     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nSOL\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n20.042581\n\n\n2023-04-10\n20.304337\n\n\n2023-04-11\n20.875123\n\n\n2023-04-12\n23.005655\n\n\n2023-04-13\n23.830533\n\n\n\n\n\n\n\n\n# Merge all three data frames and save to csv\n# Export the DataFrame to a CSV file\ncrypto_df = btc_df.join(eth_df, how = 'inner').join(sol_df, how = 'inner')\ncrypto_df.info()\ncrypto_df.to_csv(\"crypto_price_data.csv\", index=False)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 180 entries, 2023-04-09 to 2023-10-05\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     180 non-null    float64\n 1   ETH     180 non-null    float64\n 2   SOL     180 non-null    float64\ndtypes: float64(3)\nmemory usage: 5.6 KB\n\n\n\ncrypto_df.head()\n\n\n\n\n\n\n\n\nBTC\nETH\nSOL\n\n\nTimestamp\n\n\n\n\n\n\n\n2023-04-09\n27968.128047\n1851.050671\n20.042581\n\n\n2023-04-10\n28351.236994\n1859.940387\n20.304337\n\n\n2023-04-11\n29657.974137\n1909.882061\n20.875123\n\n\n2023-04-12\n30260.936109\n1892.938911\n23.005655\n\n\n2023-04-13\n29904.138695\n1920.223031\n23.830533"
  }
]