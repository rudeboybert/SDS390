[
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS02 mini-presentations today. Info in Problem Sets"
  },
  {
    "objectID": "index.html#lecture",
    "href": "index.html#lecture",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture"
  },
  {
    "objectID": "index.html#announcements-1",
    "href": "index.html#announcements-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nUpdate PS02 in-class presentation format in Problem Sets\nPS02 mini-presentations this coming Tuesday"
  },
  {
    "objectID": "index.html#lecture-1",
    "href": "index.html#lecture-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nCode blocks below\nRefresher on conditions for inference for linear regression\n\n\nlibrary(fpp3)\n\n# Code block 1: Fit model to data and generate forecasts ----\n# Tidy:\nfull_data &lt;- aus_production %&gt;% \n  filter_index(\"1992 Q1\" ~ \"2010 Q2\")\n\n# Visualize:\nfull_data %&gt;% \n  autoplot(Beer)\n\n# Specify: Take full data and fit 3 models\nbeer_fit_1 &lt;- full_data |&gt;\n  model(\n    Mean = MEAN(Beer),\n    `Naïve` = NAIVE(Beer),\n    `Seasonal naïve` = SNAIVE(Beer)\n  )\n\n# Generate forecasts 14 time periods (quarters) into future\nbeer_forecast_1 &lt;- beer_fit_1 |&gt; \n  forecast(h = 14)\n\n# Plot full data and forecasts\nplot1 &lt;- beer_forecast_1 |&gt;\n  autoplot(full_data, level = NULL) +\n  labs(\n    y = \"Megalitres\",\n    title = \"Forecasts for quarterly beer production\"\n  ) +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\nplot1\n\n\n\n# Code block 2: Training test set approach ----\n# Define training data to be up to 2007: fit the model to this data\ntraining_data &lt;- full_data |&gt;\n  filter_index(\"1992 Q1\" ~ \"2006 Q4\")\n\n# Define test data to be 2007 and beyond:\ntest_data &lt;- full_data |&gt;\n  filter_index(\"2007 Q1\" ~ \"2010 Q2\")\n\n# Fit 3 models only to training data\nbeer_fit_2 &lt;- training_data |&gt;\n  model(\n    Mean = MEAN(Beer),\n    `Naïve` = NAIVE(Beer),\n    `Seasonal naïve` = SNAIVE(Beer)\n  )\n\n# Generate forecasts for 14 quarters\nbeer_forecast_2 &lt;- beer_fit_2 |&gt; \n  forecast(h = 14)\n\n# Plot training data and forecasts only\nplot2 &lt;- beer_forecast_2 |&gt;\n  autoplot(training_data, level = NULL) +\n  labs(\n    y = \"Megalitres\",\n    title = \"Forecasts for quarterly beer production\"\n  ) +\n  guides(colour = guide_legend(title = \"Forecast\"))\nplot2\n`\n# Add the test data. We can now evaluate quality of predictions\nplot2 +\n  autolayer(test_data, colour = \"black\")\n\n\n\n# Code block 3: Residual analysis ----\n# Get residuals\naugment(beer_fit_1) %&gt;% \n  View()\n\n# Focus only on seasonal naive method:\nseasonal_naive_augment &lt;- augment(beer_fit_1) %&gt;% \n  filter(.model == \"Seasonal naïve\")\n\n# Check 2 and 3: Mean 0 and constant variance\nseasonal_naive_augment %&gt;% \n  autoplot(.resid) +\n  geom_hline(yintercept=0, linetype = \"dashed\")\n\n# Check 1: Uncorrelated residuals\nseasonal_naive_augment %&gt;% \n  ACF(.resid) %&gt;% \n  autoplot() +\n  labs(title = \"Residuals from the naïve method\")\n\n# Check 2 and 4: Zero mean and normally distributed\nggplot(seasonal_naive_augment, aes(x = .resid)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "index.html#announcements-2",
    "href": "index.html#announcements-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nNext DC course “Visualizing Time Series Data in Python” assigned due Tue 10/31\nDiscuss PS02 in-class presentation format on Tue 10/24 using PS01 feedback from slido"
  },
  {
    "objectID": "index.html#lecture-2",
    "href": "index.html#lecture-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nlibrary(fpp3)\nlibrary(feasts)\n\n# Code block 1: FPP 4.2 ACF features ----\n# Reprint output from book\ntourism |&gt; \n  features(Trips, feat_acf)\n\n# a) Focus on only top row of output from book\ntop_row &lt;- tourism %&gt;% \n  filter(Region == \"Adelaide\", State == \"South Australia\", Purpose == \"Business\")\n\ntop_row |&gt; \n  features(Trips, feat_acf)\n\n# LOOK AT YOUR DATA!!\ntop_row %&gt;% \n  autoplot(Trips)\n\n# b) ACF at k=1 is acf1 in output from book\ntop_row %&gt;% \n  ACF(Trips) %&gt;% \n  autoplot()\n\n# c) Take differences of data using difference() from FPP 9.1\n# https://otexts.com/fpp3/stationarity.html#differencing\n# For example switch from size to growth\ntop_row %&gt;% \n  autoplot(difference(Trips))\n\n# ACF at k=1 is diff1_acf1 in output from book\ntop_row %&gt;% \n  ACF(difference(Trips)) %&gt;% \n  autoplot()\n\n\n\n# Code block 2: FPP 4.3 ACF features ----\n# Compare and contrast two classical decomposition features\n\n# a) FPP 4.3 Australia trips\ntop_row %&gt;% \n  model(\n    classical_decomposition(Trips, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total trips\")\n\n# b) FPP 3.4 Fig 3.13 from Lec 5.1\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\nus_retail_employment %&gt;%\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n# c) compare trend_strength and seasonal_strength_year\ntop_row |&gt;\n  features(Trips, feat_stl) %&gt;% \n  View()\n\nus_retail_employment |&gt;\n  features(Employed, feat_stl) %&gt;% \n  View()"
  },
  {
    "objectID": "index.html#announcements-3",
    "href": "index.html#announcements-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nStill missing a few PS01 peer evalutions\nDiscuss slido responses"
  },
  {
    "objectID": "index.html#lecture-3",
    "href": "index.html#lecture-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\n“Time Series Analysis in Python” DataCamp course Chapters 3-5 of this course focused on “autoregressive integrated moving average” (ARIMA) models. This is covered in:\n\nFPP Chapter 9\nThe 4th of 5 courses in “Time Series with Python” DC skill track (see schedule above).\n\nChalk talk\nStart PS02 (on new “Problem Sets” tab of webpage)"
  },
  {
    "objectID": "index.html#announcements-4",
    "href": "index.html#announcements-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "index.html#lecture-4",
    "href": "index.html#lecture-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nProblem Set 01 in-class presentations"
  },
  {
    "objectID": "index.html#announcements-5",
    "href": "index.html#announcements-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS1 mini-presentations on Thursday\nPS1 submission format posted below"
  },
  {
    "objectID": "index.html#lecture-5",
    "href": "index.html#lecture-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nLec 4.2 on FPP 3.4 Classical Decomposition, second attempt\n\nWhat is \\(m\\) used in example?\nHow is seasonal component \\(S_t\\) computed? What is assumed seasonality?\nCode over code block below\n\nFFP 3.5 Briefly discuss other decomposition methods used\nFFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of \\(m\\)-MA smoothing like for classical decomposition \n\n\nlibrary(fpp3)\n\n# Code block 1: Recreate Fig 3.13 ----\n# 1.a) Get data\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\n# Note index is 1M = monthly. This sets m=12 in m-MA method below\nus_retail_employment\n\n# 1.b) Recreate Fig 3.13\nus_retail_employment %&gt;%\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n\n\n# Code block 2: Recreate all 4 subplots in Fig 3.13 ----\n# Get data frame with full breakdown of decomposition\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() %&gt;% \n  # Convert from tsibble to regular tibble for data wrangling\n  as_tibble()\n\n# 2.a) Top plot: Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# 2.b) 2nd plot: trend-cycle via MA average method\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# 2.c) Extra: detrended plot\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Detrended data Original data minus trend-cycle component\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n# 2.d) 3rd plot: seasonality (values repeat every 12 months)\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"Seasonality\")\n\n# 2.e) 4th plot: Compute remainders\nggplot(full_decomposition, aes(x=Month, y = random)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n\n\n# Code block 3:  Compute full breakdown for one row: 1990 July ----\n# 3.a) LOOK at data\n# IMO most important function in RStudio: View()\n# Focus on Row 7 1990 July: where do all these values come from?\nView(full_decomposition)\n\n# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA\n# Compute mean of first 12 values\nfull_decomposition %&gt;% \n  slice(1:12) %&gt;% \n  summarize(mean = mean(Employed))\n# Compute mean of next 12 values\nfull_decomposition %&gt;% \n  slice(2:13) %&gt;% \n  summarize(mean = mean(Employed))\n# Now do second averaging of 2 values\n(13186 + 13170)/2\n\n# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625\nfull_decomposition &lt;- full_decomposition %&gt;% \n  mutate(detrended = Employed - trend)\n\n# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661\nfull_decomposition %&gt;% \n  mutate(month_num = month(Month)) %&gt;% \n  group_by(month_num) %&gt;% \n  summarize(St = mean(detrended, na.rm = TRUE))\n\n# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610\n# y_t - T_hat_t - S_hat_t\n13170.1 - 13177.76 - (-13.311661)"
  },
  {
    "objectID": "index.html#announcements-6",
    "href": "index.html#announcements-6",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nComment on the generalizability of everything I say"
  },
  {
    "objectID": "index.html#lecture-6",
    "href": "index.html#lecture-6",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\n\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Modified version of code to produce FPP Fig 3.10\naus_exports &lt;- global_economy |&gt;\n  filter(Country == \"Australia\") \n\n# Note number of rows\naus_exports\n\n# Set m and plot\nm &lt;- 28\naus_exports |&gt;\n  mutate(\n    `m-MA` = slider::slide_dbl(Exports, mean,\n                .before = m, .after = m, .complete = TRUE)\n  ) |&gt;\n  autoplot(Exports) +\n  geom_line(aes(y = `m-MA`), colour = \"#D55E00\") +\n  labs(y = \"% of GDP\",\n       title = \"Total Australian exports\") +\n  guides(colour = guide_legend(title = \"series\"))\n\n\n# Code block 2 ----\n# Classical decomposition breakdown\n\n# Plot TS data in questions\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\nus_retail_employment\n\n# Code to create full Fig 3.13\nus_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n# Get data frame with all the decomposition parts\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components()\n\nfull_decomposition\n\n# Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# Step 2: Subtract trend-cycle from original data\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Original data minus trend-cycle component\")\n\n# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"For each season compute average\")\n\n# Step 4: Compute remainders. 4th plot of Fig 3.13\nggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\")"
  },
  {
    "objectID": "index.html#announcements-7",
    "href": "index.html#announcements-7",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nThe next course “Time Series Analysis in Python” due Tue 10/3 Thu 10/5 9:25am.\nI’m keeping up with screencasts, still need to finish Chapter 4.\nSlido responses:\n\n“I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I’m doing a small portion of a data analysis. I find myself”going through the motions” and feeling it is tedious because I don’t think I’m really comprehending the importance of each step.”\n“Making stupid mistakes on the syntax and kind of confused about the difference between [] and . when calling an attribute”\n“Worried I won’t retain my understanding”\n“I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it”\n\nMain tip: “Optimal frustration”"
  },
  {
    "objectID": "index.html#lecture-7",
    "href": "index.html#lecture-7",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 3.1 Transformations. \\(\\log10\\) transformations:\n\nWhat are \\(\\log\\) (base \\(e\\)) and \\(\\log10\\) (base 10) tranformations? Example table\nEffect on visualizations: Example figure\n\nFPP 3.2 Time series decompositions\nProblem Set 1 posted"
  },
  {
    "objectID": "index.html#problem-set-1",
    "href": "index.html#problem-set-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Problem Set 1",
    "text": "Problem Set 1"
  },
  {
    "objectID": "index.html#announcements-8",
    "href": "index.html#announcements-8",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nFirst problem set assigned on Tue, which will build into first mini-presentation\nMountain Day recap\nOriginally assigned course “Manipulating Time Series Data in Python” due next Tue 9/26 before class\nGo to Roster Google Sheet (top right of page) and fill DC columns"
  },
  {
    "objectID": "index.html#lecture-8",
    "href": "index.html#lecture-8",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nDataCamp\n\nPoll class on sli.do about DC\nDC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.\nProf. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3\nScreencasts location\nContinuing Time Series with Python skill track, the next course “Time Series Analysis in Python” due Tue 10/3. If there is an Exercise you’d like me to do in class, let me know.\n\nFinish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Lag plots: relationship of a TS variable with itself in the past\n# Create regular TS plot of data in Fig 2.19 Beer production over time\nrecent_production &lt;- aus_production |&gt;\n  filter(year(Quarter) &gt;= 2000)\n\n# Note time index meta-data = 1Q = quarter\nrecent_production\n\n# Note patterns\nrecent_production |&gt; autoplot(Beer)"
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": "Manipulating Time Series Data in Python",
    "section": "",
    "text": "import pandas as pd\n\n# Create the range of dates here\nseven_days = pd.date_range(start = '2017-1-1', periods=7, freq='D')\n\n# Iterate over the dates and print the number and name of the weekday\nfor day in seven_days:\n    print(day.dayofweek, day.day_name())\n\n6 Sunday\n0 Monday\n1 Tuesday\n2 Wednesday\n3 Thursday\n4 Friday\n5 Saturday"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html",
    "href": "PS/PS02_presentations/PS02_betty_chi.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: Chi Qiu 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html#method-i",
    "href": "PS/PS02_presentations/PS02_betty_chi.html#method-i",
    "title": "Problem Set 02",
    "section": "Method I",
    "text": "Method I\n\n# Source: Data Camp\n# Fit an AR(2) model\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\nmod = ARIMA(df, order=(2,0,0))\nres = mod.fit()\n\n\n# Print the summary of the result\nprint(res.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                 3778\nModel:                 ARIMA(2, 0, 0)   Log Likelihood                9411.044\nDate:                Tue, 24 Oct 2023   AIC                         -18814.088\nTime:                        10:29:52   BIC                         -18789.140\nSample:                    01-27-2010   HQIC                        -18805.219\n                         - 05-31-2020                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        633.8438   1.51e-09    4.2e+11      0.000     633.844     633.844\nar.L1          1.8933      0.001   2352.900      0.000       1.892       1.895\nar.L2         -0.8933      0.001  -1109.748      0.000      -0.895      -0.892\nsigma2         0.0004   4.66e-07    858.072      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   8.02   Jarque-Bera (JB):         233549300.72\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               8.23   Skew:                             5.50\nProb(H) (two-sided):                  0.00   Kurtosis:                      1221.00\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n[2] Covariance matrix is singular or near-singular, with condition number 2.32e+21. Standard errors may be unstable.\n\n\n\n# In-sample & Out-of-sample forecasts\n\nfig, ax = plt.subplots()\ndf['2010':].plot(ax=ax, label='Original', marker='o', markersize=3, color='yellow')\n\nplot_predict(res, start='2020-06-01', end='2022-06-01', ax=ax)\nplot_predict(res, start='2010-01-27', end='2020-06-01', ax=ax, alpha=None)\n\nplt.legend([\"Original\", \"Out-of-Sample Forecast\", \"95% Confidence Interval\", \"In-Sample Forecast\"])\nplt.show()"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html#method-ii",
    "href": "PS/PS02_presentations/PS02_betty_chi.html#method-ii",
    "title": "Problem Set 02",
    "section": "Method II",
    "text": "Method II\n\n# Source: ChatGPT\n# Fit an AR(2) model\nmodel = sm.tsa.AutoReg(df['dbh'], lags=2)\nresults = model.fit()\n\n\n# Print the summary of the result\nprint(results.summary())\n\n                            AutoReg Model Results                             \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                 3778\nModel:                     AutoReg(2)   Log Likelihood                9422.887\nMethod:               Conditional MLE   S.D. of innovations              0.020\nDate:                Mon, 23 Oct 2023   AIC                         -18837.775\nTime:                        16:59:57   BIC                         -18812.829\nSample:                    01-29-2010   HQIC                        -18828.906\n                         - 05-31-2020                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0017      0.016      0.105      0.916      -0.029       0.032\ndbh.L1         1.8840      0.008    247.624      0.000       1.869       1.899\ndbh.L2        -0.8840      0.008   -116.183      0.000      -0.899      -0.869\n                                    Roots                                    \n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            1.0000           +0.0000j            1.0000            0.0000\nAR.2            1.1312           +0.0000j            1.1312            0.0000\n-----------------------------------------------------------------------------\n\n\n\n# In-sample fitted forecasts\npred_insample = results.fittedvalues\n\n# Out-of-sample forecasts\nforecast_outsample = results.get_prediction(start=len(df), end=len(df) + 730)  # 730 days = 2 years * 365 days\n\n# Get forecast values and confidence intervals\nforecast_mean = forecast_outsample.predicted_mean\nforecast_conf_int = forecast_outsample.conf_int()\n\n# Plot the data and forecasts\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['dbh'], label='Original', linestyle='-', marker='o', markersize=3, color='blue')\nplt.plot(pred_insample.index, pred_insample, label='In-Sample Forecast', color='orange')\nplt.plot(forecast_mean.index, forecast_mean, label='Out-of-Sample Forecast', color='red')\n\n# Fill the area between the confidence intervals\nplt.fill_between(forecast_conf_int.index, forecast_conf_int.iloc[:, 0], forecast_conf_int.iloc[:, 1], color='pink', alpha=0.3, label='95% CI')\n\nplt.xlabel('Date')\nplt.ylabel('DBH')\nplt.legend()\nplt.title('AR(2) Forecast for Tree Size')\nplt.show()\n\n\n\n\nComment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem. - Both forecasts fall within the 95% confidence interval, thus both are reliable. But the first CI is much wider than the second CI, thus the forecast quality might be lower - The second method provides a better forecast given its increasing trend - After running the first method, I ran the second method through ChatGPT to explore other model options that will improve the quality"
  },
  {
    "objectID": "PS/PS02_albert.html",
    "href": "PS/PS02_albert.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n1. Load and explore data\n\nLoad the CSV data of biweekly dendroband measures for tree with tag 82203\nLook at the raw data (rows and variables) and meta-data as Amir always told me to do\nNote that DBH is the only non-index variable we will work with, thus simplify the data frame to only have this variable\nPlot the time series of DBH (diameter at breast height, in mm) as we’ve done numerous times in DataCamp: just using .plot() with no arguments\nNow plot the time series where each of the 147 observed data point is marked with a dot. In other words, your plot should at the very least have these points.\nWhat is the difference in information presented between the two plots?\nDescribe any patterns you observe in the time series where each observed data point is marked with a dot\n\n\ntag_82203 = pd.read_csv('tag_82203.csv', parse_dates = ['date'], index_col = 'date')\ntag_82203.head()\n\n\n\n\n\n\n\n\ntag\nstemtag\nsurvey.ID\nyear\nmonth\nday\nintraannual\nsp\nmeasure\ncodes\nnotes\nstatus\nstemID\ndendDiam\ndbh_orig\nnew.band\ndendroID\ndbh\nscenario\ndata_source\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010-01-27\n82203\n1\n2010.01\n2010\n1\n27\n1\nlitu\n26.20\nNaN\nNaN\nalive\n10045\n610.0\n611.3\n1\n106\n611.300000\n0\nbiweekly_dbh\n\n\n2011-01-15\n82203\n1\n2011.01\n2011\n1\n15\n1\nlitu\n41.10\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n616.049980\n1\nbiweekly_dbh\n\n\n2011-05-09\n82203\n1\n2011.02\n2011\n5\n9\n1\nlitu\n40.62\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n615.896874\n1\nbiweekly_dbh\n\n\n2011-05-13\n82203\n1\n2011.03\n2011\n5\n13\n1\nlitu\n40.21\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n615.766085\n1\nbiweekly_dbh\n\n\n2011-05-16\n82203\n1\n2011.04\n2011\n5\n16\n1\nlitu\n42.55\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n616.512610\n1\nbiweekly_dbh\n\n\n\n\n\n\n\n\ntag_82203.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 147 entries, 2010-01-27 to 2020-05-31\nData columns (total 20 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   tag          147 non-null    int64  \n 1   stemtag      147 non-null    int64  \n 2   survey.ID    147 non-null    float64\n 3   year         147 non-null    int64  \n 4   month        147 non-null    int64  \n 5   day          147 non-null    int64  \n 6   intraannual  147 non-null    int64  \n 7   sp           147 non-null    object \n 8   measure      147 non-null    float64\n 9   codes        1 non-null      object \n 10  notes        4 non-null      object \n 11  status       147 non-null    object \n 12  stemID       147 non-null    int64  \n 13  dendDiam     6 non-null      float64\n 14  dbh_orig     147 non-null    float64\n 15  new.band     147 non-null    int64  \n 16  dendroID     147 non-null    int64  \n 17  dbh          147 non-null    float64\n 18  scenario     147 non-null    int64  \n 19  data_source  147 non-null    object \ndtypes: float64(5), int64(10), object(5)\nmemory usage: 24.1+ KB\n\n\n\n# Recall to select a variable but return a data frame, not a series, you need to specify your variables as a list.\ntag_82203 = tag_82203[['dbh']]\n\n\ntag_82203.plot()\nplt.show()\n\n\n\n\n\ntag_82203.plot(marker='o', markersize=3)\nplt.show()\n\n\n\n\nThe original plot connects each consecutive point, so we have a hard time seeing at what interval the data is collected. The plot with the observed points marked with dots shows:\n\nThere was one value in 2010 and then a jump\nValues are not collected in winter\n\n\n\n2. Decomposition Plots\nCreate a seasonal decomposition plot of this data to answer the following questions: 1. What is the approximate growth (in mm) per year trend for this tree 1. What is the approximate range in seasonal variation in growth around this trend? 1. When are the peaks and when are the valleys in this seasonal variation? 1. Are there periods of time where we might be skeptical of the above trend and seasonality?\nNote: You may need to do some data transformation as seen in the first DataCamp course to get a valid plot\n\n# Downsample data from daily to monthly by taking average\ntag_82203_monthly = tag_82203.resample('m').mean()\n# Interpolate all missing values\ntag_82203_monthly.interpolate(inplace=True)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(tag_82203_monthly, model='additive')\n\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(411)\nplt.plot(tag_82203_monthly, label='Original')\nplt.legend(loc='upper left')\nplt.title('Original Time Series')\n\nplt.subplot(412)\nplt.plot(decomposition.trend, label='Trend')\nplt.legend(loc='upper left')\nplt.title('Trend Component')\n\nplt.subplot(413)\nplt.plot(decomposition.seasonal, label='Seasonality')\nplt.legend(loc='upper left')\nplt.title('Seasonal Component')\n\nplt.subplot(414)\nplt.plot(decomposition.resid, label='Residuals')\nplt.legend(loc='upper left')\nplt.title('Residual Component')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nIn the 9 years from 2011 to 2020, the tree grew from 620m to 650mm = (650-620)/9 = 3.33mm per year about\nPlus or minus 1 mm, so close to 1/3rd of annual growth!\nPeaks are in summer, valleys in late winter\nLooking are the residuals, I’d be suspicious of early values at 2011 and late values in 2019 and 2020\n\n\n\n3. Forecasting using an AR(2) model\nIn this exercise you will use an AR(2) model as seen in the second datacamp course “Time Series Analysis in Python” to forecast this tree’s size exactly two years into the future. Unfortunately since we aren’t covering ARIMA models until later in the course, we won’t be able to:\n\nAscertain the appropriateness of using an AR(2) model. Ex: are all assumptions met?\nFully interpret the model output’s meaning\nDiagnose any issues.\n\nHowever, we will simply view this code as a minimally viable product that we will improve on later. Instructions: 1. Fit an appropriate AR(2) model 1. Print the summary of the result 1. Plot the TS data along with both in-sample fitted forecasts AND out-of-sample forecasts exactly two years into the future 1. Comment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem.\n\n# Earliste date recorded is 2010-01-31\ntag2.head()\n\n\n\n\n\n\n\n\ndbh\n\n\ndate\n\n\n\n\n\n2010-01-31\n611.3\n\n\n2010-02-28\n611.3\n\n\n2010-03-31\n611.3\n\n\n2010-04-30\n611.3\n\n\n2010-05-31\n611.3\n\n\n\n\n\n\n\n\n# Last date recorded is 2020-05-31, so we will forecast upto 2022-05-31\ntag2.tail()\n\n\n\n\n\n\n\n\ndbh\n\n\ndate\n\n\n\n\n\n2020-01-31\n657.856802\n\n\n2020-02-29\n657.856802\n\n\n2020-03-31\n657.920522\n\n\n2020-04-30\n658.548156\n\n\n2020-05-31\n658.533295\n\n\n\n\n\n\n\n\n# Code from DataCamp \"Time Series Analysis in Python\" course, Chapter 3, Video 2, Exercise 2\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\nmod = ARIMA(tag2, order=(2,0,0))\nresult = mod.fit()\nresult.summary()\n\n\nSARIMAX Results\n\n\nDep. Variable:\ndbh\nNo. Observations:\n125\n\n\nModel:\nARIMA(2, 0, 0)\nLog Likelihood\n-169.785\n\n\nDate:\nWed, 11 Oct 2023\nAIC\n347.570\n\n\nTime:\n11:20:47\nBIC\n358.883\n\n\nSample:\n01-31-2010\nHQIC\n352.166\n\n\n\n- 05-31-2020\n\n\n\n\nCovariance Type:\nopg\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n634.2981\n21.607\n29.356\n0.000\n591.949\n676.647\n\n\nar.L1\n1.3073\n0.071\n18.423\n0.000\n1.168\n1.446\n\n\nar.L2\n-0.3085\n0.071\n-4.365\n0.000\n-0.447\n-0.170\n\n\nsigma2\n0.8410\n0.036\n23.067\n0.000\n0.770\n0.912\n\n\n\n\n\n\nLjung-Box (L1) (Q):\n1.51\nJarque-Bera (JB):\n1459.08\n\n\nProb(Q):\n0.22\nProb(JB):\n0.00\n\n\nHeteroskedasticity (H):\n1.27\nSkew:\n3.31\n\n\nProb(H) (two-sided):\n0.45\nKurtosis:\n18.38\n\n\n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Plot\nfig, ax = plt.subplots()\ntag2.plot(ax=ax)\nplot_predict(result, start='2010-01-31', end='2021-05-31', ax=ax)\nplt.show()\n\n\n\n\nWe observe that for in-sample values from 2011 and onwards up until the last observation on 2020-05-31, the AR(2) model fits the data reasonably well. However the out-of-sample forecasts into the future make little sense.\nMaking taking differences will return better results? i.e. growth per month?\n\nmod = ARIMA(tag2.diff(), order=(2,0,0))\nresult = mod.fit()\n\nfig, ax = plt.subplots()\ntag2.diff().plot(ax=ax)\nplot_predict(result, start='2010-01-31', end='2021-05-31', ax=ax)\nplt.show()\n\n\n\n\nThe forecast of growth seems a little more reasonable, especially when forecasting out-of-sample. We could then undo the difference to get DBH size values. However, we are trusting the appropriateness of AR(2) on blind faith; hopefually we can revisit later in the semester."
  },
  {
    "objectID": "PS/PS02.html",
    "href": "PS/PS02.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n1. Load and explore data\n\nLoad the CSV data of biweekly dendroband measures for tree with tag 82203\nLook at the raw data (rows and variables) and meta-data as Amir always told me to do\nNote that DBH is the only non-index variable we will work with, thus simplify the data frame to only have this variable\nPlot the time series of DBH (diameter at breast height, in mm) as we’ve done numerous times in DataCamp: just using .plot() with no arguments\nNow plot the time series where each of the 147 observed data point is marked with a dot. In other words, your plot should at the very least have these points.\nWhat is the difference in information presented between the two plots?\nDescribe any patterns you observe in the time series where each observed data point is marked with a dot\n\n\n\n2. Decomposition Plots\nCreate a seasonal decomposition plot of this data to answer the following questions: 1. What is the approximate growth (in mm) per year trend for this tree 1. What is the approximate range in seasonal variation in growth around this trend? 1. When are the peaks and when are the valleys in this seasonal variation? 1. Are there periods of time where we might be skeptical of the above trend and seasonality?\nNote: You may need to do some data transformation as seen in the first DataCamp course to get a valid plot\n\n\n3. Forecasting using an AR(2) model\nIn this exercise you will use an AR(2) model as seen in the second datacamp course “Time Series Analysis in Python” to forecast this tree’s size exactly two years into the future. Unfortunately since we aren’t covering ARIMA models until later in the course, we won’t be able to:\n\nAscertain the appropriateness of using an AR(2) model. Ex: are all assumptions met?\nFully interpret the model output’s meaning\nDiagnose any issues.\n\nHowever, we will simply view this code as a minimally viable product that we will improve on later. Instructions: 1. Fit an appropriate AR(2) model 1. Print the summary of the result 1. Plot the TS data along with both in-sample fitted forecasts AND out-of-sample forecasts exactly two years into the future 1. Comment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem."
  },
  {
    "objectID": "PS/PS01_albert.html#get-btc-data",
    "href": "PS/PS01_albert.html#get-btc-data",
    "title": "Problem Set 01",
    "section": "Get BTC data",
    "text": "Get BTC data\n\n# ChatGPT Code from https://chat.openai.com/share/572f7333-82a9-411f-95df-4f543fd3ae96\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nbtc_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(btc_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nbtc_df = df.set_index('Timestamp')\nbtc_df.rename(columns={\"Price\": \"BTC\"}, inplace = True)\n\nbtc_df.info()\nbtc_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:09:11\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nBTC\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n27968.128047\n\n\n2023-04-10\n28351.236994\n\n\n2023-04-11\n29657.974137\n\n\n2023-04-12\n30260.936109\n\n\n2023-04-13\n29904.138695"
  },
  {
    "objectID": "PS/PS01_albert.html#get-eth-data",
    "href": "PS/PS01_albert.html#get-eth-data",
    "title": "Problem Set 01",
    "section": "Get ETH data",
    "text": "Get ETH data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\neth_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(eth_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\neth_df = df.set_index('Timestamp')\neth_df.rename(columns={\"Price\": \"ETH\"}, inplace = True)\n\neth_df.info()\neth_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 05:28:41\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ETH     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nETH\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n1851.050671\n\n\n2023-04-10\n1859.940387\n\n\n2023-04-11\n1909.882061\n\n\n2023-04-12\n1892.938911\n\n\n2023-04-13\n1920.223031"
  },
  {
    "objectID": "PS/PS01_albert.html#get-sol-data",
    "href": "PS/PS01_albert.html#get-sol-data",
    "title": "Problem Set 01",
    "section": "Get SOL data",
    "text": "Get SOL data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/solana/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nsol_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(sol_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nsol_df = df.set_index('Timestamp')\nsol_df.rename(columns={\"Price\": \"SOL\"}, inplace = True)\n\nsol_df.info()\nsol_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:22:21\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   SOL     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nSOL\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n20.042581\n\n\n2023-04-10\n20.304337\n\n\n2023-04-11\n20.875123\n\n\n2023-04-12\n23.005655\n\n\n2023-04-13\n23.830533\n\n\n\n\n\n\n\n\n# Merge all three data frames and save to csv\n# Export the DataFrame to a CSV file\ncrypto_df = btc_df.join(eth_df, how = 'inner').join(sol_df, how = 'inner')\ncrypto_df.info()\ncrypto_df.to_csv(\"crypto_price_data.csv\", index=False)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 180 entries, 2023-04-09 to 2023-10-05\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     180 non-null    float64\n 1   ETH     180 non-null    float64\n 2   SOL     180 non-null    float64\ndtypes: float64(3)\nmemory usage: 5.6 KB\n\n\n\ncrypto_df.head()\n\n\n\n\n\n\n\n\nBTC\nETH\nSOL\n\n\nTimestamp\n\n\n\n\n\n\n\n2023-04-09\n27968.128047\n1851.050671\n20.042581\n\n\n2023-04-10\n28351.236994\n1859.940387\n20.304337\n\n\n2023-04-11\n29657.974137\n1909.882061\n20.875123\n\n\n2023-04-12\n30260.936109\n1892.938911\n23.005655\n\n\n2023-04-13\n29904.138695\n1920.223031\n23.830533"
  },
  {
    "objectID": "PS/PS01.html",
    "href": "PS/PS01.html",
    "title": "Problem Set 01",
    "section": "",
    "text": "Name:\nField that you’re interested in applying TS and forecasting to (ecology, econ, weather, etc):\n\n\nObtain a CSV of time series data\n\nBy whatever means, get a .csv file of time series data relating to any topic: ecological, financial, etc.\nThere should be at least three variables of data\nIf you download from the web, include a link. If you use ChatGPT, include a link to your shared search\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport requests\nimport pandas as pd\n\n\n\n1. Time series plots\n\nPlot the raw time series data\nIdentify any obvious patterns in your time series\nIdentify, if any, interesting insights these patterns suggest\n\n\n\n2. Scatterplots\n\nPlot a 3x3 grid of all pairwise scatterplots\nIdentify, if any, interesting trends or insight\n\n\n\n3. Lag & autocorrelation plots\n\nPlot a lag plot of any variable of your choice to visualize its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nPlot an autocorrelation plot of this same variable to quantify its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nIdentify, if any, interesting insight from these lag plots"
  },
  {
    "objectID": "PS.html",
    "href": "PS.html",
    "title": "Problem Sets",
    "section": "",
    "text": "Slides on PS02 data’s context. Prof Kim quote: “Numbers are numbers, but data has context”\nMy thoughts about ChatGPT. It is a very powerful tool, but you need to be:\n\nKnowledgeable enough to understand the code\nExperienced enough in your domain and programming to know what to prompt\nDisciplined enough to sanity check your results\n\n\n\n\n\nPosted on Slack under #general\nIndividual PS02.ipynb files\n\nComplete PS02.ipynb by Tue 10/24 class time\nDue Thu 10/26 9:25am on Moodle\n\nIn-class on Tue 10/24: “Think, Pair, Share” exercise\n\nClarifications:\n\nUse the dbh variable, not the dbh_orig variable.\nD’oh! Say you do 5 queries on ChatGPT and then get the share URL link. Now saw you do a 6th query. The share URL does NOT reflect this 6th search. So you will need to get the share URL link AFTER you’re done your PS02.\n\n\n\n\n\nGroups still made at random, presenters still chosen at random\nNo need to merge two files, you can select only one person’s and modify it (if at all)\nReminder: submission due on Moodle before next class\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\n 1 Xu             Xiaoman        1\n 2 An             Rachael        1\n 3 Pu             Betty          2 ***\n 4 Qiu            Chi            2 ***\n 5 Tha Ra Wun     Tint           3\n 6 Martin         Teddy          3\n 7 Knecht         Beata          4\n 8 Murray         Kiera          4\n 9 O'Meara        Abbey          5 ***\n10 Yang           Christy        5 ***\n11 Basnet Chettri Charavee       6\n12 Huang          Juniper        6\n13 Ding           Jenny          7\n14 Slosser        Tillie         7\n15 Kogalovski     Aleksandra     8\n16 Anesko         Greta          8\n\n\n\n\nBetty + Chi and Abbey + Chrity solutions here\nProf Kim’s solution"
  },
  {
    "objectID": "PS.html#instructions",
    "href": "PS.html#instructions",
    "title": "Problem Sets",
    "section": "",
    "text": "Posted on Slack under #general\nIndividual PS02.ipynb files\n\nComplete PS02.ipynb by Tue 10/24 class time\nDue Thu 10/26 9:25am on Moodle\n\nIn-class on Tue 10/24: “Think, Pair, Share” exercise\n\nClarifications:\n\nUse the dbh variable, not the dbh_orig variable.\nD’oh! Say you do 5 queries on ChatGPT and then get the share URL link. Now saw you do a 6th query. The share URL does NOT reflect this 6th search. So you will need to get the share URL link AFTER you’re done your PS02."
  },
  {
    "objectID": "PS.html#in-class-presentations",
    "href": "PS.html#in-class-presentations",
    "title": "Problem Sets",
    "section": "",
    "text": "Groups still made at random, presenters still chosen at random\nNo need to merge two files, you can select only one person’s and modify it (if at all)\nReminder: submission due on Moodle before next class\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\n 1 Xu             Xiaoman        1\n 2 An             Rachael        1\n 3 Pu             Betty          2 ***\n 4 Qiu            Chi            2 ***\n 5 Tha Ra Wun     Tint           3\n 6 Martin         Teddy          3\n 7 Knecht         Beata          4\n 8 Murray         Kiera          4\n 9 O'Meara        Abbey          5 ***\n10 Yang           Christy        5 ***\n11 Basnet Chettri Charavee       6\n12 Huang          Juniper        6\n13 Ding           Jenny          7\n14 Slosser        Tillie         7\n15 Kogalovski     Aleksandra     8\n16 Anesko         Greta          8"
  },
  {
    "objectID": "PS.html#example-solutions",
    "href": "PS.html#example-solutions",
    "title": "Problem Sets",
    "section": "",
    "text": "Betty + Chi and Abbey + Chrity solutions here\nProf Kim’s solution"
  },
  {
    "objectID": "PS.html#instructions-1",
    "href": "PS.html#instructions-1",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\n\nPosted on Slack under #general\nIndividual PS01.ipynb files\n\nDue Thu 10/5 9:25am on moodle (see Moodle link on top right of page)\nSubmit both a PS01.ipynb where all code is reproducible and a .csv file of your data\n\nIn-class on Thu 10/5: “Think, Pair, Share” exercise\n\nI will randomly create teams of pairs. Any remaining odd number student will be paired with me.\nYou will show each other your code and prepare a single mini-presentation .ipynb\nI will pick 2-3 pairs at random to present their work in front of the class\nYou will rate your peer’s preparation using this Google Form\n\n\nClarifications added afterwards:\n\nDataset should be at least 100 rows\nFor mini-presentation, you will have to choose one of the two datasets\nDo problem set in python"
  },
  {
    "objectID": "PS.html#in-class-presentations-1",
    "href": "PS.html#in-class-presentations-1",
    "title": "Problem Sets",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nI will make groups at random using code below. I will join a group if there is an odd person left out.\nWork together for 20 minutes to come up with single .ipynb\nI will pick two teams at random to present to the class\nI will present my work to the class. click here\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form Code to generate groups:\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\n 1 Yang           Christy        1\n 2 O'Meara        Abbey          1\n 3 Slosser        Tillie         2 ***\n 4 Murray         Kiera          2 ***\n 5 Khan           Nubraz         3\n 6 Ding           Jenny          3\n 7 Anesko         Greta          4\n 8 Martin         Teddy          4\n 9 Xu             Xiaoman        5\n10 An             Rachael        5\n11 Kogalovski     Aleksandra     6\n12 Qiu            Chi            6\n13 Knecht         Beata          7 ***\n14 Basnet Chettri Charavee       7 ***\n15 Tha Ra Wun     Tint           8\n16 Huang          Juniper        8\n17 Pu             Betty          9"
  },
  {
    "objectID": "PS.html#example-solutions-1",
    "href": "PS.html#example-solutions-1",
    "title": "Problem Sets",
    "section": "Example Solutions",
    "text": "Example Solutions\n\nTillie + Kiera and Beata + Charavee’s solutions here\nProf Kim’s solution"
  }
]