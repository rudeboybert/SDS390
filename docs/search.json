[
  {
    "objectID": "PS.html#instructions",
    "href": "PS.html#instructions",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\n\nPosted on Slack under #general\nIndividual PS04.ipynb files\n\nComplete PS04.ipynb by Thu 12/7 class time\nDue Tue 12/12 9:25am on Moodle\n\nIn-class on Thu 12/7: “Think, Pair, Share” exercise\n\nClarifications:"
  },
  {
    "objectID": "PS.html#in-class-presentations",
    "href": "PS.html#in-class-presentations",
    "title": "Problem Sets",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nIf you can’t make it to lecture, it is your responsibility to send a screencast recording of you narrating your work by the end of the day.\nGroups still made at random, presenters still chosen at random\nNo need to merge two files, you can select only one person’s and modify it (if at all)\nReminder: submission due on Moodle before next class\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented"
  },
  {
    "objectID": "PS.html#example-solutions",
    "href": "PS.html#example-solutions",
    "title": "Problem Sets",
    "section": "Example Solutions",
    "text": "Example Solutions"
  },
  {
    "objectID": "PS.html#instructions-1",
    "href": "PS.html#instructions-1",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\n\nPosted on Slack under #general\nIndividual PS03.ipynb files\n\nComplete PS03.ipynb by Tue 11/14 class time\nDue Thu 11/16 9:25am on Moodle\n\nIn-class on Tue 11/14: “Think, Pair, Share” exercise\n\nClarifications:"
  },
  {
    "objectID": "PS.html#in-class-presentations-1",
    "href": "PS.html#in-class-presentations-1",
    "title": "Problem Sets",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nGroups still made at random, presenters still chosen at random\nNo need to merge two files, you can select only one person’s and modify it (if at all)\nReminder: submission due on Moodle before next class\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\nBasnet Chettri Charavee       1 ***\nAn             Rachael        1 ***\nSlosser        Tillie         2\nO'Meara        Abbey          2\nHuang          Juniper        3 ***\nMurray         Kiera          3 ***\nTha Ra Wun     Tint           4  \nYang           Christy        4\nXu             Xiaoman        5\nPu             Betty          5\nKnecht         Beata          6\nQiu            Chi            6\nMartin         Teddy          X1\nKogalovski     Aleksandra     X1\nDing           Jenny          X2\nAnesko         Greta          X2\nKhan           Nubraz         X3"
  },
  {
    "objectID": "PS.html#example-solutions-1",
    "href": "PS.html#example-solutions-1",
    "title": "Problem Sets",
    "section": "Example Solutions",
    "text": "Example Solutions\n\nPresentations:\n\nCharavee + Rachel solutions\nKiera + Juniper solutions\n.zip file of both here\n\nProf Kim’s solution. Note a couple of things I did have time to address b/c of my conference last week:\n\nI didn’t polish my graphs. (An example of bad role modeling on my part: “Do as I say, not do as I do.” Apologies)\nI realized I calculated the seasonal naive forecast wrong! Where as you were supposed to using only the prior year’s worth of seasons (i.e. 4 time points each corresponding to the last four quarters), I computed the average for all past quarters! i.e. all springs, all summers, etc. And thus my solution was more complicated than necessary."
  },
  {
    "objectID": "PS.html#instructions-2",
    "href": "PS.html#instructions-2",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\n\nPosted on Slack under #general\nIndividual PS02.ipynb files\n\nComplete PS02.ipynb by Tue 10/24 class time\nDue Thu 10/26 9:25am on Moodle\n\nIn-class on Tue 10/24: “Think, Pair, Share” exercise\n\nClarifications:\n\nUse the dbh variable, not the dbh_orig variable.\nD’oh! Say you do 5 queries on ChatGPT and then get the share URL link. Now saw you do a 6th query. The share URL does NOT reflect this 6th search. So you will need to get the share URL link AFTER you’re done your PS02."
  },
  {
    "objectID": "PS.html#in-class-presentations-2",
    "href": "PS.html#in-class-presentations-2",
    "title": "Problem Sets",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nGroups still made at random, presenters still chosen at random\nNo need to merge two files, you can select only one person’s and modify it (if at all)\nReminder: submission due on Moodle before next class\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\n 1 Xu             Xiaoman        1\n 2 An             Rachael        1\n 3 Pu             Betty          2 ***\n 4 Qiu            Chi            2 ***\n 5 Tha Ra Wun     Tint           3\n 6 Martin         Teddy          3\n 7 Knecht         Beata          4\n 8 Murray         Kiera          4\n 9 O'Meara        Abbey          5 ***\n10 Yang           Christy        5 ***\n11 Basnet Chettri Charavee       6\n12 Huang          Juniper        6\n13 Ding           Jenny          7\n14 Slosser        Tillie         7\n15 Kogalovski     Aleksandra     8\n16 Anesko         Greta          8"
  },
  {
    "objectID": "PS.html#example-solutions-2",
    "href": "PS.html#example-solutions-2",
    "title": "Problem Sets",
    "section": "Example Solutions",
    "text": "Example Solutions\n\nPresentations:\n\nBetty + Chi solutions\nAbbey + Christy solutions\n.zip file of both here\n\nProf Kim’s solution"
  },
  {
    "objectID": "PS.html#instructions-3",
    "href": "PS.html#instructions-3",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\n\nPosted on Slack under #general\nIndividual PS01.ipynb files\n\nDue Thu 10/5 9:25am on moodle (see Moodle link on top right of page)\nSubmit both a PS01.ipynb where all code is reproducible and a .csv file of your data\n\nIn-class on Thu 10/5: “Think, Pair, Share” exercise\n\nI will randomly create teams of pairs. Any remaining odd number student will be paired with me.\nYou will show each other your code and prepare a single mini-presentation .ipynb\nI will pick 2-3 pairs at random to present their work in front of the class\nYou will rate your peer’s preparation using this Google Form\n\n\nClarifications added afterwards:\n\nDataset should be at least 100 rows\nFor mini-presentation, you will have to choose one of the two datasets\nDo problem set in python"
  },
  {
    "objectID": "PS.html#in-class-presentations-3",
    "href": "PS.html#in-class-presentations-3",
    "title": "Problem Sets",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nI will make groups at random using code below. I will join a group if there is an odd person left out.\nWork together for 20 minutes to come up with single .ipynb\nI will pick two teams at random to present to the class\nI will present my work to the class. click here\nAnswer the sli.do poll here\nRate your peer’s preparation using this Google Form Code to generate groups:\n\n\nlibrary(tidyverse)\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_frac(1) %&gt;% \n  mutate(team = rep(1:9, length = 17)) %&gt;% \n  arrange(team)\n\nread_csv(\"390.csv\") %&gt;% \n  filter(First != \"Albert\") %&gt;% \n  sample_n(2)\n\n*** = presented\n 1 Yang           Christy        1\n 2 O'Meara        Abbey          1\n 3 Slosser        Tillie         2 ***\n 4 Murray         Kiera          2 ***\n 5 Khan           Nubraz         3\n 6 Ding           Jenny          3\n 7 Anesko         Greta          4\n 8 Martin         Teddy          4\n 9 Xu             Xiaoman        5\n10 An             Rachael        5\n11 Kogalovski     Aleksandra     6\n12 Qiu            Chi            6\n13 Knecht         Beata          7 ***\n14 Basnet Chettri Charavee       7 ***\n15 Tha Ra Wun     Tint           8\n16 Huang          Juniper        8\n17 Pu             Betty          9"
  },
  {
    "objectID": "PS.html#example-solutions-3",
    "href": "PS.html#example-solutions-3",
    "title": "Problem Sets",
    "section": "Example Solutions",
    "text": "Example Solutions\n\nPresentations:\n\nTillie + Kiera solutions\nBeata + Charavee solutions\n.zip file of both here\n\nProf Kim’s solution"
  },
  {
    "objectID": "PS/PS03_presentations/PS03_charavee_rachel.html",
    "href": "PS/PS03_presentations/PS03_charavee_rachel.html",
    "title": "Problem Set 03",
    "section": "",
    "text": "Instructions: 1. Name: Charavee Basnet Chettri 1. Create a new ChatGPT chat called “SDS390 PS03” that contains all the prompts you used for this problem set. After you’ve completed your assignment, click the share button and paste the URL in Markdown format here (shared ChatGPT links don’t auto-update with subsequent queries).\nOverview of this PS: You will be recreating graphs and explicitly computing values from “FPP 5 - The forecaster’s toolbox” in python. Specifically\n\nFPP 5.2 - Some simple forecasting methods: Recreate Fig 5.7\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\nFPP 5.8 - Evaluating point forecast accuracy: Recompute 4 RMSE values in table using data wrangling\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\n\nOverall instructions:\n\nDo not use a function from a specialized time series forecasting specific python library to do this PS. Rather use pandas data wrangling, matplotlib, or any other package we’ve used to date (like for autocorrelation functions)\nDepending on your data wrangling approach for the questions below, you may get a warning that says “A value is trying to be set on a copy of a slice from a DataFrame.” As long as your values are correct, you may ignore this warning\nUsing the lessons you learned in the 3rd DataCamp on Data Viz, Chapters 1 and 2:\n\nGive all your plots titles\nLabel all axes\nMake any other cosmetic changes you like\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nimport math\n\nimport statsmodels.api as sm\nfrom pylab import rcParams\n\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\nimport numpy as np\n\n\nLoad and explore data\n\naus_production = pd.read_csv(\"PS03_aus_production.csv\", parse_dates = ['Quarter'], index_col = 'Quarter')\n# aus_production = aus_production.to_period('Q')\nprint(aus_production.head())\nprint(\"\\n\")\nprint(aus_production.tail())\n\n            Beer\nQuarter         \n1992-01-01   443\n1992-04-01   410\n1992-07-01   420\n1992-10-01   532\n1993-01-01   433\n\n\n            Beer\nQuarter         \n2009-04-01   398\n2009-07-01   419\n2009-10-01   488\n2010-01-01   414\n2010-04-01   374\n\n\n\n\nFPP 5.2: Recreate Fig 5.7\n\nRecall from Lec 7.2 this requires training (1992 Q1 - 2006 Q4) vs test set (2007 Q1 - 2010 Q2) splitting of data\n\n\n#creating Train and Test set \ntrain_main = aus_production.loc['1992-01-01':'2006-10-01']\ntest_main = aus_production.loc['2007-01-01':'2010-04-01']\n\n#solving: “A value is trying to be set on a copy of a slice from a DataFrame\"\npd.options.mode.chained_assignment = None\n\n\ntrain = train_main.copy()\ntest = test_main.copy()\n\n\n#mean method \nprint(train.mean())\ntest['Mean']= 436.45\n\nBeer    436.45\ndtype: float64\n\n\n\n#naive method\nprint(train.tail(1))\ntest['Naive']= 491\ntest\n\n            Beer\nQuarter         \n2006-10-01   491\n\n\n\n\n\n\n\n\n\nBeer\nMean\nNaive\n\n\nQuarter\n\n\n\n\n\n\n\n2007-01-01\n427\n436.45\n491\n\n\n2007-04-01\n383\n436.45\n491\n\n\n2007-07-01\n394\n436.45\n491\n\n\n2007-10-01\n473\n436.45\n491\n\n\n2008-01-01\n420\n436.45\n491\n\n\n2008-04-01\n390\n436.45\n491\n\n\n2008-07-01\n410\n436.45\n491\n\n\n2008-10-01\n488\n436.45\n491\n\n\n2009-01-01\n415\n436.45\n491\n\n\n2009-04-01\n398\n436.45\n491\n\n\n2009-07-01\n419\n436.45\n491\n\n\n2009-10-01\n488\n436.45\n491\n\n\n2010-01-01\n414\n436.45\n491\n\n\n2010-04-01\n374\n436.45\n491\n\n\n\n\n\n\n\n\n#this is a function that when called performs the seasonal naive method; citation in the Sources.\n#The explanation what each line does given by myself.\n\ndef pysnaive(train_series,seasonal_periods,forecast_horizon):\n        \n        last_season=train_series.iloc[-seasonal_periods:]\n        #This line extracts the last 4 elements from the train set. \n        #This represents the last observed season in the training data\n        \n        reps=int(np.ceil(forecast_horizon/seasonal_periods)) \n        #This line calculates number of repetitions (`reps`) needed to cover the entire forecast horizon. \n        #The np.ceil function ensures that even if the division doesn't result in an exact integer, \n        #it is rounded up to the nearest integer\n        \n        fcarray=np.tile(last_season,reps)\n        #Use np.tile to repeat the last season array (last_season) `reps` times, \n        #creating an array (fcarray) that covers the entire forecast horizon\n        \n        fcast=pd.Series(fcarray[:forecast_horizon])\n        #Create a pandas Series (fcast) by taking the first `forecast_horizon` elements from the repeated array. \n        #This represents the forecasted values\n             \n        return fcast\n\n\n#calling the above defined function\nsnaive = pysnaive(train[\"Beer\"], \n                seasonal_periods=4,\n                forecast_horizon=14)\n\n#mutating to the test dataset \ntest[\"Seasonal Naive\"] = snaive.values \n\n\nplt.plot(test)\nplt.plot(test['Beer'])\nplt.legend(test,loc='upper right')\nplt.plot(train)\nplt.title('Fig.1 Quarterly Beer Production in Australia')\nplt.xlabel('Quarter')\nplt.ylabel('# of Beers')\nplt.show()\n\n\n\n\n\n\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\n\nCompute the three residuals diagnostics plot in Fig 5.13 for the naive method forecasts\nDo this for the training data in the Beer data above. This is because when doing a residual diagnostic of a model, you want to compare observed values \\(y_t\\) and fitted / predicted / forecasted values \\(\\hat{y}_t\\) for the data you used to fit the model.\nIt’s already obvious that there are much better choices than the naive method for forecasting. For each of the three residual diagnostics plots:\n\nComment on the residuals pattern you observe\nExplain why the pattern you observe is consistent with the fact that there are much better choices than the naive method for forecasting\n\n\n\nresid = test_main.copy()\nresid['Beer_hat'] = 491\nresid['Residual'] = resid['Beer'].sub(resid['Beer_hat'])\nresid_reset = resid.reset_index()\n\n\nresid_reset\n\n\n\n\n\n\n\n\nQuarter\nBeer\nBeer_hat\nResidual\n\n\n\n\n0\n2007-01-01\n427\n491\n-64\n\n\n1\n2007-04-01\n383\n491\n-108\n\n\n2\n2007-07-01\n394\n491\n-97\n\n\n3\n2007-10-01\n473\n491\n-18\n\n\n4\n2008-01-01\n420\n491\n-71\n\n\n5\n2008-04-01\n390\n491\n-101\n\n\n6\n2008-07-01\n410\n491\n-81\n\n\n7\n2008-10-01\n488\n491\n-3\n\n\n8\n2009-01-01\n415\n491\n-76\n\n\n9\n2009-04-01\n398\n491\n-93\n\n\n10\n2009-07-01\n419\n491\n-72\n\n\n11\n2009-10-01\n488\n491\n-3\n\n\n12\n2010-01-01\n414\n491\n-77\n\n\n13\n2010-04-01\n374\n491\n-117\n\n\n\n\n\n\n\n\n#residuals v. quarter\nplt.plot(resid_reset['Quarter'], resid_reset['Residual'], marker='o', linestyle='-')\nplt.title('Residuals vs. Quarter')\nplt.xlabel('Quarter')\nplt.ylabel('# of Beers')\nplt.ylim(-120, 120)\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n#residual autocorrelation plot\nplot_acf(resid_reset['Residual'], alpha=0.5, lags=13)\nplt.show()\n\n\n\n\n\n#histogram of residuals\nresid_reset['Residual'].hist(bins=7, alpha=0.7)\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Histogram of Residuals')\nplt.show()\n\n\n\n\n\nObservation + Explanation\nThe results of residual diagnostics show that there may be better choices than the naive method of forecasting.\nIn the first plot, we observe that the residuals range from -117 to -3. This observation shows that the naive method does not account for all the trend signals of the data. As seen in the first plot and the histogram, most of the residuals are negative, which means that the naive method overestimates. It is intuitive given how the method forecasts 491 for dips in beer production.\nIn the second plot, we observe that residuals are correlated with the past lagged values of the residuals. We know that the autocorrelation with lag zero always equals 1 because this represents the autocorrelation between the term and itself. As discussed in class, it is important to note that there are not enough data points for residual analysis. With the available information, this autocorrelation plot suggests that the residuals do not follow a random pattern. Firstly, we observe a positive autocorrelation between the residual from the 4th quarter with seasonality every 4 quarters. Interestingly, Australia experiences the summer around this quarter. Secondly, we observe a negative correlation between the residual from the 2nd quarter with seasonality every 4 quarters from that point onwards. Interestingly again, we observe that the 2nd quarter sees the biggest decreases in beer production since this is around the fall. This repetitive up-and-down component displays seasonality the forecasting method does not capture.\nIn the thrid plot, we observe that the residuals do not follow a normal distribution. Therefore, the prediction intervals from the naive forecast will be inaccurate which does not capture all of the potential values that Beer production may take.\nEven visually in the Fig 1, we observe that the naive forecast of 491, does not capture the seasonality (or the highs and lows) associated with the beer production.\n\n\n\nFPP 5.8 - Evaluating point forecast accuracy: Recompute RMSE and MAE values in table using data wrangling\nFor the three forecasting methods above, compute the:\n\nRoot mean squared error\nMean absolute error\n\nwhich gives a single numerical measure of the overall error of the model.\nNote that the table in FPP 5.8 uses a slightly different training set 1992 Q1 - 2007 Q4, instead of 1992 Q1 - 2006 Q4, so you will get slightly different error values, but they should still be close.\n\ntest['res_mean']= test['Beer'].sub(test['Mean'])\ntest['res_naive']= test['Beer'].sub(test['Naive'])\ntest['res_snaive']= test ['Beer'].sub(test['Seasonal Naive'])\n\n\n#Calculating the MAE for each method\nmae_mean = round(test['res_mean'].abs().mean(),2)\nmae_naive = round (test['res_naive'].abs().mean(),2)\nmae_snaive = round (test['res_snaive'].abs().mean(),2)\n\n#Calculating the RMSE for each method\nrmse_mean = round (math.sqrt(pow(test['res_mean'],2).mean()),2)\nrmse_naive = round (math.sqrt(pow(test['res_naive'],2).mean()),2)\nrmse_snaive = round (math.sqrt(pow(test['res_snaive'],2).mean()),2)\n\ntable = {\n    'Method': ['Mean', 'Naive', 'Seasonal Naive'],\n    'RMSE': [rmse_mean, rmse_naive, rmse_snaive],\n    'MAE': [mae_mean, mae_naive, mae_snaive]\n}\n\ntable = pd.DataFrame(table)\n\ntable\n\n\n\n\n\n\n\n\nMethod\nRMSE\nMAE\n\n\n\n\n0\nMean\n38.89\n35.47\n\n\n1\nNaive\n78.62\n70.07\n\n\n2\nSeasonal Naive\n13.49\n11.50\n\n\n\n\n\n\n\nSince RMSE and MAE of the Seasonal Naive forecasting method are the lowest relative to the other methods, we can conclude that the Seasonal Naive method will deliver the highest accuracy compared to the other methods.\n\n\nSources\n\nSeasonal Naive Method Function https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/fbprophet/ensemble_forecast/uncertainty/simulation/2020/04/21/timeseries-part2.html#Seasonal-Naive-Forecast-model\nRMSE Calculation https://www.javatpoint.com/rsme-root-mean-square-error-in-python\nMAE Calculation https://www.geeksforgeeks.org/how-to-calculate-mean-absolute-error-in-python/\n\n\n\nAppendix\n\n# #this is a function that when called performs the seasonal naive method\n# def pysnaive(train_series,seasonal_periods,forecast_horizon):\n#     '''\n#     Python implementation of Seasonal Naive Forecast. \n#     This should work similar to https://otexts.com/fpp2/simple-methods.html\n#     Returns two arrays\n#      &gt; fitted: Values fitted to the training dataset\n#      &gt; fcast: seasonal naive forecast\n    \n#     Author: Sandeep Pawar\n    \n#     Date: Apr 9, 2020\n    \n#     Ver: 1.0\n    \n#     train_series: Pandas Series\n#         Training Series to be used for forecasting. This should be a valid Pandas Series. \n#         Length of the Training set should be greater than or equal to number of seasonal periods\n        \n#     Seasonal_periods: int\n#         No of seasonal periods\n#         Yearly=1\n#         Quarterly=4\n#         Monthly=12\n#         Weekly=52\n        \n\n#     Forecast_horizon: int\n#         Number of values to forecast into the future\n    \n#     e.g. \n#     fitted_values = pysnaive(train,12,12)[0]\n#     fcast_values = pysnaive(train,12,12)[1]\n#     '''\n    \n#     if len(train_series)&gt;= seasonal_periods: #checking if there are enough observations in the training data\n        \n#         last_season=train_series.iloc[-seasonal_periods:]\n#         #This line extracts the last 4 elements from the train set. \n#         #This represents the last observed season in the training data\n        \n#         reps=int(np.ceil(forecast_horizon/seasonal_periods)) \n#         #This line calculates number of repetitions (`reps`) needed to cover the entire forecast horizon. \n#         #The np.ceil function ensures that even if the division doesn't result in an exact integer, \n#         #it is rounded up to the nearest integer\n        \n#         fcarray=np.tile(last_season,reps)\n#         #Use np.tile to repeat the last season array (last_season) `reps` times, \n#         #creating an array (fcarray) that covers the entire forecast horizon\n        \n#         fcast=pd.Series(fcarray[:forecast_horizon])\n#         #Create a pandas Series (fcast) by taking the first forecast_horizon elements from the repeated array. \n#         #This represents the forecasted values\n        \n#         fitted = train_series.shift(seasonal_periods)\n#         #Create a Series (fitted) representing the fitted values by shifting the train_series by seasonal_periods. \n#         #This is commonly done in time series analysis to represent a lagged version of the series.\n#     else:\n#         fcast=print(\"Length of the trainining set must be greater than number of seasonal periods\") \n    \n#     return fitted, fcast\n\n\n# #Fitted values\n# py_snaive_fit = pysnaive(train[\"Beer\"], \n#                      seasonal_periods=4,\n#                      forecast_horizon=14)[0]\n\n# #forecast\n# py_snaive = pysnaive(train[\"Beer\"], \n#                      seasonal_periods=4,\n#                      forecast_horizon=14)[1]\n\n# #Residuals\n# py_snaive_resid = (train[\"Beer\"] - py_snaive_fit).dropna()\n\n# test[\"Seasonal Naive\"] = py_snaive.values \n\n\n# # Calculating RMSE and MAE with package\n\n# error= test.copy()\n\n# #rmse and mae for mean\n\n# mse_mean = sklearn.metrics.mean_squared_error(error['Beer'], error['Mean'])  \n# rmse_mean = math.sqrt(mse_mean) \n\n# mae_mean = sklearn.metrics.mean_absolute_error(error['Beer'], error['Mean'])\n\n# #rmse and mae for naive\n# mse_naive = sklearn.metrics.mean_squared_error(error['Beer'], error['Naive'])  \n# rmse_naive= math.sqrt(mse_naive) \n\n# mae_naive = sklearn.metrics.mean_absolute_error(error['Beer'], error['Naive'])\n\n# #rmse and mae for seasonal naive\n# mse_snaive = sklearn.metrics.mean_squared_error(error['Beer'], error['Seasonal Naive'])  \n# rmse_snaive= math.sqrt(mse_snaive) \n\n# mae_snaive = sklearn.metrics.mean_absolute_error(error['Beer'], error['Seasonal Naive'])\n\n# table = {\n#     'Method': ['Mean', 'Naive', 'Seasonal Naive'],\n#     'RMSE': [rmse_mean, rmse_naive, rmse_snaive],\n#     'MAE': [mae_mean, mae_mean, mae_mean]\n# }\n\n# table = pd.DataFrame(table)\n\n# print(table)\n\nNameError: name 'sklearn' is not defined"
  },
  {
    "objectID": "PS/PS03.html",
    "href": "PS/PS03.html",
    "title": "Problem Set 03",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS03” that contains all the prompts you used for this problem set. After you’ve completed your assignment, click the share button and paste the URL in Markdown format here (shared ChatGPT links don’t auto-update with subsequent queries).\nOverview of this PS: You will be recreating graphs and explicitly computing values from “FPP 5 - The forecaster’s toolbox” in python. Specifically\n\nFPP 5.2 - Some simple forecasting methods: Recreate Fig 5.7\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\nFPP 5.8 - Evaluating point forecast accuracy: Recompute the 3 RMSE and 3 MAE values (corresponding to naive, mean, and seasonal naive methods) in table using data wrangling\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\n\nOverall instructions:\n\nDo not use a function from a specialized time series forecasting specific python library to do this PS. Rather use pandas data wrangling, matplotlib, or any other package we’ve used to date (like for autocorrelation functions)\nDepending on your data wrangling approach for the questions below, you may get a warning that says A value is trying to be set on a copy of a slice from a DataFrame. As long as your values are correct, you may ignore this warning\nUsing the lessons you learned in the 3rd DataCamp on Data Viz, Chapters 1 and 2:\n\nGive all your plots titles\nLabel all axes\nMake any other cosmetic changes you like\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nLoad and explore data\n\naus_production = pd.read_csv(\"PS03_aus_production.csv\", parse_dates = ['Quarter'], index_col = 'Quarter')\n# aus_production = aus_production.to_period('Q')\nprint(aus_production.head())\nprint(\"\\n\")\nprint(aus_production.tail())\n\n            Beer\nQuarter         \n1992-01-01   443\n1992-04-01   410\n1992-07-01   420\n1992-10-01   532\n1993-01-01   433\n\n\n            Beer\nQuarter         \n2009-04-01   398\n2009-07-01   419\n2009-10-01   488\n2010-01-01   414\n2010-04-01   374\n\n\n\n\nFPP 5.2: Recreate Fig 5.7\n\nRecall from Lec 7.2 this requires training (1992 Q1 - 2006 Q4) vs test set (2007 Q1 - 2010 Q2) splitting of data\n\n\n\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\n\nCompute the three residuals diagnostics plot in Fig 5.13 for the naive method forecasts\nDo this for the training data in the Beer data above. This is because when doing a residual diagnostic of a model, you want to compare observed values \\(y_t\\) and fitted / predicted / forecasted values \\(\\hat{y}_t\\) for the data you used to fit the model.\nIt’s already obvious that there are much better choices than the naive method for forecasting. For each of the three residual diagnostics plots:\n\nComment on the residuals pattern you observe\nExplain why the pattern you observe is consistent with the fact that there are much better choices than the naive method for forecasting\n\n\n\n\nFPP 5.8 - Evaluating point forecast accuracy: Recompute RMSE and MAE values in table using data wrangling\nFor the three forecasting methods above, compute the:\n\nRoot mean squared error\nMean absolute error\n\nwhich gives a single numerical measure of the overall error of the model.\nNote that the table in FPP 5.8 uses a slightly different training set 1992 Q1 - 2007 Q4, instead of 1992 Q1 - 2006 Q4, so you will get slightly different error values, but they should still be close.\n\n\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\nI didn’t have time to scaffold this question appropriately."
  },
  {
    "objectID": "PS/final_project.html",
    "href": "PS/final_project.html",
    "title": "Final Project",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “Final Project” that contains all the prompts you used for this problem set. After you’ve completed your assignment, click the share button and paste the URL in Markdown format here (shared ChatGPT links don’t auto-update with subsequent queries).\n\nExecutive Summary\n\nTake home message\n\n\n\nIntroduction\n\nContext of data\nsource of data\nany research questions\n\n\n\nExploratory Data Analysis\n\nData wrangling\nLook at your data before fitting any models\n\n\n\nForecasting and Modeling\nFirst: - Specify and estimate forecasts using between 3-4 different models - Visualize forecasts - Evaluate the forecasting accuracy of all models and compare\nThen: - Choose a model that you think is best; justify this choice - Do a residual analysis - Forecast into the future using the model of your choice\n\n\nAppendix\n\nOptional: only if you feel like sharing a laugh or something cool\nStuff you tried that maybe that didn’t work? Failure is a necessary condition to success thus we should encourage failures as much as we encourage successes"
  },
  {
    "objectID": "PS/PS02.html",
    "href": "PS/PS02.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n1. Load and explore data\n\nLoad the CSV data of biweekly dendroband measures for tree with tag 82203\nLook at the raw data (rows and variables) and meta-data as Amir always told me to do\nNote that DBH is the only non-index variable we will work with, thus simplify the data frame to only have this variable\nPlot the time series of DBH (diameter at breast height, in mm) as we’ve done numerous times in DataCamp: just using .plot() with no arguments\nNow plot the time series where each of the 147 observed data point is marked with a dot. In other words, your plot should at the very least have these points.\nWhat is the difference in information presented between the two plots?\nDescribe any patterns you observe in the time series where each observed data point is marked with a dot\n\n\n\n2. Decomposition Plots\nCreate a seasonal decomposition plot of this data to answer the following questions: 1. What is the approximate growth (in mm) per year trend for this tree 1. What is the approximate range in seasonal variation in growth around this trend? 1. When are the peaks and when are the valleys in this seasonal variation? 1. Are there periods of time where we might be skeptical of the above trend and seasonality?\nNote: You may need to do some data transformation as seen in the first DataCamp course to get a valid plot\n\n\n3. Forecasting using an AR(2) model\nIn this exercise you will use an AR(2) model as seen in the second datacamp course “Time Series Analysis in Python” to forecast this tree’s size exactly two years into the future. Unfortunately since we aren’t covering ARIMA models until later in the course, we won’t be able to:\n\nAscertain the appropriateness of using an AR(2) model. Ex: are all assumptions met?\nFully interpret the model output’s meaning\nDiagnose any issues.\n\nHowever, we will simply view this code as a minimally viable product that we will improve on later. Instructions: 1. Fit an appropriate AR(2) model 1. Print the summary of the result 1. Plot the TS data along with both in-sample fitted forecasts AND out-of-sample forecasts exactly two years into the future 1. Comment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem."
  },
  {
    "objectID": "PS/PS03_albert.html",
    "href": "PS/PS03_albert.html",
    "title": "Problem Set 03",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS03” that contains all the prompts you used for this problem set. After you’ve completed your assignment, click the share button and paste the URL in Markdown format here (shared ChatGPT links don’t auto-update with subsequent queries).\nOverview of this PS: You will be recreating graphs and explicitly computing values from “FPP 5 - The forecaster’s toolbox” in python. Specifically\n\nFPP 5.2 - Some simple forecasting methods: Recreate Fig 5.7\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\nFPP 5.8 - Evaluating point forecast accuracy: Recompute the 3 RMSE and 3 MAE values (corresponding to naive, mean, and seasonal naive methods) in table using data wrangling\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\n\nOverall instructions:\n\nDo not use a function from a specialized time series forecasting specific python library to do this PS. Rather use pandas data wrangling, matplotlib, or any other package we’ve used to date (like for autocorrelation functions)\nDepending on your data wrangling approach for the questions below, you may get a warning that says A value is trying to be set on a copy of a slice from a DataFrame. As long as your values are correct, you may ignore this warning\nUsing the lessons you learned in the 3rd DataCamp on Data Viz, Chapters 1 and 2:\n\nGive all your plots titles\nLabel all axes\nMake any other cosmetic changes you like\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nLoad and explore data\n\naus_production = pd.read_csv(\"PS03_aus_production.csv\", parse_dates = ['Quarter'], index_col = 'Quarter')\n# aus_production = aus_production.to_period('Q')\nprint(aus_production.head())\nprint(\"\\n\")\nprint(aus_production.tail())\n\n            Beer\nQuarter         \n1992-01-01   443\n1992-04-01   410\n1992-07-01   420\n1992-10-01   532\n1993-01-01   433\n\n\n            Beer\nQuarter         \n2009-04-01   398\n2009-07-01   419\n2009-10-01   488\n2010-01-01   414\n2010-04-01   374\n\n\n\n\nFPP 5.2: Recreate Fig 5.7\n\nRecall from Lec 7.2 this requires training (1992 Q1 - 2006 Q4) vs test set (2007 Q1 - 2010 Q2) splitting of data\n\n\n# Create training set and verify dates\ntrain = aus_production.loc[:'2006']\nprint(train.head())\nprint(\"\\n\")\nprint(train.tail())\n\n            Beer\nQuarter         \n1992-01-01   443\n1992-04-01   410\n1992-07-01   420\n1992-10-01   532\n1993-01-01   433\n\n\n            Beer\nQuarter         \n2005-10-01   482\n2006-01-01   438\n2006-04-01   386\n2006-07-01   405\n2006-10-01   491\n\n\n\n# Create test set and verify dates\ntest = aus_production.loc['2007':]\nprint(test.head())\nprint(\"\\n\")\nprint(test.tail())\n\n            Beer\nQuarter         \n2007-01-01   427\n2007-04-01   383\n2007-07-01   394\n2007-10-01   473\n2008-01-01   420\n\n\n            Beer\nQuarter         \n2009-04-01   398\n2009-07-01   419\n2009-10-01   488\n2010-01-01   414\n2010-04-01   374\n\n\n\ntest[\"mean_method\"] = train['Beer'].mean()\ntest[\"naive_method\"] = train.iloc[-1, ].Beer\ntest\n\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/3639978134.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"mean_method\"] = train['Beer'].mean()\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/3639978134.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"naive_method\"] = train.iloc[-1, ].Beer\n\n\n\n\n\n\n\n\n\nBeer\nmean_method\nnaive_method\n\n\nQuarter\n\n\n\n\n\n\n\n2007-01-01\n427\n436.45\n491\n\n\n2007-04-01\n383\n436.45\n491\n\n\n2007-07-01\n394\n436.45\n491\n\n\n2007-10-01\n473\n436.45\n491\n\n\n2008-01-01\n420\n436.45\n491\n\n\n2008-04-01\n390\n436.45\n491\n\n\n2008-07-01\n410\n436.45\n491\n\n\n2008-10-01\n488\n436.45\n491\n\n\n2009-01-01\n415\n436.45\n491\n\n\n2009-04-01\n398\n436.45\n491\n\n\n2009-07-01\n419\n436.45\n491\n\n\n2009-10-01\n488\n436.45\n491\n\n\n2010-01-01\n414\n436.45\n491\n\n\n2010-04-01\n374\n436.45\n491\n\n\n\n\n\n\n\n\ntest['month'] = test.index.month\ntest\n\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/2545785027.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['month'] = test.index.month\n\n\n\n\n\n\n\n\n\nBeer\nmean_method\nnaive_method\nmonth\n\n\nQuarter\n\n\n\n\n\n\n\n\n2007-01-01\n427\n436.45\n491\n1\n\n\n2007-04-01\n383\n436.45\n491\n4\n\n\n2007-07-01\n394\n436.45\n491\n7\n\n\n2007-10-01\n473\n436.45\n491\n10\n\n\n2008-01-01\n420\n436.45\n491\n1\n\n\n2008-04-01\n390\n436.45\n491\n4\n\n\n2008-07-01\n410\n436.45\n491\n7\n\n\n2008-10-01\n488\n436.45\n491\n10\n\n\n2009-01-01\n415\n436.45\n491\n1\n\n\n2009-04-01\n398\n436.45\n491\n4\n\n\n2009-07-01\n419\n436.45\n491\n7\n\n\n2009-10-01\n488\n436.45\n491\n10\n\n\n2010-01-01\n414\n436.45\n491\n1\n\n\n2010-04-01\n374\n436.45\n491\n4\n\n\n\n\n\n\n\n\nseasonal = test.groupby('month').mean()[['Beer']].rename(columns = {'Beer':'seasonal_naive'})\nseasonal\n\n\n\n\n\n\n\n\nseasonal_naive\n\n\nmonth\n\n\n\n\n\n1\n419.000000\n\n\n4\n386.250000\n\n\n7\n407.666667\n\n\n10\n483.000000\n\n\n\n\n\n\n\n\ntest = test.join(seasonal, on = 'month')\n\n\ntest\n\n\n\n\n\n\n\n\nBeer\nmean_method\nnaive_method\nmonth\nseasonal_naive\n\n\nQuarter\n\n\n\n\n\n\n\n\n\n2007-01-01\n427\n436.45\n491\n1\n419.000000\n\n\n2007-04-01\n383\n436.45\n491\n4\n386.250000\n\n\n2007-07-01\n394\n436.45\n491\n7\n407.666667\n\n\n2007-10-01\n473\n436.45\n491\n10\n483.000000\n\n\n2008-01-01\n420\n436.45\n491\n1\n419.000000\n\n\n2008-04-01\n390\n436.45\n491\n4\n386.250000\n\n\n2008-07-01\n410\n436.45\n491\n7\n407.666667\n\n\n2008-10-01\n488\n436.45\n491\n10\n483.000000\n\n\n2009-01-01\n415\n436.45\n491\n1\n419.000000\n\n\n2009-04-01\n398\n436.45\n491\n4\n386.250000\n\n\n2009-07-01\n419\n436.45\n491\n7\n407.666667\n\n\n2009-10-01\n488\n436.45\n491\n10\n483.000000\n\n\n2010-01-01\n414\n436.45\n491\n1\n419.000000\n\n\n2010-04-01\n374\n436.45\n491\n4\n386.250000\n\n\n\n\n\n\n\n\n# Plot what we do have\nplt.figure(figsize=(12, 6))\nplt.plot(train.index, train[\"Beer\"])\nplt.plot(test.index, test[\"Beer\"])\nplt.plot(test.index, test[\"mean_method\"])\nplt.plot(test.index, test[\"naive_method\"])\nplt.plot(test.index, test[\"seasonal_naive\"])\nplt.show()\n\n\n\n\n\n\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\n\nCompute the three residuals diagnostics plot in Fig 5.13 for the naive method forecasts\nDo this for the training data in the Beer data above. This is because when doing a residual diagnostic of a model, you want to compare observed values \\(y_t\\) and fitted / predicted / forecasted values \\(\\hat{y}_t\\) for the data you used to fit the model.\nIt’s already obvious that there are much better choices than the naive method for forecasting. For each of the three residual diagnostics plots:\n\nComment on the residuals pattern you observe\nExplain why the pattern you observe is consistent with the fact that there are much better choices than the naive method for forecasting\n\n\n\ntrain[\"mean_method\"] = train['Beer'].mean()\ntrain[\"naive_method\"] = train.iloc[-1, ].Beer\n\ntrain['month'] = train.index.month\ntrain = train.join(seasonal, on = 'month')\n\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/2917350215.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"mean_method\"] = train['Beer'].mean()\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/2917350215.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"naive_method\"] = train.iloc[-1, ].Beer\n/var/folders/d_/6fhql14j7hd93zfzcn1x3ttxn8st2y/T/ipykernel_19900/2917350215.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train['month'] = train.index.month\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nBeer\nmean_method\nnaive_method\nmonth\nseasonal_naive\n\n\nQuarter\n\n\n\n\n\n\n\n\n\n1992-01-01\n443\n436.45\n491.0\n1\n419.000000\n\n\n1992-04-01\n410\n436.45\n491.0\n4\n386.250000\n\n\n1992-07-01\n420\n436.45\n491.0\n7\n407.666667\n\n\n1992-10-01\n532\n436.45\n491.0\n10\n483.000000\n\n\n1993-01-01\n433\n436.45\n491.0\n1\n419.000000\n\n\n\n\n\n\n\n\ntrain['residual'] = train['Beer'] - train['naive_method']\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(train.index, train['residual'], color='blue')\nplt.scatter(train.index, train['residual'], color='blue', marker='o', s=10)\nplt.xlabel('Date')\nplt.ylabel('DBH')\nplt.title('DBH Time Series with Observed Data Points')\nplt.grid(True)\nplt.show()\nplt.show()\n\n\n\n\n\nmax_k = 30\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\n# Create the autocorrelation plot (ACF)\nplt.figure(figsize=(12, 6))\nplot_acf(train['residual'], lags=max_k, ax=plt.gca())\nplt.show()\n\n\n\n\n\ntrain.hist(column = 'residual', bins = 30)\nplt.show()\n\n\n\n\n\n\nFPP 5.8 - Evaluating point forecast accuracy: Recompute RMSE and MAE values in table using data wrangling\nFor the three forecasting methods above, compute the:\n\nRoot mean squared error\nMean absolute error\n\nwhich gives a single numerical measure of the overall error of the model.\nNote that the table in FPP 5.8 uses a slightly different training set 1992 Q1 - 2007 Q4, instead of 1992 Q1 - 2006 Q4, so you will get slightly different error values, but they should still be close.\n\nimport math\n\ntest['residual'] = test['Beer'] - test['naive_method']\nMSE = test['residual'].pow(2).mean()\nRMSE = math.sqrt(MSE)\nMAE = test['residual'].abs().mean()\nprint(RMSE)\nprint(MAE)\n\n78.62251585900823\n70.07142857142857\n\n\n\ntest['residual'] = test['Beer'] - test['mean_method']\nMSE = test['residual'].pow(2).mean()\nRMSE = math.sqrt(MSE)\nMAE = test['residual'].abs().mean()\nimport math\nprint(RMSE)\nprint(MAE)\n\n38.88971127248366\n35.47142857142857\n\n\n\ntest['residual'] = test['Beer'] - test['seasonal_naive']\nMSE = test['residual'].pow(2).mean()\nRMSE = math.sqrt(MSE)\nMAE = test['residual'].abs().mean()\nimport math\nprint(RMSE)\nprint(MAE)\n\n7.970555934485493\n6.88095238095238\n\n\n\n\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\nI didn’t have time to scaffold this question appropriately. Note to self:"
  },
  {
    "objectID": "PS/PS01_presentations/PS01_tillie_kiera.html",
    "href": "PS/PS01_presentations/PS01_tillie_kiera.html",
    "title": "Problem Set 01",
    "section": "",
    "text": "Name: Kiera Murray\nField that you’re interested in applying TS and forecasting to (ecology, econ, weather, etc): weather, climate, human effects on the environment\n\n\nObtain a CSV of time series data\n\nBy whatever means, get a .csv file of time series data relating to any topic: ecological, financial, etc.\nThere should be at least three variables of data\nIf you download from the web, include a link. If you use ChatGPT, include a link to your shared search\n\nData source: https://data.world/data-society/global-climate-change-data “Global Climate Change Data: Global Land Temperatures By Major City”\nChatGPT searches: https://chat.openai.com/share/e7611222-f3a4-4acb-9104-2e7ca5258102\n\n# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n# Read in raw data\ndata = pd.read_csv('GlobalLandTemperaturesByMajorCity.csv')\n\n# Select 3 cities to analyze, and rearrange the dataframe so each city is its own column\ndataStPetersburg = data[data['City'] == 'Saint Petersburg']\ndataSingapore = data[data['City'] == 'Singapore']\ndataMelbourne = data[data['City'] == 'Melbourne']\ndata = pd.merge(dataStPetersburg, dataSingapore, on='dt', how='inner') #combine St. Petersburg & Singapore data\ndata = pd.merge(data, dataMelbourne, on='dt', how='inner') #add Melbourne data\ndata.rename(columns={'AverageTemperature_x': 'StPetersburg_AvgTemp', 'AverageTemperature_y': 'Singapore_AvgTemp', 'AverageTemperature': 'Melbourne_AvgTemp'}, inplace=True)\ndata = data[['dt', 'StPetersburg_AvgTemp', 'Singapore_AvgTemp', 'Melbourne_AvgTemp']] #get rid of unneeded columns\n\n# Set datetime index and filter out values before 1950 (otherwise there's too much data to visualize)\ndata['dt'] = pd.to_datetime(data['dt'])\ndata.set_index('dt', inplace=True)\ndata = data[data.index &gt; '1950']\ndata = data.dropna()\n\n# Look at the modified data\ndata\n\n\n\n\n\n\n\n\nStPetersburg_AvgTemp\nSingapore_AvgTemp\nMelbourne_AvgTemp\n\n\ndt\n\n\n\n\n\n\n\n1950-02-01\n-5.829\n26.368\n17.980\n\n\n1950-03-01\n-2.812\n26.775\n16.769\n\n\n1950-04-01\n5.366\n26.691\n13.641\n\n\n1950-05-01\n8.655\n27.268\n10.472\n\n\n1950-06-01\n13.055\n27.367\n7.962\n\n\n...\n...\n...\n...\n\n\n2013-04-01\n2.167\n27.767\n14.762\n\n\n2013-05-01\n12.355\n28.083\n11.836\n\n\n2013-06-01\n17.185\n28.662\n9.365\n\n\n2013-07-01\n17.234\n27.487\n9.774\n\n\n2013-08-01\n17.153\n27.372\n10.441\n\n\n\n\n763 rows × 3 columns\n\n\n\n\n\n1. Time series plots\n\nPlot the raw time series data\nIdentify any time series patterns\nIdentify any interesting trends insight\n\n\nplt.figure(figsize=(15, 6))\n\ndata['StPetersburg_AvgTemp'].plot(linewidth=1, label='St. Petersburg')\ndata['Singapore_AvgTemp'].plot(linewidth=1, label='Singapore', color='green')\ndata['Melbourne_AvgTemp'].plot(linewidth=1, label='Melbourne', color='orange')\n\nplt.xlabel('Date')\nplt.ylabel('Monthly Average Temperature (C)')\nplt.title('Raw Time Series Data:  Monthly Average Temperature for 3 Cities, 1950-2013')\nplt.legend()\nplt.yticks(np.arange(-20, 35, 5))\nplt.grid(True)\n\nplt.show()\n\n\n\n\nAll 3 city temperatures show strong seasonality, with colder temperatures in the winter and hotter temperatures in the summer. St. Petersburg, Russia, experiences both the coldest temperatures and the greatest seasonal changes as a consequence of its high northern latitude. Singapore, which is close to the equator, experiences both the warmest temperatures and the smallest variation. Melbourne, Australia, is located at middling latitude in the Southern Hemisphere, and so experiences moderate average temperature and variation, but with summer and winter opposite to the Northern Hemisphere cities.\nBecause of climate change, all 3 city temperatures show a subtle upward trend starting around 1985.\n\n\n2. Scatterplots\n\nPlot a 3x3 grid of all pairwise scatterplots\nIdentify any interesting trends insight\n\n\nsns.pairplot(data)\n\n\n\n\nThe St. Petersburg and Singapore average monthly temperatures are positively correlated, because both cities are in the Northern Hemisphere, meaning they experience the same season at the same time. However, all other temperature pairs are negatively correlated because Melbourne is in the Southern Hemisphere, meaning that it is always experencing the oppsite season as the other cities.\nThe plots involving Singapore’s monthly temperatures appear to show a weaker correlation than the ones with only St. Petersburg and Melbourne temperatures. This may be because Singapore experiences much less seasonal variation than the other cities.\n\n\n3. Lag & autocorrelation plots\n\nPlot a lag plot of any variable of your choice to visualize its relationship to itself in the past. Use a maximum \\(k\\) value of your choice\nPlot an autocorrelation plot of this same variable to quantify its relationship to itself in the past. Use a maximum \\(k\\) value of your choice\n\n\n# Lag plots of St. Petersberg average monthly temperature\n# Code mostly written by ChatGPT\n\n\n# Define the maximum lag value (k)\nmax_k = 12\n\n# Create a colormap for months\ncmap = ListedColormap(['blue', 'dodgerblue', 'green', 'limegreen', 'forestgreen', 'yellow', 'gold', 'khaki', 'red', 'tomato', 'darkred', 'deepskyblue'])\n\n# Create separate scatterplots for each lag value\nplt.figure(figsize=(12, 8))\n\nfor lag in range(1, max_k + 1):\n    plt.subplot(3, 4, lag)  # Create a 3x4 grid of subplots\n    lagged_col = data['StPetersburg_AvgTemp'].shift(lag)\n    months = data.index.month  # Extract the month from the index\n    scatter = plt.scatter(data['StPetersburg_AvgTemp'], lagged_col, c=months, cmap=cmap)\n    plt.title(f'Lag {lag}')\n    plt.xlabel('St Peters. Temp (t)')\n    plt.ylabel(f'St Peters. Temp (t-{lag})')\n    plt.colorbar(scatter, ticks=np.arange(1, 13), label='Month')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAbove are lag plots for St. Petersburg average monthly temperature, with lags ranging from 1 (a month) to 12 (a year). Lag 12, in which a month’s average temperature is plotted against itself, shows the strongest positive correlation. The strongest negative correlation appears at Lag 6, in which a month is plotted against its seasonal opposite. The plots at Lag 3 and Lag 9 show the weakest correlation, when a month is plotted against one with a middling temperature difference. The ‘donut’ shape is caused by St. Petersburg’s extreme seasonal temperature variation.\n\n# Autocorrelation plot of St. Petersberg average monthly temperature\n\n\n# Define the maximum lag value (k)\nmax_k = 60\n\n# Create the autocorrelation plot (ACF)\nplt.figure(figsize=(12, 6))\nplot_acf(data['StPetersburg_AvgTemp'], lags=max_k, ax=plt.gca())\nplt.title(f'Autocorrelation Plot: St. Petersberg Average Monthly Temperature Over 5 Years')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.grid(True)\nplt.xticks(np.arange(0, 62, 2))\nplt.show()\n\n\n\n\nThe autocorrelation plot of St. Petersburg’s average monthly temperature shows strong seasonality. A month experiences the most positive autocorrelation at Lag 0, 12, 24, etc., when the same month appears. The most negative autocorrelation occurs at Lag 6, 18, 30, ect., with the month’s seasonal opposite. Autocorrelation is positive closer to the month itself and negative closer to its seasonal opposite, since average temperature changes continuously.\nWe can also observe over this plot of 5 years that the strength of autocorrelation always decays with increasing Lag.\n\n\nAppendix: Choosing cities\n\n# Look at the raw data\n\noriginaldata = pd.read_csv('GlobalLandTemperaturesByMajorCity.csv')\noriginaldata\n\n\n\n\n\n\n\n\ndt\nAverageTemperature\nAverageTemperatureUncertainty\nCity\nCountry\nLatitude\nLongitude\n\n\n\n\n0\n1849-01-01\n26.704\n1.435\nAbidjan\nCôte D'Ivoire\n5.63N\n3.23W\n\n\n1\n1849-02-01\n27.434\n1.362\nAbidjan\nCôte D'Ivoire\n5.63N\n3.23W\n\n\n2\n1849-03-01\n28.101\n1.612\nAbidjan\nCôte D'Ivoire\n5.63N\n3.23W\n\n\n3\n1849-04-01\n26.140\n1.387\nAbidjan\nCôte D'Ivoire\n5.63N\n3.23W\n\n\n4\n1849-05-01\n25.427\n1.200\nAbidjan\nCôte D'Ivoire\n5.63N\n3.23W\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239172\n2013-05-01\n18.979\n0.807\nXian\nChina\n34.56N\n108.97E\n\n\n239173\n2013-06-01\n23.522\n0.647\nXian\nChina\n34.56N\n108.97E\n\n\n239174\n2013-07-01\n25.251\n1.042\nXian\nChina\n34.56N\n108.97E\n\n\n239175\n2013-08-01\n24.528\n0.840\nXian\nChina\n34.56N\n108.97E\n\n\n239176\n2013-09-01\nNaN\nNaN\nXian\nChina\n34.56N\n108.97E\n\n\n\n\n239177 rows × 7 columns\n\n\n\n\n# Filter dates and look at what cities have the most data for this timeframe\n\noriginaldata.set_index('dt', inplace=True)\noriginaldata = originaldata[originaldata.index &gt; '1960']\noriginaldata['City'].mode()\n\n0         Abidjan\n1     Addis Abeba\n2       Ahmadabad\n3          Aleppo\n4      Alexandria\n         ...     \n95          Tokyo\n96        Toronto\n97     Umm Durman\n98          Wuhan\n99           Xian\nName: City, Length: 100, dtype: object\n\n\n\nFind some cities at diverse latitiudes to analyze:\n\n# Convert Latitude to a number\n\ndef convert_latitude(lat_str):\n    \"\"\" Written by ChatGPT \"\"\"\n    lat_value = float(lat_str.rstrip('NS'))\n    if 'S' in lat_str:\n        lat_value *= -1  # Convert to a negative value for southern latitudes\n    return lat_value\n\noriginaldata['Latitude'] = originaldata['Latitude'].apply(convert_latitude)\n\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_54075/3019456029.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  originaldata['Latitude'] = originaldata['Latitude'].apply(convert_latitude)\n\n\n\n# Find a northernmost city\n\ntest = originaldata[originaldata['Latitude'] &gt; 60]\ntest['City'].mode()\n\n0    Saint Petersburg\nName: City, dtype: object\n\n\n\n# Find an equatorial city\n\ntest = originaldata[(originaldata['Latitude'] &gt; -1) & (originaldata['Latitude'] &lt; 1)]\ntest['City'].mode()\n\n0      Nairobi\n1    Singapore\nName: City, dtype: object\n\n\n\n# Find a southernmost city\n\ntest = originaldata[originaldata['Latitude'] &lt; -35]\ntest['City'].mode()\n\n0    Melbourne\nName: City, dtype: object"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html",
    "href": "PS/PS02_presentations/PS02_betty_chi.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: Chi Qiu 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html#method-i",
    "href": "PS/PS02_presentations/PS02_betty_chi.html#method-i",
    "title": "Problem Set 02",
    "section": "Method I",
    "text": "Method I\n\n# Source: Data Camp\n# Fit an AR(2) model\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\nmod = ARIMA(df, order=(2,0,0))\nres = mod.fit()\n\n\n# Print the summary of the result\nprint(res.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                 3778\nModel:                 ARIMA(2, 0, 0)   Log Likelihood                9411.044\nDate:                Tue, 24 Oct 2023   AIC                         -18814.088\nTime:                        10:29:52   BIC                         -18789.140\nSample:                    01-27-2010   HQIC                        -18805.219\n                         - 05-31-2020                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        633.8438   1.51e-09    4.2e+11      0.000     633.844     633.844\nar.L1          1.8933      0.001   2352.900      0.000       1.892       1.895\nar.L2         -0.8933      0.001  -1109.748      0.000      -0.895      -0.892\nsigma2         0.0004   4.66e-07    858.072      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   8.02   Jarque-Bera (JB):         233549300.72\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               8.23   Skew:                             5.50\nProb(H) (two-sided):                  0.00   Kurtosis:                      1221.00\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n[2] Covariance matrix is singular or near-singular, with condition number 2.32e+21. Standard errors may be unstable.\n\n\n\n# In-sample & Out-of-sample forecasts\n\nfig, ax = plt.subplots()\ndf['2010':].plot(ax=ax, label='Original', marker='o', markersize=3, color='yellow')\n\nplot_predict(res, start='2020-06-01', end='2022-06-01', ax=ax)\nplot_predict(res, start='2010-01-27', end='2020-06-01', ax=ax, alpha=None)\n\nplt.legend([\"Original\", \"Out-of-Sample Forecast\", \"95% Confidence Interval\", \"In-Sample Forecast\"])\nplt.show()"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_betty_chi.html#method-ii",
    "href": "PS/PS02_presentations/PS02_betty_chi.html#method-ii",
    "title": "Problem Set 02",
    "section": "Method II",
    "text": "Method II\n\n# Source: ChatGPT\n# Fit an AR(2) model\nmodel = sm.tsa.AutoReg(df['dbh'], lags=2)\nresults = model.fit()\n\n\n# Print the summary of the result\nprint(results.summary())\n\n                            AutoReg Model Results                             \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                 3778\nModel:                     AutoReg(2)   Log Likelihood                9422.887\nMethod:               Conditional MLE   S.D. of innovations              0.020\nDate:                Mon, 23 Oct 2023   AIC                         -18837.775\nTime:                        16:59:57   BIC                         -18812.829\nSample:                    01-29-2010   HQIC                        -18828.906\n                         - 05-31-2020                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0017      0.016      0.105      0.916      -0.029       0.032\ndbh.L1         1.8840      0.008    247.624      0.000       1.869       1.899\ndbh.L2        -0.8840      0.008   -116.183      0.000      -0.899      -0.869\n                                    Roots                                    \n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            1.0000           +0.0000j            1.0000            0.0000\nAR.2            1.1312           +0.0000j            1.1312            0.0000\n-----------------------------------------------------------------------------\n\n\n\n# In-sample fitted forecasts\npred_insample = results.fittedvalues\n\n# Out-of-sample forecasts\nforecast_outsample = results.get_prediction(start=len(df), end=len(df) + 730)  # 730 days = 2 years * 365 days\n\n# Get forecast values and confidence intervals\nforecast_mean = forecast_outsample.predicted_mean\nforecast_conf_int = forecast_outsample.conf_int()\n\n# Plot the data and forecasts\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['dbh'], label='Original', linestyle='-', marker='o', markersize=3, color='blue')\nplt.plot(pred_insample.index, pred_insample, label='In-Sample Forecast', color='orange')\nplt.plot(forecast_mean.index, forecast_mean, label='Out-of-Sample Forecast', color='red')\n\n# Fill the area between the confidence intervals\nplt.fill_between(forecast_conf_int.index, forecast_conf_int.iloc[:, 0], forecast_conf_int.iloc[:, 1], color='pink', alpha=0.3, label='95% CI')\n\nplt.xlabel('Date')\nplt.ylabel('DBH')\nplt.legend()\nplt.title('AR(2) Forecast for Tree Size')\nplt.show()\n\n\n\n\nComment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem. - Both forecasts fall within the 95% confidence interval, thus both are reliable. But the first CI is much wider than the second CI, thus the forecast quality might be lower - The second method provides a better forecast given its increasing trend - After running the first method, I ran the second method through ChatGPT to explore other model options that will improve the quality"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours start late today: 11:30am - 12:05pm\nClass on Tuesday"
  },
  {
    "objectID": "index.html#lecture",
    "href": "index.html#lecture",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 9.3 Autoregressive model: Specific cases of AR(1) model\n\nRandom walk example\nRandom walk with drift example\n\nFPP 9.4 Moving Average modeling (recall: this is not moving average smoothing)\nFPP 9.5 Non-seasonal ARIMA models\nFPP 9.7 Modeling procedure\n\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# Code block 1: FPP 9.4 Fig 9.6 a) -----\n# Errors\nepsilon &lt;- rnorm(n=100, mean=0, sd=1)\n\n# Store values of yt here\nyt &lt;- rep(0, times = 100)\nyt[1] &lt;- 20\nfor(t in 2:100){\n  yt[t] &lt;- 20 + epsilon[t] + 0.8*epsilon[t-1]\n}\nplot(yt, type = 'l')\n\n\n# Code block 2: FPP 9.5 example (in different order) ------\n# Load data\negypt_exports &lt;- global_economy |&gt;\n  filter(Code == \"EGY\") %&gt;% \n  select(Year, Exports)\negypt_exports\n\n# Visualize raw data:\negypt_exports |&gt;\n  autoplot(Exports) +\n  labs(y = \"% of GDP\", title = \"Egyptian exports\")\n\n# Visualize autocorrelation function:\negypt_exports %&gt;% \n  ACF(Exports) %&gt;% \n  autoplot()\n\n# Visualize partial autocorrelation function:\negypt_exports %&gt;% \n  PACF(Exports) %&gt;% \n  autoplot()\n\n# Fit ARIMA(p,d,q) where you instead of specifying (p,d,q), you let function\n# find these values for you\nfit &lt;- egypt_exports |&gt;\n  model(ARIMA(Exports))\n\n# Get model fitted parameters\nreport(fit)\n\nfit |&gt; \n  forecast(h=10) |&gt; \n  autoplot(egypt_exports) +\n  labs(y = \"% of GDP\", title = \"Egyptian exports\")"
  },
  {
    "objectID": "index.html#announcements-1",
    "href": "index.html#announcements-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS4 attendance policy\nFinal project added instructions"
  },
  {
    "objectID": "index.html#lecture-1",
    "href": "index.html#lecture-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 9.1 Stationarity and differencing\nFPP 9.2 Backshift notation\n\nFPP 9.3 Autoregressive model\n\nRandom walk example\nRandom walk with drift example\nRecall PS2\n\n\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(patchwork)\n\n# Code block 1: Fig 9.1 a) and b) data Google Stocks\ngoogle_2015 &lt;- gafa_stock |&gt;\n  filter(Symbol == \"GOOG\", year(Date) == 2015)\n\n# Not stationary:\ngoogle_2015 %&gt;% autoplot(Close)\n\n# Differences are stationary:\ngoogle_2015 %&gt;% autoplot(difference(Close))\n\n# Compare autocorrelation functions:\nplot1 &lt;- google_2015 %&gt;% ACF(Close) %&gt;% autoplot()\nplot2 &lt;- google_2015 %&gt;% ACF(difference(Close)) %&gt;% autoplot()\nplot1 + plot2\n\n\n# Code block 2: FPP 9.3 Fig 9.5a) Simulate 100 values of AR1 process\nAR1_ex &lt;- rep(0, 100)\n\n# Set first value\nAR1_ex[1] &lt;- 11.5\n\nfor(t in 2:100){\n  # Intercept 18 + slope of previous obs -0.8 + epsilon random noise\n  AR1_ex[t] = 18 - 0.8*AR1_ex[t-1] + rnorm(n=1, mean=0, sd=1)\n}\nplot(AR1_ex, type = 'l')"
  },
  {
    "objectID": "index.html#announcements-2",
    "href": "index.html#announcements-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nNo in-class lecture\nFinal project extra notes\n\nFit between 2-4 models, one of which needs to be an ARIMA model\nDo a residual analysis of the model selected for forecasting into the future using all the available data"
  },
  {
    "objectID": "index.html#lecture-2",
    "href": "index.html#lecture-2",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nRecorded lecture posted in Screencasts tab of webpage (top right). Please post questions in Slack under #questions including the point in time in video your question corresponds to. Topics:\n\nFPP 8.2 Exponential smoothing with trend\nFPP 8.3 Exponential smoothing with seasonality\n\nPS4 files posted on Slack, please read instructions on Problem Sets page\n\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# Code block 1: Example from FPP 8.2 on exponential smoothing with trend ----\n# a) Get and visualize data\naus_economy &lt;- global_economy |&gt;\n  filter(Code == \"AUS\") |&gt;\n  mutate(Pop = Population / 1e6)\nautoplot(aus_economy, Pop) +\n  labs(y = \"Millions\", title = \"Australian population\")\n\n\n# b) Regular exponential smoothing with trend\nfit &lt;- aus_economy |&gt;\n  model(\n    # Additive (i.e. non-multiplicative) error and trend, no seasonality:\n    AAN = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\n\n# Forecast ten times points into future\nfc &lt;- fit |&gt; \n  forecast(h = 10)\n\n# NOT IN BOOK: get the fitted alpha/beta and l0/b0 values\n# NOTE: CRAZY high alpha! Very high weight on previous observation!\nreport(fit)\ntidy(fit)\n\n# Fitted values in orange, forecasts in blue.\n# Note:\n# 1. Confidence band is narrow\n# 1. Problem forecast increases forever into the future\nfc |&gt;\n  autoplot(aus_economy) +\n  geom_line(aes(y = .fitted), col=\"#D55E00\", data = augment(fit)) +\n  labs(y = \"Millions\", title = \"Australian population\") +\n  guides(colour = \"none\")\n\n\n# c) Dampened exponential smoothing with trend\nfit &lt;- aus_economy |&gt;\n  model(\n    # Previous model: Additive (i.e. non-multiplicative) error and trend, no seasonality:\n    # AAN = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n    # Dampened model with given phi: Additive error, dampened trend with given phi parameter, no seasonality:\n    # AAN = ETS(Pop ~ error(\"A\") + trend(\"Ad\", phi = 0.8) + season(\"N\"))\n    # Dampened model letting phi be estimated:\n    AAN = ETS(Pop ~ error(\"A\") + trend(\"Ad\") + season(\"N\"))\n  )\n\n# Forecast ten times points into future\nfc &lt;- fit |&gt; \n  forecast(h = 10)\n\n# NOT IN BOOK: get the fitted alpha/beta and l0/b0 values\n# NOTE: Another high value of alpha\nreport(fit)\ntidy(fit)\n\n# Fitted values in orange, forecasts in blue.\n# Note:\n# 1. How dampening parameter flattens out forecast curve.\nfc |&gt;\n  autoplot(aus_economy) +\n  geom_line(aes(y = .fitted), col=\"#D55E00\", data = augment(fit)) +\n  labs(y = \"Millions\", title = \"Australian population\") +\n  guides(colour = \"none\")\n\n\n\n# Code block 2: Example from FPP 8.3 on exponential smoothing with seasonality ----\naus_holidays &lt;- tourism |&gt;\n  filter(Purpose == \"Holiday\") |&gt;\n  summarise(Trips = sum(Trips)/1e3)\n\nautoplot(aus_holidays, Trips) +\n  labs(y = \"Thousands\", title = \"Australian trips\")\n\nfit &lt;- aus_holidays |&gt;\n  model(\n    # Additive and multiplicative models:\n    #additive = ETS(Trips ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n    #multiplicative = ETS(Trips ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n    # Additive and multiplicative models with dampening:\n    additive = ETS(Trips ~ error(\"A\") + trend(\"Ad\", phi = 0.2) + season(\"A\")),\n    multiplicative = ETS(Trips ~ error(\"M\") + trend(\"Ad\", phi = 0.2) + season(\"M\"))\n  )\n\n# NOT IN BOOK: get the fitted alpha/beta and l0/b0 values\nreport(fit)\ntidy(fit)\n\nfc &lt;- fit |&gt; \n  forecast(h = \"3 years\")\n\n# Plot both models:\n# NOTE\nfc |&gt;\n  autoplot(aus_holidays, level = NULL) +\n  labs(title=\"Australian domestic tourism\",\n       y=\"Overnight trips (millions)\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "index.html#announcements-3",
    "href": "index.html#announcements-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nResidual analysis: on which data? train or test or both?"
  },
  {
    "objectID": "index.html#lecture-3",
    "href": "index.html#lecture-3",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nSimple exponential smoothing\n\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\n# Code block 1: Study weights in an exponential model ----\n# Choose 0 &lt; alpha &lt;= 1\nalpha &lt;- 0.9\nk &lt;- 1:11\n\n# Inspect and visualize weights:\nalpha*(1-alpha)^(k-1)\nplot(k, alpha*(1-alpha)^(k-1))\n\n# As consider more and more weights, weights sum to 1\nsum(alpha*(1-alpha)^(k-1))\n\n\n\n# Code block 2: Example from FPP 8.1 ----\n# Get and visualize data\nalgeria_economy &lt;- global_economy |&gt;\n  filter(Country == \"Algeria\")\nalgeria_economy |&gt;\n  autoplot(Exports) +\n  labs(y = \"% of GDP\", title = \"Exports: Algeria\")\n\n# Fit simple exponential smoothing model using ETS() function:\n# Additive, not multiplicative model. No trend or season for now.\nfit &lt;- algeria_economy |&gt;\n  model(ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")))\n\n# NOT IN BOOK: get the fitted alpha and l0 values\nreport(fit)\ntidy(fit)\n\n# Forecast 5 time points into the future\nfc &lt;- fit |&gt;\n  forecast(h = 5)\n\n# Fitted values in orange, forecasts in blue.\n# Note:\n# 1. LARGE forecast error interval\n# 2. This is only simple exponential smoothing\n# 3. Comparing orange and black lines, we see degree of smoothing is not that high\nfc |&gt;\n  autoplot(algeria_economy) +\n  geom_line(aes(y = .fitted), col=\"#D55E00\", data = augment(fit)) +\n  labs(y=\"% of GDP\", title=\"Exports: Algeria\") +\n  guides(colour = \"none\")"
  },
  {
    "objectID": "index.html#announcements-4",
    "href": "index.html#announcements-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS3 mini-presentations today. Info in Problem Sets\nFinalized final project information posted in Final Project"
  },
  {
    "objectID": "index.html#announcements-5",
    "href": "index.html#announcements-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nIn-class guest lecture today with Prof. Will Hopper\nWork on PS3. Note link to rendered output of all previous PS .ipynb files have been posted in Problem Sets"
  },
  {
    "objectID": "index.html#announcements-6",
    "href": "index.html#announcements-6",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nZoom lecture today starting at regular class time 10am (Zoom link sent on Slack)\nAnswer Poll Everywhere question in Slack #general on whether you prefer group or individual final projects\nDiscuss final project. Info in Problem Sets"
  },
  {
    "objectID": "index.html#announcements-7",
    "href": "index.html#announcements-7",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nNo lecture on Thursday from Cromwell Day\nZoom lecture on Tuesday 10/7\nIn-class lecture on Thursday 10/9 with Prof. Will Hopper"
  },
  {
    "objectID": "index.html#lecture-4",
    "href": "index.html#lecture-4",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nStart PS3"
  },
  {
    "objectID": "index.html#announcements-8",
    "href": "index.html#announcements-8",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nFill out PS2 peer eval Google Form and slido poll"
  },
  {
    "objectID": "index.html#lecture-5",
    "href": "index.html#lecture-5",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 5.5: Prediction intervals\nFPP 5.7: Forecasting with STL decomposition method\nFPP 5.8: Assessing prediction accuracy"
  },
  {
    "objectID": "index.html#announcements-9",
    "href": "index.html#announcements-9",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS2 mini-presentations today. Info in Problem Sets"
  },
  {
    "objectID": "index.html#lecture-6",
    "href": "index.html#lecture-6",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture"
  },
  {
    "objectID": "index.html#announcements-10",
    "href": "index.html#announcements-10",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nUpdate PS2 in-class presentation format in Problem Sets\nPS2 mini-presentations this coming Tuesday"
  },
  {
    "objectID": "index.html#lecture-7",
    "href": "index.html#lecture-7",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nCode blocks below\nRefresher on conditions for inference for linear regression\n\n\nlibrary(fpp3)\n\n# Code block 1: Fit model to data and generate forecasts ----\n# Tidy:\nfull_data &lt;- aus_production %&gt;% \n  filter_index(\"1992 Q1\" ~ \"2010 Q2\")\n\n# Visualize:\nfull_data %&gt;% \n  autoplot(Beer)\n\n# Specify: Take full data and fit 3 models\nbeer_fit_1 &lt;- full_data |&gt;\n  model(\n    Mean = MEAN(Beer),\n    `Naïve` = NAIVE(Beer),\n    `Seasonal naïve` = SNAIVE(Beer)\n  )\n\n# Generate forecasts 14 time periods (quarters) into future\nbeer_forecast_1 &lt;- beer_fit_1 |&gt; \n  forecast(h = 14)\n\n# Plot full data and forecasts\nplot1 &lt;- beer_forecast_1 |&gt;\n  autoplot(full_data, level = NULL) +\n  labs(\n    y = \"Megalitres\",\n    title = \"Forecasts for quarterly beer production\"\n  ) +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\nplot1\n\n\n\n# Code block 2: Training test set approach ----\n# Define training data to be up to 2007: fit the model to this data\ntraining_data &lt;- full_data |&gt;\n  filter_index(\"1992 Q1\" ~ \"2006 Q4\")\n\n# Define test data to be 2007 and beyond:\ntest_data &lt;- full_data |&gt;\n  filter_index(\"2007 Q1\" ~ \"2010 Q2\")\n\n# Fit 3 models only to training data\nbeer_fit_2 &lt;- training_data |&gt;\n  model(\n    Mean = MEAN(Beer),\n    `Naïve` = NAIVE(Beer),\n    `Seasonal naïve` = SNAIVE(Beer)\n  )\n\n# Generate forecasts for 14 quarters\nbeer_forecast_2 &lt;- beer_fit_2 |&gt; \n  forecast(h = 14)\n\n# Plot training data and forecasts only\nplot2 &lt;- beer_forecast_2 |&gt;\n  autoplot(training_data, level = NULL) +\n  labs(\n    y = \"Megalitres\",\n    title = \"Forecasts for quarterly beer production\"\n  ) +\n  guides(colour = guide_legend(title = \"Forecast\"))\nplot2\n`\n# Add the test data. We can now evaluate quality of predictions\nplot2 +\n  autolayer(test_data, colour = \"black\")\n\n\n\n# Code block 3: Residual analysis ----\n# Get residuals\naugment(beer_fit_1) %&gt;% \n  View()\n\n# Focus only on seasonal naive method:\nseasonal_naive_augment &lt;- augment(beer_fit_1) %&gt;% \n  filter(.model == \"Seasonal naïve\")\n\n# Check 2 and 3: Mean 0 and constant variance\nseasonal_naive_augment %&gt;% \n  autoplot(.resid) +\n  geom_hline(yintercept=0, linetype = \"dashed\")\n\n# Check 1: Uncorrelated residuals\nseasonal_naive_augment %&gt;% \n  ACF(.resid) %&gt;% \n  autoplot() +\n  labs(title = \"Residuals from the naïve method\")\n\n# Check 2 and 4: Zero mean and normally distributed\nggplot(seasonal_naive_augment, aes(x = .resid)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "index.html#announcements-11",
    "href": "index.html#announcements-11",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nNext DC course “Visualizing Time Series Data in Python” assigned due Tue 10/31\nDiscuss PS2 in-class presentation format on Tue 10/24 using PS1 feedback from slido"
  },
  {
    "objectID": "index.html#lecture-8",
    "href": "index.html#lecture-8",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nlibrary(fpp3)\nlibrary(feasts)\n\n# Code block 1: FPP 4.2 ACF features ----\n# Reprint output from book\ntourism |&gt; \n  features(Trips, feat_acf)\n\n# a) Focus on only top row of output from book\ntop_row &lt;- tourism %&gt;% \n  filter(Region == \"Adelaide\", State == \"South Australia\", Purpose == \"Business\")\n\ntop_row |&gt; \n  features(Trips, feat_acf)\n\n# LOOK AT YOUR DATA!!\ntop_row %&gt;% \n  autoplot(Trips)\n\n# b) ACF at k=1 is acf1 in output from book\ntop_row %&gt;% \n  ACF(Trips) %&gt;% \n  autoplot()\n\n# c) Take differences of data using difference() from FPP 9.1\n# https://otexts.com/fpp3/stationarity.html#differencing\n# For example switch from size to growth\ntop_row %&gt;% \n  autoplot(difference(Trips))\n\n# ACF at k=1 is diff1_acf1 in output from book\ntop_row %&gt;% \n  ACF(difference(Trips)) %&gt;% \n  autoplot()\n\n\n\n# Code block 2: FPP 4.3 ACF features ----\n# Compare and contrast two classical decomposition features\n\n# a) FPP 4.3 Australia trips\ntop_row %&gt;% \n  model(\n    classical_decomposition(Trips, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total trips\")\n\n# b) FPP 3.4 Fig 3.13 from Lec 5.1\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\nus_retail_employment %&gt;%\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n# c) compare trend_strength and seasonal_strength_year\ntop_row |&gt;\n  features(Trips, feat_stl) %&gt;% \n  View()\n\nus_retail_employment |&gt;\n  features(Employed, feat_stl) %&gt;% \n  View()"
  },
  {
    "objectID": "index.html#announcements-12",
    "href": "index.html#announcements-12",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nStill missing a few PS1 peer evalutions\nDiscuss slido responses"
  },
  {
    "objectID": "index.html#lecture-9",
    "href": "index.html#lecture-9",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\n“Time Series Analysis in Python” DataCamp course Chapters 3-5 of this course focused on “autoregressive integrated moving average” (ARIMA) models. This is covered in:\n\nFPP Chapter 9\nThe 4th of 5 courses in “Time Series with Python” DC skill track (see schedule above).\n\nChalk talk\nStart PS2 (on new “Problem Sets” tab of webpage)"
  },
  {
    "objectID": "index.html#announcements-13",
    "href": "index.html#announcements-13",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "index.html#lecture-10",
    "href": "index.html#lecture-10",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nProblem Set 01 in-class presentations"
  },
  {
    "objectID": "index.html#announcements-14",
    "href": "index.html#announcements-14",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nPS1 mini-presentations on Thursday\nPS1 submission format posted below"
  },
  {
    "objectID": "index.html#lecture-11",
    "href": "index.html#lecture-11",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nLec 4.2 on FPP 3.4 Classical Decomposition, second attempt\n\nWhat is \\(m\\) used in example?\nHow is seasonal component \\(S_t\\) computed? What is assumed seasonality?\nCode over code block below\n\nFFP 3.5 Briefly discuss other decomposition methods used\nFFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of \\(m\\)-MA smoothing like for classical decomposition \n\n\nlibrary(fpp3)\n\n# Code block 1: Recreate Fig 3.13 ----\n# 1.a) Get data\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\n# Note index is 1M = monthly. This sets m=12 in m-MA method below\nus_retail_employment\n\n# 1.b) Recreate Fig 3.13\nus_retail_employment %&gt;%\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n\n\n# Code block 2: Recreate all 4 subplots in Fig 3.13 ----\n# Get data frame with full breakdown of decomposition\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() %&gt;% \n  # Convert from tsibble to regular tibble for data wrangling\n  as_tibble()\n\n# 2.a) Top plot: Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# 2.b) 2nd plot: trend-cycle via MA average method\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# 2.c) Extra: detrended plot\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Detrended data Original data minus trend-cycle component\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n# 2.d) 3rd plot: seasonality (values repeat every 12 months)\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"Seasonality\")\n\n# 2.e) 4th plot: Compute remainders\nggplot(full_decomposition, aes(x=Month, y = random)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\") +\n  geom_hline(yintercept = 0, col = \"red\")\n\n\n\n# Code block 3:  Compute full breakdown for one row: 1990 July ----\n# 3.a) LOOK at data\n# IMO most important function in RStudio: View()\n# Focus on Row 7 1990 July: where do all these values come from?\nView(full_decomposition)\n\n# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA\n# Compute mean of first 12 values\nfull_decomposition %&gt;% \n  slice(1:12) %&gt;% \n  summarize(mean = mean(Employed))\n# Compute mean of next 12 values\nfull_decomposition %&gt;% \n  slice(2:13) %&gt;% \n  summarize(mean = mean(Employed))\n# Now do second averaging of 2 values\n(13186 + 13170)/2\n\n# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625\nfull_decomposition &lt;- full_decomposition %&gt;% \n  mutate(detrended = Employed - trend)\n\n# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661\nfull_decomposition %&gt;% \n  mutate(month_num = month(Month)) %&gt;% \n  group_by(month_num) %&gt;% \n  summarize(St = mean(detrended, na.rm = TRUE))\n\n# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610\n# y_t - T_hat_t - S_hat_t\n13170.1 - 13177.76 - (-13.311661)"
  },
  {
    "objectID": "index.html#announcements-15",
    "href": "index.html#announcements-15",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nComment on the generalizability of everything I say"
  },
  {
    "objectID": "index.html#lecture-12",
    "href": "index.html#lecture-12",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\n\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Modified version of code to produce FPP Fig 3.10\naus_exports &lt;- global_economy |&gt;\n  filter(Country == \"Australia\") \n\n# Note number of rows\naus_exports\n\n# Set m and plot\nm &lt;- 28\naus_exports |&gt;\n  mutate(\n    `m-MA` = slider::slide_dbl(Exports, mean,\n                .before = m, .after = m, .complete = TRUE)\n  ) |&gt;\n  autoplot(Exports) +\n  geom_line(aes(y = `m-MA`), colour = \"#D55E00\") +\n  labs(y = \"% of GDP\",\n       title = \"Total Australian exports\") +\n  guides(colour = guide_legend(title = \"series\"))\n\n\n# Code block 2 ----\n# Classical decomposition breakdown\n\n# Plot TS data in questions\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\nus_retail_employment\n\n# Code to create full Fig 3.13\nus_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical additive decomposition of total\n                  US retail employment\")\n\n# Get data frame with all the decomposition parts\nfull_decomposition &lt;- us_retail_employment |&gt;\n  model(\n    classical_decomposition(Employed, type = \"additive\")\n  ) |&gt;\n  components()\n\nfull_decomposition\n\n# Original TS data\nggplot(full_decomposition, aes(x=Month, y = Employed)) +\n  geom_line() +\n  labs(title = \"Original data\")\n\n# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = trend)) +\n  geom_line() +\n  labs(title = \"trend-cycle component\")\n\n# Step 2: Subtract trend-cycle from original data\nggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +\n  geom_line() +\n  labs(title = \"Original data minus trend-cycle component\")\n\n# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13\nggplot(full_decomposition, aes(x=Month, y = seasonal)) +\n  geom_line() +\n  labs(title = \"For each season compute average\")\n\n# Step 4: Compute remainders. 4th plot of Fig 3.13\nggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +\n  geom_line() +\n  labs(title = \"Remainder i.e. noise\")"
  },
  {
    "objectID": "index.html#announcements-16",
    "href": "index.html#announcements-16",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nThe next course “Time Series Analysis in Python” due Tue 10/3 Thu 10/5 9:25am.\nI’m keeping up with screencasts, still need to finish Chapter 4.\nSlido responses:\n\n“I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I’m doing a small portion of a data analysis. I find myself”going through the motions” and feeling it is tedious because I don’t think I’m really comprehending the importance of each step.”\n“Making stupid mistakes on the syntax and kind of confused about the difference between [] and . when calling an attribute”\n“Worried I won’t retain my understanding”\n“I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it”\n\nMain tip: “Optimal frustration”"
  },
  {
    "objectID": "index.html#lecture-13",
    "href": "index.html#lecture-13",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nFPP 3.1 Transformations. \\(\\log10\\) transformations:\n\nWhat are \\(\\log\\) (base \\(e\\)) and \\(\\log10\\) (base 10) tranformations? Example table\nEffect on visualizations: Example figure\n\nFPP 3.2 Time series decompositions\nProblem Set 1 posted"
  },
  {
    "objectID": "index.html#problem-set-1",
    "href": "index.html#problem-set-1",
    "title": "SDS390: Ecological Forecasting",
    "section": "Problem Set 1",
    "text": "Problem Set 1"
  },
  {
    "objectID": "index.html#announcements-17",
    "href": "index.html#announcements-17",
    "title": "SDS390: Ecological Forecasting",
    "section": "Announcements",
    "text": "Announcements\n\nFirst problem set assigned on Tue, which will build into first mini-presentation\nMountain Day recap\nOriginally assigned course “Manipulating Time Series Data in Python” due next Tue 9/26 before class\nGo to Roster Google Sheet (top right of page) and fill DC columns"
  },
  {
    "objectID": "index.html#lecture-14",
    "href": "index.html#lecture-14",
    "title": "SDS390: Ecological Forecasting",
    "section": "Lecture",
    "text": "Lecture\n\nDataCamp\n\nPoll class on sli.do about DC\nDC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.\nProf. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3\nScreencasts location\nContinuing Time Series with Python skill track, the next course “Time Series Analysis in Python” due Tue 10/3. If there is an Exercise you’d like me to do in class, let me know.\n\nFinish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below\n\n\nlibrary(fpp3)\n\n# Code block 1 ----\n# Lag plots: relationship of a TS variable with itself in the past\n# Create regular TS plot of data in Fig 2.19 Beer production over time\nrecent_production &lt;- aus_production |&gt;\n  filter(year(Quarter) &gt;= 2000)\n\n# Note time index meta-data = 1Q = quarter\nrecent_production\n\n# Note patterns\nrecent_production |&gt; autoplot(Beer)"
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "Final Project",
    "section": "",
    "text": "Basic info\n\nMajor due dates:\n\nThu 12/14 5pm: 5min video presentation on Moodle\nMon 12/18 8am: Feedback on video presentations returned by Prof. Kim\nThu 12/21 3pm: Final version of notebook due on Moodle\n\nGroups:\n\nGoogle form for group member preferences. Complete by Wed 11/15 9pm.\nNo difference in expectation between group and individual projects\n\nInstructions/clarifications added afterwards:\n\n11/21: Fit between 2-4 models, one of which needs to be an ARIMA model\n11/21: Do a residual analysis of the model selected for forecasting into the future using all the available data\n\n\n\n\nCriteria\nThis project will be graded much like a paper written for a humanities class: this is both a quantitative and qualitative analysis where there isn’t a right answer.\n\nDid you incorporate feedback for video presentation in your final report?\nYou are not being graded on finding “statistically significant” results i.e. the outcome. You are being graded on the quality of your analysis i.e. the process.\nIs your analysis reproducible? If I run the notebook on my computer, will I be able to generate your results?\nIs presentation clean?\n\nDid you use markdown text formatting?\nAre all image axes and titles labeled?\nAre all images “standalone” in that you could use them in a slide deck or share them on social media?\nIs code commented?\n\nDo you adequately but also succinctly convey the context of data and the problem at hand?\nDo you appropriately apply the methods and code learned in problem sets?\nDo you go above and beyond literal descriptions of the results and focus on providing insight? In other words, did you not just focus on the “what”, but also focus on the “why”?\n\n\n\nFormat\n\nFollow structure of final_project.ipynb shared on Slack, which in turn follows the tidy forecasting workflow\nFind your own data. No specific requirements to the data, but it has to have enough protein to apply what you’ve learned this semester.\nResearch question: Somewhat limited since you will be forecasting into the future\nLength of analysis:\n\nNo more than 4 visualizations (visualization of raw data + 4 more). Ask questions for what constitutes a visualization in #questions on Slack\n“Ink to information” ratio should be kept low\n\n\n\n\nVideo presentation\nThis is an opportunity to get feedback before the final paper is due. You will be narrating your notebook in a 5min max screencast.\n\nBe sure to test and watch your recording first\nNotebook doesn’t have to be 100% polished, but it has to be polished enough for me to give feedback"
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": "Manipulating Time Series Data in Python",
    "section": "",
    "text": "import pandas as pd\n\n# Create the range of dates here\nseven_days = pd.date_range(start = '2017-1-1', periods=7, freq='D')\n\n# Iterate over the dates and print the number and name of the weekday\nfor day in seven_days:\n    print(day.dayofweek, day.day_name())\n\n6 Sunday\n0 Monday\n1 Tuesday\n2 Wednesday\n3 Thursday\n4 Friday\n5 Saturday"
  },
  {
    "objectID": "PS/PS02_presentations/PS02_abbey_christy.html",
    "href": "PS/PS02_presentations/PS02_abbey_christy.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: Christy Yang 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n1. Load and explore data\n\nLoad the CSV data of biweekly dendroband measures for tree with tag 82203\nLook at the raw data (rows and variables) and meta-data as Amir always told me to do\nNote that DBH is the only non-index variable we will work with, thus simplify the data frame to only have this variable\nPlot the time series of DBH (diameter at breast height, in mm) as we’ve done numerous times in DataCamp: just using .plot() with no arguments\nNow plot the time series where each of the 147 observed data point is marked with a dot. In other words, your plot should at the very least have these points.\nWhat is the difference in information presented between the two plots?\nDescribe any patterns you observe in the time series where each observed data point is marked with a dot\n\n6.The ‘Time Series of DBH with Observed Points’ is a scatter plot that displays the actual data points of the DBH measurements. Each individual data point corresponds directly to an exact value recorded in the data table. This plot provides the precise discrete observations.\nIn contrast, the ‘Time Series of DBH’ connects these data points with a continuous line, creating a line graph that illustrates the trends and variations in DBH over time. This line line graph offers a smoother and continuous view of how DBH values, better for the observation of trends and patterns in the data.\n7.The plot as a whole reveals a consistent and upward trend in DBH (Diameter at Breast Height) from 2010 to 2020, which aligns with the natural growth pattern of trees. Additionally, the plot illustrates the influence of seasonal variation on DBH. DBH experiences a rapid increase during the second and third quarters of each year. However, it is important to note that there are data gaps in the first and fourth quarters of each year, resulting in blank areas on the plot during those periods. These gaps may be due to lack of measurement practices.\n\ndata = pd.read_csv(\"tag_82203.csv\")\nprint(data.index)\n\n# Inspect the data\nprint(\"Raw data:\")\nprint(data.head())\nprint(\"Data info:\")\nprint(data.info())\n# Get metadata\nmetadata = data.describe()\nprint(\"Metadata data:\")\nprint(metadata)\n\n#Set date column as index\ndata['date'] = pd.to_datetime(data['date'])\ndata.set_index('date', inplace=True)\n\ndbh = data['dbh']\nprint(\"dbh index\")\nprint(dbh.index)\n\n\n#Haven't explicitly set an index for the original DataFrame data, the index is a default integer-based index starting from 0 and incrementing by 1 for each row.\ndbh.plot()\nplt.xlabel('Time')\nplt.ylabel('diameter at breast height(mm)')\nplt.title('Time Series of DBH')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nplt.plot(dbh, 'o', markersize=3,label='Observed Data Points')\nplt.title(\"Time Series of DBH with Observed Point\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"diameter at breast height(mm)\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nRangeIndex(start=0, stop=147, step=1)\nRaw data:\n     tag  stemtag  survey.ID  year  month  day  intraannual    sp  measure  \\\n0  82203        1    2010.01  2010      1   27            1  litu    26.20   \n1  82203        1    2011.01  2011      1   15            1  litu    41.10   \n2  82203        1    2011.02  2011      5    9            1  litu    40.62   \n3  82203        1    2011.03  2011      5   13            1  litu    40.21   \n4  82203        1    2011.04  2011      5   16            1  litu    42.55   \n\n  codes  ... status stemID  dendDiam  dbh_orig  new.band  dendroID  \\\n0   NaN  ...  alive  10045     610.0     611.3         1       106   \n1   NaN  ...  alive  10045       NaN     611.3         0       106   \n2   NaN  ...  alive  10045       NaN     611.3         0       106   \n3   NaN  ...  alive  10045       NaN     611.3         0       106   \n4   NaN  ...  alive  10045       NaN     611.3         0       106   \n\n         date         dbh  scenario   data_source  \n0  2010-01-27  611.300000         0  biweekly_dbh  \n1  2011-01-15  616.049980         1  biweekly_dbh  \n2  2011-05-09  615.896874         1  biweekly_dbh  \n3  2011-05-13  615.766085         1  biweekly_dbh  \n4  2011-05-16  616.512610         1  biweekly_dbh  \n\n[5 rows x 21 columns]\nData info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 147 entries, 0 to 146\nData columns (total 21 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   tag          147 non-null    int64  \n 1   stemtag      147 non-null    int64  \n 2   survey.ID    147 non-null    float64\n 3   year         147 non-null    int64  \n 4   month        147 non-null    int64  \n 5   day          147 non-null    int64  \n 6   intraannual  147 non-null    int64  \n 7   sp           147 non-null    object \n 8   measure      147 non-null    float64\n 9   codes        1 non-null      object \n 10  notes        4 non-null      object \n 11  status       147 non-null    object \n 12  stemID       147 non-null    int64  \n 13  dendDiam     6 non-null      float64\n 14  dbh_orig     147 non-null    float64\n 15  new.band     147 non-null    int64  \n 16  dendroID     147 non-null    int64  \n 17  date         147 non-null    object \n 18  dbh          147 non-null    float64\n 19  scenario     147 non-null    int64  \n 20  data_source  147 non-null    object \ndtypes: float64(5), int64(10), object(6)\nmemory usage: 24.2+ KB\nNone\nMetadata data:\n           tag  stemtag    survey.ID         year       month         day  \\\ncount    147.0    147.0   147.000000   147.000000  147.000000  147.000000   \nmean   82203.0      1.0  2014.919531  2014.836735    6.823129   15.598639   \nstd        0.0      0.0     2.876154     2.890800    2.331149    8.695503   \nmin    82203.0      1.0  2010.010000  2010.000000    1.000000    1.000000   \n25%    82203.0      1.0  2012.135000  2012.000000    5.000000    9.000000   \n50%    82203.0      1.0  2015.020000  2015.000000    7.000000   15.000000   \n75%    82203.0      1.0  2017.115000  2017.000000    9.000000   23.000000   \nmax    82203.0      1.0  2020.060000  2020.000000   12.000000   31.000000   \n\n       intraannual     measure   stemID    dendDiam    dbh_orig    new.band  \\\ncount        147.0  147.000000    147.0    6.000000  147.000000  147.000000   \nmean           1.0   80.126599  10045.0  629.500000  625.032650    0.013605   \nstd            0.0   37.011726      0.0   15.565989   14.113042    0.116242   \nmin            1.0   14.330000  10045.0  610.000000  611.300000    0.000000   \n25%            1.0   55.510000  10045.0  616.750000  611.300000    0.000000   \n50%            1.0   79.610000  10045.0  631.500000  627.799988    0.000000   \n75%            1.0  110.085000  10045.0  641.000000  627.800000    0.000000   \nmax            1.0  147.000000  10045.0  648.000000  654.000000    1.000000   \n\n         dendroID         dbh     scenario  \ncount  147.000000  147.000000   147.000000  \nmean   224.285714  634.753251    14.707483  \nstd    290.730210   12.742316   166.028518  \nmin    106.000000  611.300000     0.000000  \n25%    106.000000  624.144951     1.000000  \n50%    106.000000  631.291928     1.000000  \n75%    106.000000  644.381672     1.000000  \nmax    934.000000  658.634184  2014.000000  \ndbh index\nDatetimeIndex(['2010-01-27', '2011-01-15', '2011-05-09', '2011-05-13',\n               '2011-05-16', '2011-05-24', '2011-06-06', '2011-06-13',\n               '2011-06-20', '2011-06-27',\n               ...\n               '2019-09-05', '2019-10-02', '2019-10-17', '2019-11-06',\n               '2020-03-11', '2020-03-30', '2020-04-14', '2020-05-02',\n               '2020-05-15', '2020-05-31'],\n              dtype='datetime64[ns]', name='date', length=147, freq=None)\n\n\n\n\n\n\n\n\n\n# Select data from the year range 2014-2016\ndata_2014_2016 = data['2014-01-01':'2016-12-31']\n\n# Create a scatter plot for the 'dbh' column within the selected date range\nplt.figure(figsize=(12, 6))\nplt.scatter(data_2014_2016.index, data_2014_2016['dbh'], marker='o', s=30, label='Observed Data Points', color='b')\nplt.title(\"Scatter Plot of DBH (2014-2016)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"DBH (mm)\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n2. Decomposition Plots\nCreate a seasonal decomposition plot of this data to answer the following questions: 1. What is the approximate growth (in mm) per year trend for this tree 1. What is the approximate range in seasonal variation in growth around this trend? 1. When are the peaks and when are the valleys in this seasonal variation? 1. Are there periods of time where we might be skeptical of the above trend and seasonality?\nNote: You may need to do some data transformation as seen in the first DataCamp course to get a valid plot\n\nApproximate Growth (in mm) per Year Trend:4.7 The approximate annual growth trend can be estimated by examining the ‘Trend Component’ plot.\nApproximate Range in Seasonal Variation in Growth:[-1,1] The seasonal component represents the periodic fluctuations in growth. Values near 1 indicate the peak of the seasonal pattern, representing periods with higher-than-average values. Values near -1 indicate the trough of the seasonal pattern, representing periods with lower-than-average values. Values near 0 indicate average value.\nPeaks:The third quarter of each year Valleys:The second quarter of each year\nPeriods of skeption may be related to irregularities in the data. The residual component represents the unexplained variations in the data after accounting for the trend and seasonality. The residual plot shows some remaining seasonality patterns, also called autocorrelation, it can be an indication of seasonality that hasn’t been completely removed by the decomposition process. This suggests that there may be external factors or seasonal patterns that are not completely captured by the seasonal decomposition. It seems that the two abnormal increases in early 2011 and the first half of 2019 in the residual plot correspond to the lack of measurement at that time.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# The seasonal_decompose does not handle missing values in the input time series data. \n# Resample the data to fill in missing monthes using forward fill to ensure having data for each month, allowing to better analyze these patterns.\ndata_resampled = data['dbh'].resample('M').mean().interpolate()\nprint(\"Resample data:\")\nprint(data_resampled)\nresult = seasonal_decompose(data_resampled, model='additive')\n\n\ntrend = result.trend\nseasonal = result.seasonal\nresidual = result.resid\nprint(trend.describe())\n\n# Plot the components\nplt.figure(figsize=(12, 8))\n\nplt.subplot(411)\nplt.plot(data_resampled, label='Original Data')\nplt.legend()\nplt.title('Original Data')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend()\nplt.title('Trend Component')\n\nplt.subplot(413)\nplt.plot(seasonal, label='Seasonal')\nplt.legend()\nplt.title('Seasonal Component')\n# Format x-axis to show year and selected months (Jan, Apr, Jul, Oct)\nmonths_to_show = [1, 4, 7, 10]  # Months to display\nplt.gca().xaxis.set_major_locator(MonthLocator(bymonth=months_to_show, bymonthday=1))\nplt.gca().xaxis.set_major_formatter(DateFormatter(\"%Y-%b\"))\nplt.xticks(rotation=45) \n\nplt.subplot(414)\nplt.plot(residual, label='Residual')\nplt.legend()\nplt.title('Residual Component')\n\n\nplt.tight_layout()\n\nplt.show()\n\nResample data:\ndate\n2010-01-31    611.300000\n2010-02-28    611.695832\n2010-03-31    612.091663\n2010-04-30    612.487495\n2010-05-31    612.883327\n                 ...    \n2020-01-31    657.888662\n2020-02-29    657.904592\n2020-03-31    657.920522\n2020-04-30    658.548156\n2020-05-31    658.533295\nFreq: M, Name: dbh, Length: 125, dtype: float64\ncount    113.000000\nmean     633.512231\nstd       11.701697\nmin      613.674990\n25%      623.943440\n50%      632.240265\n75%      642.794528\nmax      657.540481\nName: trend, dtype: float64\n\n\n\n\n\n\napproximate_annual_growth = (data_resampled.iloc[-1] - data_resampled.iloc[0]) /(data_resampled.index.year[-1]-data_resampled.index.year[0])\nprint(approximate_annual_growth)\n\n4.723329467062672\n\n\n\n\n3. Forecasting using an AR(2) model\nIn this exercise you will use an AR(2) model as seen in the second datacamp course “Time Series Analysis in Python” to forecast this tree’s size exactly two years into the future. Unfortunately since we aren’t covering ARIMA models until later in the course, we won’t be able to:\n\nAscertain the appropriateness of using an AR(2) model. Ex: are all assumptions met?\nFully interpret the model output’s meaning\nDiagnose any issues.\n\nHowever, we will simply view this code as a minimally viable product that we will improve on later. Instructions: 1. Fit an appropriate AR(2) model 1. Print the summary of the result 1. Plot the TS data along with both in-sample fitted forecasts AND out-of-sample forecasts exactly two years into the future 1. Comment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem.\nAR(2,0,0) Model parameters: const 634.536762 ar.L1 1.406552 ar.L2 -0.407624 sigma2 0.629942\nAR(2,0,2) Model parameters: const 634.994306 ar.L1 1.575204 ar.L2 -0.576148 ma.L1 -0.195509 ma.L2 -0.018960 sigma2 0.626859\nAdjust the model to let forcast value in increasing trend: The moving average order is 2, which represents 2 lag terms of the forecast errors to include in the model. It captures past forecast errors and their influence on the current value. But the positive AIC indicated a bad fitting.(disappear after a few running…)\nThe out-of-sample forecast doesn’t capture seasonality because the ARIMA(2,0,2) model used in this example is a simple autoregressive model that doesn’t explicitly show model seasonality.To improve the forecast quality, we can experiment with different ARIMA orders or models specifically designed to capture seasonality.\n\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\n# Fit an AR(2) model\nmodel = ARIMA(data_resampled, order=(2, 0, 2))\nmodel_fitted = model.fit()\n\n# Print the summary of the result\nprint(model_fitted.summary())\nprint(model_fitted.params)\n\nfig, ax=plt.subplots()\ndata_resampled.plot(ax=ax)\nplot_predict(model_fitted,start=0, end='2022',ax=ax)\nplt.legend(loc='upper left')\nplt.show()\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                  125\nModel:                 ARIMA(2, 0, 2)   Log Likelihood                -151.598\nDate:                Tue, 24 Oct 2023   AIC                            315.196\nTime:                        10:41:24   BIC                            332.166\nSample:                    01-31-2010   HQIC                           322.090\n                         - 05-31-2020                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        634.9943     22.538     28.174      0.000     590.821     679.168\nar.L1          1.5752      0.397      3.967      0.000       0.797       2.353\nar.L2         -0.5761      0.398     -1.449      0.147      -1.355       0.203\nma.L1         -0.1955      0.403     -0.485      0.627      -0.985       0.594\nma.L2         -0.0190      0.174     -0.109      0.913      -0.360       0.322\nsigma2         0.6269      0.030     20.869      0.000       0.568       0.686\n===================================================================================\nLjung-Box (L1) (Q):                   0.66   Jarque-Bera (JB):              2322.77\nProb(Q):                              0.42   Prob(JB):                         0.00\nHeteroskedasticity (H):               3.29   Skew:                             3.50\nProb(H) (two-sided):                  0.00   Kurtosis:                        22.92\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nconst     634.994306\nar.L1       1.575204\nar.L2      -0.576148\nma.L1      -0.195509\nma.L2      -0.018960\nsigma2      0.626859\ndtype: float64\n\n\n\n\n\n\n# Fit an AR(2) model\nmodel2 = ARIMA(data_resampled, order=(2, 0, 0))\nmodel2_fitted = model2.fit()\n\n# Print the summary of the result\nprint(model2_fitted.summary())\nprint(model2_fitted.params)\n\nfig, ax=plt.subplots()\ndata_resampled.plot(ax=ax)\nplot_predict(model2_fitted,start=0, end='2022',ax=ax)\nplt.legend(loc='upper left')\nplt.show()\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                    dbh   No. Observations:                  125\nModel:                 ARIMA(2, 0, 0)   Log Likelihood                -151.911\nDate:                Tue, 24 Oct 2023   AIC                            311.822\nTime:                        10:41:59   BIC                            323.135\nSample:                    01-31-2010   HQIC                           316.417\n                         - 05-31-2020                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        634.5368     22.073     28.747      0.000     591.274     677.800\nar.L1          1.4066      0.056     25.173      0.000       1.297       1.516\nar.L2         -0.4076      0.058     -7.003      0.000      -0.522      -0.294\nsigma2         0.6299      0.030     21.244      0.000       0.572       0.688\n===================================================================================\nLjung-Box (L1) (Q):                   1.65   Jarque-Bera (JB):              2335.09\nProb(Q):                              0.20   Prob(JB):                         0.00\nHeteroskedasticity (H):               3.37   Skew:                             3.46\nProb(H) (two-sided):                  0.00   Kurtosis:                        23.01\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nconst     634.536762\nar.L1       1.406552\nar.L2      -0.407624\nsigma2      0.629942\ndtype: float64"
  },
  {
    "objectID": "PS/PS01_presentations/PS01_charavee_beata.html",
    "href": "PS/PS01_presentations/PS01_charavee_beata.html",
    "title": "Obtain a CSV of time series data",
    "section": "",
    "text": "# Problem Set 01\n1. Name: Charavee Basnet Chettri\n1. Field that you're interested in applying TS and forecasting to (ecology, econ, weather, etc): econ, ecology\n\nSyntaxError: unterminated string literal (detected at line 3) (1272513780.py, line 3)\n\n\n\nBy whatever means, get a .csv file of time series data relating to any topic: ecological, financial, etc.\nThere should be at least three variables of data\nIf you download from the web, include a link. If you use ChatGPT, include a link to your shared search\n\n\nWhere I got the data\n\ndata: https://www.zillow.com/research/data/ This particular dataset can be found under ‘LIST AND SALE PRICES’ named ‘Median List Price (Raw, All Homes, Weekly’ for geography ‘Metro & US’. Brief information: The dataset has the median list price for homes in cities. This is a raw data set. The data is recorded in a weekly frequency. The data ranges from 2018 to the present.\n\n\nimport pandas as pd\n\n## data manipulation\n\ndata = pd.read_csv(\"mlp_week.csv\")\ndata_drop = data.drop(columns=['RegionID', 'RegionType', 'StateName'])\n\n#pivoting data so that dates are in rows\ndata_long = pd.melt (data_drop, \n                     id_vars=['SizeRank','RegionName'],\n                     var_name= 'date',\n                     value_name= 'md_lp')\n\n#turn dates into datetime objects and set them as index\ndata_long['Date'] = pd.to_datetime(data_long['date'])\ndata_long.set_index('Date', inplace=True)\n\ndata_filtered = data_long[(data_long['SizeRank'] == 1) |\n                          (data_long['SizeRank'] == 2) |\n                          (data_long['SizeRank'] == 3) |\n                          (data_long['SizeRank'] == 937)]\n\n#pivot so that each variable is a city, state\ndata_wide = data_filtered.pivot(index = None,\n                                columns = 'RegionName',\n                                values = 'md_lp')\n\n\n\n1. Time series plots\n\nPlot the raw time series data\nIdentify any time series patterns\nIdentify any interesting trends insight\n\n\nimport matplotlib.pyplot as plt\n\n\ndata_wide.plot()\nplt.title('Median House List Price (Raw), 2018-Present')\nplt.xlabel('Date (weekly)')\nplt.ylabel('Median List Price(in millions)')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n\n&lt;matplotlib.legend.Legend at 0x1646d1f90&gt;\n\n\n\n\n\n\nObservations\n\nThe four cities from four different states were chosen according to their population size rank. Craig, CO (i.e. rank 937) with the intention to contextualize the median list prices of the top 3 cities by size.\nIn the given time frame, Los Angeles, CA sees the highest median house list prices touching approximately 1M dollars, followed by New York, NY. Furthermore, there’s a steep rise in prices post 2023 (which could due to the economic situation).\nThe median list prices in Chicago, IL, show consistency over the time period. It is interesting to note that there may be a slight seasonal pattern. We can see slight dips in prices at the end of each year.\nMedian list prices see an uptick in Craig, CO after the 2020. This trend could be correlated with the onset of the Covid-19 pandemic.\nPerhaps, regardless of the population, median home prices tends to increase over time, if not stay consistent.\n\n\n## Thought exercise, when graphing period-to-period change for each city.\nchng = data_wide.pct_change()\nchng.plot(subplots = True)\nplt.title('% Change in Median House List Price, 2018-Present')\nplt.xlabel('Date (weekly)')\nplt.ylabel('% Change')\n\nText(0, 0.5, '% Change')\n\n\n\n\n\n\n\n\n2. Scatterplots\n\nPlot a 3x3 grid of all pairwise scatterplots\nIdentify any interesting trends insight\n\n\nimport seaborn as sns\n\n\nsns.pairplot(data_wide, diag_kind='auto', markers=None)\n\n\n\n\n\nObservations\n\nOn average, there is a positive and stronger correlation between the variables. In other words, the median list price for houses in New York, NY increases with those in Los Angeles, CA. It is intuitive, because in addition to other demand and supply-side factors like inflation etc, the value and/or price of house or such property increases over time. (Hence, it is seen as an investment opportunity too).\nThere are some missing values, as seen by the time series visualizations as well.\nThe correlation between median list price levels are much stronger than among changes in median list prices. (graph below)\n\n\n## Thought Exercise \n\nsns.pairplot(chng, diag_kind='auto', markers=None)\n\n\n\n\n\n\n\n3. Lag & autocorrelation plots\n\nPlot a lag plot of any variable of your choice to visualize its relationship to itself in the past. Use a maximum \\(k\\) value of your choice\nPlot an autocorrelation plot of this same variable to quantify its relationship to itself in the past. Use a maximum \\(k\\) value of your choice\n\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n\n#1 week past lag\nvar = 'Los Angeles, CA'\nlag = -1\ndata_wide['lagged'] = data_wide[var].shift(lag)\n\n# Create a lag plot\nplt.figure(figsize=(8, 6))\nplt.scatter(data_wide[var], data_wide['lagged'], alpha=0.5)\nplt.title(f'Lag Plot of {var} (Lag={lag})')=\nplt.xlabel(f'{var} (t)')\nplt.ylabel(f'{var} (t{lag})')\nplt.show()\n\n\n\n\n\n#2 week past lag\nvar = 'Los Angeles, CA'\nlag = -2\n\ndata_wide['lagged'] = data_wide[var].shift(lag)\n\n# Create a lag plot\nplt.figure(figsize=(8, 6))\nplt.scatter(data_wide[var], data_wide['lagged'], alpha=0.5)\nplt.title(f'Lag Plot of {var} (Lag={lag})')\nplt.xlabel(f'{var} (t)')\nplt.ylabel(f'{var} (t{lag})')\nplt.show()\n\n\n\n\n\n#4 week past lag\nvar = 'Los Angeles, CA'\nlag = -3\n\ndata_wide['lagged'] = data_wide[var].shift(lag)\n\n# Create a lag plot\nplt.figure(figsize=(8, 6))\nplt.scatter(data_wide[var], data_wide['lagged'], alpha=0.5)\nplt.title(f'Lag Plot of {var} (Lag={lag})')\nplt.xlabel(f'{var} (t)')\nplt.ylabel(f'{var} (t{lag})')\nplt.show()\n\n\n\n\n\n#4 week past lag\nvar = 'Los Angeles, CA'\nlag = -4\n\ndata_wide['lagged'] = data_wide[var].shift(lag)\n\n# Create a lag plot\nplt.figure(figsize=(8, 6))\nplt.scatter(data_wide[var], data_wide['lagged'], alpha=0.5)\nplt.title(f'Lag Plot of {var} (Lag={lag})')\nplt.xlabel(f'{var} (t)')\nplt.ylabel(f'{var} (t{lag})')\nplt.show()\n\n\n\n\n\n#plt.figure(figsize=(8, 6))\nplot_acf(data_wide['Los Angeles, CA'].dropna(), alpha=0.5, lags=30)\nplt.show()\n\n\n\n\n\nObservations\n\nWe see momentum from our lag plot and autocorrelation, where the median list prices moves in the same direction as the lagged value. In this case that is positive and upwards.\n\n\n## Thought exercise \n\nautocorr1 = data_wide['Los Angeles, CA'].autocorr()\nautocorr2 = chng['Los Angeles, CA'].autocorr()\n\nprint (autocorr1, '&gt;', autocorr2)\n\n0.9967370507624936 &gt; 0.1934229694617877\n\n\n\n\nThings to potentially further explore\n\nI would forward fill the missing values for some variables (rather than dropping the NA afterwards)\nI would compare with the prices of rentals (hence, demand) since they are sectors of the same industry (to an extent).\nI would compare with income of people as well over time in these given regions. Though that would require resampling so that we compare across the same frequencies.\n\n\nI got help with coding from\n\nData Camp exercises\nchatgpt: https://chat.openai.com/share/292c583a-37df-4bcf-81d0-77eef88cfd67"
  },
  {
    "objectID": "PS/PS02_albert.html",
    "href": "PS/PS02_albert.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS02” that contains all the prompts you used for this problem set, click the share button, and paste the URL in Markdown format here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n1. Load and explore data\n\nLoad the CSV data of biweekly dendroband measures for tree with tag 82203\nLook at the raw data (rows and variables) and meta-data as Amir always told me to do\nNote that DBH is the only non-index variable we will work with, thus simplify the data frame to only have this variable\nPlot the time series of DBH (diameter at breast height, in mm) as we’ve done numerous times in DataCamp: just using .plot() with no arguments\nNow plot the time series where each of the 147 observed data point is marked with a dot. In other words, your plot should at the very least have these points.\nWhat is the difference in information presented between the two plots?\nDescribe any patterns you observe in the time series where each observed data point is marked with a dot\n\n\ntag_82203 = pd.read_csv('tag_82203.csv', parse_dates = ['date'], index_col = 'date')\ntag_82203.head()\n\n\n\n\n\n\n\n\ntag\nstemtag\nsurvey.ID\nyear\nmonth\nday\nintraannual\nsp\nmeasure\ncodes\nnotes\nstatus\nstemID\ndendDiam\ndbh_orig\nnew.band\ndendroID\ndbh\nscenario\ndata_source\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010-01-27\n82203\n1\n2010.01\n2010\n1\n27\n1\nlitu\n26.20\nNaN\nNaN\nalive\n10045\n610.0\n611.3\n1\n106\n611.300000\n0\nbiweekly_dbh\n\n\n2011-01-15\n82203\n1\n2011.01\n2011\n1\n15\n1\nlitu\n41.10\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n616.049980\n1\nbiweekly_dbh\n\n\n2011-05-09\n82203\n1\n2011.02\n2011\n5\n9\n1\nlitu\n40.62\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n615.896874\n1\nbiweekly_dbh\n\n\n2011-05-13\n82203\n1\n2011.03\n2011\n5\n13\n1\nlitu\n40.21\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n615.766085\n1\nbiweekly_dbh\n\n\n2011-05-16\n82203\n1\n2011.04\n2011\n5\n16\n1\nlitu\n42.55\nNaN\nNaN\nalive\n10045\nNaN\n611.3\n0\n106\n616.512610\n1\nbiweekly_dbh\n\n\n\n\n\n\n\n\ntag_82203.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 147 entries, 2010-01-27 to 2020-05-31\nData columns (total 20 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   tag          147 non-null    int64  \n 1   stemtag      147 non-null    int64  \n 2   survey.ID    147 non-null    float64\n 3   year         147 non-null    int64  \n 4   month        147 non-null    int64  \n 5   day          147 non-null    int64  \n 6   intraannual  147 non-null    int64  \n 7   sp           147 non-null    object \n 8   measure      147 non-null    float64\n 9   codes        1 non-null      object \n 10  notes        4 non-null      object \n 11  status       147 non-null    object \n 12  stemID       147 non-null    int64  \n 13  dendDiam     6 non-null      float64\n 14  dbh_orig     147 non-null    float64\n 15  new.band     147 non-null    int64  \n 16  dendroID     147 non-null    int64  \n 17  dbh          147 non-null    float64\n 18  scenario     147 non-null    int64  \n 19  data_source  147 non-null    object \ndtypes: float64(5), int64(10), object(5)\nmemory usage: 24.1+ KB\n\n\n\n# Recall to select a variable but return a data frame, not a series, you need to specify your variables as a list.\ntag_82203 = tag_82203[['dbh']]\n\n\ntag_82203.plot()\nplt.show()\n\n\n\n\n\ntag_82203.plot(marker='o', markersize=3)\nplt.show()\n\n\n\n\nThe original plot connects each consecutive point, so we have a hard time seeing at what interval the data is collected. The plot with the observed points marked with dots shows:\n\nThere was one value in 2010 and then a jump\nValues are not collected in winter\n\n\n\n2. Decomposition Plots\nCreate a seasonal decomposition plot of this data to answer the following questions: 1. What is the approximate growth (in mm) per year trend for this tree 1. What is the approximate range in seasonal variation in growth around this trend? 1. When are the peaks and when are the valleys in this seasonal variation? 1. Are there periods of time where we might be skeptical of the above trend and seasonality?\nNote: You may need to do some data transformation as seen in the first DataCamp course to get a valid plot\n\n# Downsample data from daily to monthly by taking average\ntag_82203_monthly = tag_82203.resample('m').mean()\n# Interpolate all missing values\ntag_82203_monthly.interpolate(inplace=True)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(tag_82203_monthly, model='additive')\n\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(411)\nplt.plot(tag_82203_monthly, label='Original')\nplt.legend(loc='upper left')\nplt.title('Original Time Series')\n\nplt.subplot(412)\nplt.plot(decomposition.trend, label='Trend')\nplt.legend(loc='upper left')\nplt.title('Trend Component')\n\nplt.subplot(413)\nplt.plot(decomposition.seasonal, label='Seasonality')\nplt.legend(loc='upper left')\nplt.title('Seasonal Component')\n\nplt.subplot(414)\nplt.plot(decomposition.resid, label='Residuals')\nplt.legend(loc='upper left')\nplt.title('Residual Component')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nIn the 9 years from 2011 to 2020, the tree grew from 620m to 650mm = (650-620)/9 = 3.33mm per year about\nPlus or minus 1 mm, so close to 1/3rd of annual growth!\nPeaks are in summer, valleys in late winter\nLooking are the residuals, I’d be suspicious of early values at 2011 and late values in 2019 and 2020\n\n\n\n3. Forecasting using an AR(2) model\nIn this exercise you will use an AR(2) model as seen in the second datacamp course “Time Series Analysis in Python” to forecast this tree’s size exactly two years into the future. Unfortunately since we aren’t covering ARIMA models until later in the course, we won’t be able to:\n\nAscertain the appropriateness of using an AR(2) model. Ex: are all assumptions met?\nFully interpret the model output’s meaning\nDiagnose any issues.\n\nHowever, we will simply view this code as a minimally viable product that we will improve on later. Instructions: 1. Fit an appropriate AR(2) model 1. Print the summary of the result 1. Plot the TS data along with both in-sample fitted forecasts AND out-of-sample forecasts exactly two years into the future 1. Comment on the quality of the out-of-sample forecast. If you have any ideas on how to improve the forecast state them, if not no problem.\n\n# Earliste date recorded is 2010-01-31\ntag2.head()\n\n\n\n\n\n\n\n\ndbh\n\n\ndate\n\n\n\n\n\n2010-01-31\n611.3\n\n\n2010-02-28\n611.3\n\n\n2010-03-31\n611.3\n\n\n2010-04-30\n611.3\n\n\n2010-05-31\n611.3\n\n\n\n\n\n\n\n\n# Last date recorded is 2020-05-31, so we will forecast upto 2022-05-31\ntag2.tail()\n\n\n\n\n\n\n\n\ndbh\n\n\ndate\n\n\n\n\n\n2020-01-31\n657.856802\n\n\n2020-02-29\n657.856802\n\n\n2020-03-31\n657.920522\n\n\n2020-04-30\n658.548156\n\n\n2020-05-31\n658.533295\n\n\n\n\n\n\n\n\n# Code from DataCamp \"Time Series Analysis in Python\" course, Chapter 3, Video 2, Exercise 2\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_predict\n\nmod = ARIMA(tag2, order=(2,0,0))\nresult = mod.fit()\nresult.summary()\n\n\nSARIMAX Results\n\n\nDep. Variable:\ndbh\nNo. Observations:\n125\n\n\nModel:\nARIMA(2, 0, 0)\nLog Likelihood\n-169.785\n\n\nDate:\nWed, 11 Oct 2023\nAIC\n347.570\n\n\nTime:\n11:20:47\nBIC\n358.883\n\n\nSample:\n01-31-2010\nHQIC\n352.166\n\n\n\n- 05-31-2020\n\n\n\n\nCovariance Type:\nopg\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n634.2981\n21.607\n29.356\n0.000\n591.949\n676.647\n\n\nar.L1\n1.3073\n0.071\n18.423\n0.000\n1.168\n1.446\n\n\nar.L2\n-0.3085\n0.071\n-4.365\n0.000\n-0.447\n-0.170\n\n\nsigma2\n0.8410\n0.036\n23.067\n0.000\n0.770\n0.912\n\n\n\n\n\n\nLjung-Box (L1) (Q):\n1.51\nJarque-Bera (JB):\n1459.08\n\n\nProb(Q):\n0.22\nProb(JB):\n0.00\n\n\nHeteroskedasticity (H):\n1.27\nSkew:\n3.31\n\n\nProb(H) (two-sided):\n0.45\nKurtosis:\n18.38\n\n\n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Plot\nfig, ax = plt.subplots()\ntag2.plot(ax=ax)\nplot_predict(result, start='2010-01-31', end='2021-05-31', ax=ax)\nplt.show()\n\n\n\n\nWe observe that for in-sample values from 2011 and onwards up until the last observation on 2020-05-31, the AR(2) model fits the data reasonably well. However the out-of-sample forecasts into the future make little sense.\nMaking taking differences will return better results? i.e. growth per month?\n\nmod = ARIMA(tag2.diff(), order=(2,0,0))\nresult = mod.fit()\n\nfig, ax = plt.subplots()\ntag2.diff().plot(ax=ax)\nplot_predict(result, start='2010-01-31', end='2021-05-31', ax=ax)\nplt.show()\n\n\n\n\nThe forecast of growth seems a little more reasonable, especially when forecasting out-of-sample. We could then undo the difference to get DBH size values. However, we are trusting the appropriateness of AR(2) on blind faith; hopefually we can revisit later in the semester."
  },
  {
    "objectID": "PS/PS01_albert.html#get-btc-data",
    "href": "PS/PS01_albert.html#get-btc-data",
    "title": "Problem Set 01",
    "section": "Get BTC data",
    "text": "Get BTC data\n\n# ChatGPT Code from https://chat.openai.com/share/572f7333-82a9-411f-95df-4f543fd3ae96\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nbtc_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(btc_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nbtc_df = df.set_index('Timestamp')\nbtc_df.rename(columns={\"Price\": \"BTC\"}, inplace = True)\n\nbtc_df.info()\nbtc_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:09:11\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nBTC\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n27968.128047\n\n\n2023-04-10\n28351.236994\n\n\n2023-04-11\n29657.974137\n\n\n2023-04-12\n30260.936109\n\n\n2023-04-13\n29904.138695"
  },
  {
    "objectID": "PS/PS01_albert.html#get-eth-data",
    "href": "PS/PS01_albert.html#get-eth-data",
    "title": "Problem Set 01",
    "section": "Get ETH data",
    "text": "Get ETH data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\neth_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(eth_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\neth_df = df.set_index('Timestamp')\neth_df.rename(columns={\"Price\": \"ETH\"}, inplace = True)\n\neth_df.info()\neth_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 05:28:41\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ETH     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nETH\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n1851.050671\n\n\n2023-04-10\n1859.940387\n\n\n2023-04-11\n1909.882061\n\n\n2023-04-12\n1892.938911\n\n\n2023-04-13\n1920.223031"
  },
  {
    "objectID": "PS/PS01_albert.html#get-sol-data",
    "href": "PS/PS01_albert.html#get-sol-data",
    "title": "Problem Set 01",
    "section": "Get SOL data",
    "text": "Get SOL data\n\n# CoinGecko API endpoint for historical market data\nurl = \"https://api.coingecko.com/api/v3/coins/solana/market_chart\"\n\n# Specify the number of days to retrieve (180 days for 6 months)\nparams = {\n    \"vs_currency\": \"usd\",\n    \"days\": \"180\",\n}\n\n# Send GET request to the API\nresponse = requests.get(url, params=params)\n\n# Get the BTC price data from the API response\nsol_price_data = response.json()[\"prices\"]\n\n# Convert the price data to a Pandas DataFrame\ndf = pd.DataFrame(sol_price_data, columns=[\"Timestamp\", \"Price\"])\n\n# Convert the timestamp to datetime\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n\n\n# Set index of data frame to be date\nsol_df = df.set_index('Timestamp')\nsol_df.rename(columns={\"Price\": \"SOL\"}, inplace = True)\n\nsol_df.info()\nsol_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 181 entries, 2023-04-09 00:00:00 to 2023-10-05 07:22:21\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   SOL     181 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\n\n\n\n\n\n\n\nSOL\n\n\nTimestamp\n\n\n\n\n\n2023-04-09\n20.042581\n\n\n2023-04-10\n20.304337\n\n\n2023-04-11\n20.875123\n\n\n2023-04-12\n23.005655\n\n\n2023-04-13\n23.830533\n\n\n\n\n\n\n\n\n# Merge all three data frames and save to csv\n# Export the DataFrame to a CSV file\ncrypto_df = btc_df.join(eth_df, how = 'inner').join(sol_df, how = 'inner')\ncrypto_df.info()\ncrypto_df.to_csv(\"crypto_price_data.csv\", index=False)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 180 entries, 2023-04-09 to 2023-10-05\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BTC     180 non-null    float64\n 1   ETH     180 non-null    float64\n 2   SOL     180 non-null    float64\ndtypes: float64(3)\nmemory usage: 5.6 KB\n\n\n\ncrypto_df.head()\n\n\n\n\n\n\n\n\nBTC\nETH\nSOL\n\n\nTimestamp\n\n\n\n\n\n\n\n2023-04-09\n27968.128047\n1851.050671\n20.042581\n\n\n2023-04-10\n28351.236994\n1859.940387\n20.304337\n\n\n2023-04-11\n29657.974137\n1909.882061\n20.875123\n\n\n2023-04-12\n30260.936109\n1892.938911\n23.005655\n\n\n2023-04-13\n29904.138695\n1920.223031\n23.830533"
  },
  {
    "objectID": "PS/PS04.html",
    "href": "PS/PS04.html",
    "title": "Problem Set 04",
    "section": "",
    "text": "Instructions: 1. Name: 1. Create a new ChatGPT chat called “SDS390 PS04” that contains all the prompts you used for this problem set. After you’ve completed your assignment, click the share button and paste the URL in Markdown format here (shared ChatGPT links don’t auto-update with subsequent queries).\nOverview of this PS: You will be recreating graphs and explicitly computing values from “FPP 7 and 8 - The forecaster’s toolbox” in python.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "PS/PS04.html#us-consumption",
    "href": "PS/PS04.html#us-consumption",
    "title": "Problem Set 04",
    "section": "US Consumption",
    "text": "US Consumption\nLoad and wrangle the us_consumption data set, explore the raw values, and plot the time series in an informative fashion."
  },
  {
    "objectID": "PS/PS04.html#algeria-exports",
    "href": "PS/PS04.html#algeria-exports",
    "title": "Problem Set 04",
    "section": "Algeria Exports",
    "text": "Algeria Exports\nLoad and wrangle the Algeria economy data set, explore the raw values, and plot the time series in an informative fashion."
  },
  {
    "objectID": "PS/PS04.html#australia-population",
    "href": "PS/PS04.html#australia-population",
    "title": "Problem Set 04",
    "section": "Australia Population",
    "text": "Australia Population\nLoad and wrangle the Australia economy data set, explore the raw values, and plot the time series in an informative fashion."
  },
  {
    "objectID": "PS/PS04.html#australia-tourism",
    "href": "PS/PS04.html#australia-tourism",
    "title": "Problem Set 04",
    "section": "Australia Tourism",
    "text": "Australia Tourism\nLoad and wrangle the Australia Holidays data set, explore the raw values, and plot the time series in an informative fashion."
  },
  {
    "objectID": "PS/PS01.html",
    "href": "PS/PS01.html",
    "title": "Problem Set 01",
    "section": "",
    "text": "Name:\nField that you’re interested in applying TS and forecasting to (ecology, econ, weather, etc):\n\n\nObtain a CSV of time series data\n\nBy whatever means, get a .csv file of time series data relating to any topic: ecological, financial, etc.\nThere should be at least three variables of data\nIf you download from the web, include a link. If you use ChatGPT, include a link to your shared search\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport requests\nimport pandas as pd\n\n\n\n1. Time series plots\n\nPlot the raw time series data\nIdentify any obvious patterns in your time series\nIdentify, if any, interesting insights these patterns suggest\n\n\n\n2. Scatterplots\n\nPlot a 3x3 grid of all pairwise scatterplots\nIdentify, if any, interesting trends or insight\n\n\n\n3. Lag & autocorrelation plots\n\nPlot a lag plot of any variable of your choice to visualize its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nPlot an autocorrelation plot of this same variable to quantify its relationship to itself in the past for all values of \\(k\\) between 1 and a maximum \\(k\\). Use a maximum \\(k\\) value of your choice\nIdentify, if any, interesting insight from these lag plots"
  },
  {
    "objectID": "PS/PS03_presentations/PS03_kiera_juniper.html",
    "href": "PS/PS03_presentations/PS03_kiera_juniper.html",
    "title": "Problem Set 03",
    "section": "",
    "text": "Instructions: 1. Name: Kiera Murray 1. ChatGPT queries here\nOverview of this PS: You will be recreating graphs and explicitly computing values from “FPP 5 - The forecaster’s toolbox” in python. Specifically\n\nFPP 5.2 - Some simple forecasting methods: Recreate Fig 5.7\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\nFPP 5.8 - Evaluating point forecast accuracy: Recompute 4 RMSE values in table using data wrangling\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\n\nOverall instructions:\n\nDo not use a function from a specialized time series forecasting specific python library to do this PS. Rather use pandas data wrangling, matplotlib, or any other package we’ve used to date (like for autocorrelation functions)\nDepending on your data wrangling approach for the questions below, you may get a warning that says “A value is trying to be set on a copy of a slice from a DataFrame.” As long as your values are correct, you may ignore this warning\nUsing the lessons you learned in the 3rd DataCamp on Data Viz, Chapters 1 and 2:\n\nGive all your plots titles\nLabel all axes\nMake any other cosmetic changes you like\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n\nLoad and explore data\n\naus_production = pd.read_csv(\"PS03_aus_production.csv\", parse_dates = ['Quarter'], index_col = 'Quarter')\nprint(aus_production.head())\nprint()\nprint(aus_production.tail())\n\n            Beer\nQuarter         \n1992-01-01   443\n1992-04-01   410\n1992-07-01   420\n1992-10-01   532\n1993-01-01   433\n\n            Beer\nQuarter         \n2009-04-01   398\n2009-07-01   419\n2009-10-01   488\n2010-01-01   414\n2010-04-01   374\n\n\n\n\nFPP 5.2: Recreate Fig 5.7\n\nRecall from Lec 7.2 this requires training (1992 Q1 - 2006 Q4) vs test set (2007 Q1 - 2010 Q2) splitting of data\n\n\n# split aus_production into training data and test data\ntraining = aus_production.loc['1992-01-01':'2006-10-01']\ntest = aus_production.loc['2007-01-01':'2010-10-01']\n\n# add mean forecast and naive forecast columns to test data\ntest[\"Mean_forecast\"] = training.values.mean()\ntest[\"Naive_forecast\"] = training.values.take(-1)\n\n# add seasonal naive forecast column to test data\nlast_quarter = training[\"Beer\"].tail(4).values\nsnaive_values = []\nfor i in range (0, len(test)):\n    index = i % len(last_quarter)\n    snaive_values.append(last_quarter[index])\ntest[\"Seasonal_naive_forecast\"] = snaive_values\n\n# show test data\ntest\n\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/2902619930.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Mean_forecast\"] = training.values.mean()\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/2902619930.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Naive_forecast\"] = training.values.take(-1)\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/2902619930.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Seasonal_naive_forecast\"] = snaive_values\n\n\n\n\n\n\n\n\n\nBeer\nMean_forecast\nNaive_forecast\nSeasonal_naive_forecast\n\n\nQuarter\n\n\n\n\n\n\n\n\n2007-01-01\n427\n436.45\n491\n438\n\n\n2007-04-01\n383\n436.45\n491\n386\n\n\n2007-07-01\n394\n436.45\n491\n405\n\n\n2007-10-01\n473\n436.45\n491\n491\n\n\n2008-01-01\n420\n436.45\n491\n438\n\n\n2008-04-01\n390\n436.45\n491\n386\n\n\n2008-07-01\n410\n436.45\n491\n405\n\n\n2008-10-01\n488\n436.45\n491\n491\n\n\n2009-01-01\n415\n436.45\n491\n438\n\n\n2009-04-01\n398\n436.45\n491\n386\n\n\n2009-07-01\n419\n436.45\n491\n405\n\n\n2009-10-01\n488\n436.45\n491\n491\n\n\n2010-01-01\n414\n436.45\n491\n438\n\n\n2010-04-01\n374\n436.45\n491\n386\n\n\n\n\n\n\n\n\ntraining[\"Beer\"].plot(c='black', label=\"Past beer production\")\ntest[\"Mean_forecast\"].plot(label=\"Mean method forecast\")\ntest[\"Naive_forecast\"].plot(label=\"Naive method forecast\")\ntest[\"Seasonal_naive_forecast\"].plot(label=\"Seasonal naive method forecast\")\ntest[\"Beer\"].plot(c='black', label=\"Actual future beer production\",  linestyle='dashed')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5),)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Australian beer production\\n(millions of litres)\")\nplt.title(\"Australian beer production: 3 forecasting methods\")\nplt.show()\n\n\n\n\n\n\nFPP 5.4 - Residual diagnostics: Recreate the 3 components of Fig 5.13\n\nCompute the three residuals diagnostics plot in Fig 5.13 for the naive method forecasts\nDo this for the training data in the Beer data above. This is because when doing a residual diagnostic of a model, you want to compare observed values \\(y_t\\) and fitted / predicted / forecasted values \\(\\hat{y}_t\\) for the data you used to fit the model.\nIt’s already obvious that there are much better choices than the naive method for forecasting. For each of the three residual diagnostics plots:\n\nComment on the residuals pattern you observe\nExplain why the pattern you observe is consistent with the fact that there are much better choices than the naive method for forecasting\n\n\n\n# create naive_residuals column\ntest[\"Naive_residuals\"] = test[\"Beer\"] - test[\"Naive_forecast\"]\n\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/1457876949.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Naive_residuals\"] = test[\"Beer\"] - test[\"Naive_forecast\"]\n\n\n\ntest[\"Naive_residuals\"].plot()\nplt.scatter(test.index, test['Naive_residuals'])\nplt.xlabel('Date')\nplt.ylabel('Residual value')\nplt.title('Naive method residuals')\nplt.show()\n\n\n\n\nThe naive method residuals are large, with a range of almost 100 on data whose average value is around 450, and are consistently negative. The residuals also show a clear pattern, with very high values in Q4 and low values in the other quarters (especially Q2). The residuals from a good forecast should be small, close to 0, and show no clear patterns. This plot above indicates the that naive method produces a poor forecast that is inaccurate, consistently overestimates the actual values, and fails to account for patterns in the data.\n\nplt.hist(test[\"Naive_residuals\"])\nplt.xlabel('Residual value')\nplt.ylabel('Count')\nplt.title('Histogram of residuals')\nplt.show()\n\n\n\n\nThough it’s hard to see with so few values spread over a large range, the naive method residuals are not normally distributed. The histogram is right-skewed, meaning that more of the residuals fall below the mean. This indicates a consistent error in the naive method’s predictions, as a good forecast should have normally distributed (i.e. random) residual error.\n\nplot_acf(test['Naive_residuals'], lags=len(test['Naive_residuals'])-1)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation of residuals')\nplt.show()\n\n\n\n\nThe naive method residuals show stastically significant autocorrelation at lag = 2. This indicates that the residuals are not independent, which means that there is some pattern the forecast is failing to consider. The clear pattern in the autocorrelation values across increasing lags is a product of the naive method forecast not accounting for the data’s seasonality.\n\n\nFPP 5.8 - Evaluating point forecast accuracy: Recompute RMSE and MAE values in table using data wrangling\nFor the three forecasting methods above, compute the:\n\nRoot mean squared error\nMean absolute error\n\nwhich gives a single numerical measure of the overall error of the model.\nNote that the table in FPP 5.8 uses a slightly different training set 1992 Q1 - 2007 Q4, instead of 1992 Q1 - 2006 Q4, so you will get slightly different error values, but they should still be close.\n\n# create mean_residuals and seasonal_naive_residuals columns\ntest[\"Mean_residuals\"] = test[\"Beer\"] - test[\"Mean_forecast\"]\ntest[\"Seasonal_naive_residuals\"] = test[\"Beer\"] - test[\"Seasonal_naive_forecast\"]\n\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/435131911.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Mean_residuals\"] = test[\"Beer\"] - test[\"Mean_forecast\"]\n/var/folders/qc/qz8fv61s4xs8w475xp4n7d840000gn/T/ipykernel_97026/435131911.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Seasonal_naive_residuals\"] = test[\"Beer\"] - test[\"Seasonal_naive_forecast\"]\n\n\n\n# create scores dataframe containing RMSE and MAE for the 3 residuals columns in test data\nscores = pd.DataFrame({\"Residuals\":[\"Mean_residuals\", \"Naive_residuals\", \"Seasonal_naive_residuals\"]})\nscores.set_index(\"Residuals\", inplace=True)\nscores[\"RMSE\"] = np.sqrt(np.square(test[scores.index]).mean()) # square root of the mean of the squared values for each column in test data\nscores[\"RMSE\"] = np.round(scores[\"RMSE\"], 2) # round to 2 decimal places\nscores[\"MAE\"] = np.absolute(test[scores.index]).mean() # mean of the absolute values for each column in test data\nscores[\"MAE\"] = np.round(scores[\"MAE\"], 2) # round to 2 decimal places\nscores\n\n\n\n\n\n\n\n\nRMSE\nMAE\n\n\nResiduals\n\n\n\n\n\n\nMean_residuals\n38.89\n35.47\n\n\nNaive_residuals\n78.62\n70.07\n\n\nSeasonal_naive_residuals\n13.49\n11.50\n\n\n\n\n\n\n\n\n\nFPP 5.7 - Forecasting with decomposition: Recreate Fig 5.19 by computing all values using data wrangling\nI didn’t have time to scaffold this question appropriately."
  }
]