---
title: "SDS390: Ecological Forecasting"
---


<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQw8rrRUpbMRdy01pwUDw4C65XK1ekgQZWUeWN66L5KrAwtXuWMuo1EEk-2qULxsYPforSIAIsEI1dY/pubhtml?gid=538580312&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="800"></iframe>









# Lec 5.1: Tue 10/3

## Announcements

1. PS1 mini-presentations on Thursday
1. PS1 submission format posted [below](#problem-set-1)


## Lecture

1. Lec 4.2 on FPP 3.4 Classical Decomposition, second attempt
    1. What is $m$ used in example?
    1. How is seasonal component $S_t$ computed? What is assumed seasonality?
    1. Code over code block below
1. FFP 3.5 Briefly discuss other decomposition methods used
1. FFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of $m$-MA smoothing like for classical decomposition
    ![LOESS](https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/_images/loess.gif) 


```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1: Recreate Fig 3.13 ----
# 1.a) Get data
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

# Note index is 1M = monthly. This sets m=12 in m-MA method below
us_retail_employment

# 1.b) Recreate Fig 3.13
us_retail_employment %>%
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")



# Code block 2: Recreate all 4 subplots in Fig 3.13 ----
# Get data frame with full breakdown of decomposition
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() %>% 
  # Convert from tsibble to regular tibble for data wrangling
  as_tibble()

# 2.a) Top plot: Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# 2.b) 2nd plot: trend-cycle via MA average method
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# 2.c) Extra: detrended plot
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Detrended data Original data minus trend-cycle component") +
  geom_hline(yintercept = 0, col = "red")

# 2.d) 3rd plot: seasonality (values repeat every 12 months)
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "Seasonality")

# 2.e) 4th plot: Compute remainders
ggplot(full_decomposition, aes(x=Month, y = random)) +
  geom_line() +
  labs(title = "Remainder i.e. noise") +
  geom_hline(yintercept = 0, col = "red")



# Code block 3:  Compute full breakdown for one row: 1990 July ----
# 3.a) LOOK at data
# IMO most important function in RStudio: View()
# Focus on Row 7 1990 July: where do all these values come from?
View(full_decomposition)

# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA
# Compute mean of first 12 values
full_decomposition %>% 
  slice(1:12) %>% 
  summarize(mean = mean(Employed))
# Compute mean of next 12 values
full_decomposition %>% 
  slice(2:13) %>% 
  summarize(mean = mean(Employed))
# Now do second averaging of 2 values
(13186 + 13170)/2

# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625
full_decomposition <- full_decomposition %>% 
  mutate(detrended = Employed - trend)

# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661
full_decomposition %>% 
  mutate(month_num = month(Month)) %>% 
  group_by(month_num) %>% 
  summarize(St = mean(detrended, na.rm = TRUE))

# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610
# y_t - T_hat_t - S_hat_t
13170.1 - 13177.76 - (-13.311661)
```



***



# Lec 4.2: Thu 9/28

## Announcements

1. Comment on the generalizability of everything I say


## Lecture

1. 

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Modified version of code to produce FPP Fig 3.10
aus_exports <- global_economy |>
  filter(Country == "Australia") 

# Note number of rows
aus_exports

# Set m and plot
m <- 28
aus_exports |>
  mutate(
    `m-MA` = slider::slide_dbl(Exports, mean,
                .before = m, .after = m, .complete = TRUE)
  ) |>
  autoplot(Exports) +
  geom_line(aes(y = `m-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") +
  guides(colour = guide_legend(title = "series"))


# Code block 2 ----
# Classical decomposition breakdown

# Plot TS data in questions
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
us_retail_employment

# Code to create full Fig 3.13
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

# Get data frame with all the decomposition parts
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components()

full_decomposition

# Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# Step 2: Subtract trend-cycle from original data
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Original data minus trend-cycle component")

# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "For each season compute average")

# Step 4: Compute remainders. 4th plot of Fig 3.13
ggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +
  geom_line() +
  labs(title = "Remainder i.e. noise")
```




***



# Lec 4.1: Tue 9/26

## Announcements

1. The next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due ~~Tue 10/3~~ Thu 10/5 9:25am.
1. I'm keeping up with screencasts, still need to finish Chapter 4. 
1. Slido [responses](https://admin.sli.do/event/hF9K8zvE9KGPCwexfM1YJu/analytics):
    1. "I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I'm doing a small portion of a data analysis. I find myself "going through the motions" and feeling it is tedious because I don't think I'm really comprehending the importance of each step."
    1. "Making stupid mistakes on the syntax and kind of confused about the difference between `[]` and `.` when calling an attribute"
    1. "Worried I won't retain my understanding"
    1. "I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it"
1. Main tip: "Optimal frustration"


## Lecture

1. FPP 3.1 Transformations. $\log10$ transformations:
    1. What are $\log$ (base $e$) and $\log10$ (base 10) tranformations? [Example table](https://moderndive.com/A-appendixA.html#appendix-log10-transformations)
    1. Effect on visualizations: [Example figure](https://moderndive.com/11-thinking-with-data.html#fig:log10-price-viz)
1. FPP 3.2 Time series decompositions


## Problem Set 1

1. Posted on Slack under `#questions`
1. Individual `PS01.ipynb` files
    1. Due Thu 10/5 9:25am on moodle (see Moodle link on top right of page)
    1. Submit both a `PS01.ipynb` where all code is reproducible and a `.csv` file of your data
1. In-class on Thu 10/5: "Think, Pair, Share" exercise
    1. I will randomly create teams of pairs. Any remaining odd number student will be paired with me.
    1. You will show each other your code and prepare a single mini-presentation `.ipynb`
    1. I will pick 2-3 pairs at random to present their work in front of the class
    1. You will rate your peer's preparation using this [Google Form](https://docs.google.com/forms/d/e/1FAIpQLSe6uHGVCgcHwBae-k00k6GXxmWKAhouNWivH-3NSim020bgww/viewform)
    
    
  
Clarifications added afterwards:

1. Dataset should be at least 100 rows
1. For mini-presentation, you will have to choose one of the two datasets
1. Do problem set in python



***



# Lec 3.2: Thu 9/21

## Announcements

1. First problem set assigned on Tue, which will build into first mini-presentation
1. Mountain Day recap
1. Originally assigned course ["Manipulating Time Series Data in Python"](https://app.datacamp.com/learn/courses/manipulating-time-series-data-in-python) due next Tue 9/26 before class 
1. Go to Roster Google Sheet (top right of page) and fill DC columns


## Lecture

1. DataCamp
    1. Poll class on [sli.do](https://app.sli.do/event/hF9K8zvE9KGPCwexfM1YJu) about DC
    1. DC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.
    1. Prof. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3
    1. Screencasts location
    1. Continuing [Time Series with Python](https://app.datacamp.com/learn/skill-tracks/time-series-with-python) skill track, the next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due Tue 10/3. If there is an Exercise you'd like me to do in class, let me know. 
1. Finish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Lag plots: relationship of a TS variable with itself in the past
# Create regular TS plot of data in Fig 2.19 Beer production over time
recent_production <- aus_production |>
  filter(year(Quarter) >= 2000)

# Note time index meta-data = 1Q = quarter
recent_production

# Note patterns
recent_production |> autoplot(Beer)
```



***



# Lec 2.2: Thu 9/14

- Slido results from [last time](https://analytics.slido.com/share/XIJcW_05mEyupBJ2tuIT0cTYhCVNaSKt_S6IXFz9P_85KW39KIJcX_0zn_0tn_0sn_04mEwpT_cBSQUZ7NfeRoClIynj_ST2bmxub3FsZXFubP0vmHyIUAu_SVrj9mYafEeNA6EIvbcFZweiMI4O1A7kdCS5bg)
- ChatGPT result on global temperature
- Install `fpp3` R package
- Chalk talk on FPP Chapter 2: Time Series Graphics. During chalk talk I will refer to "code blocks" below. Feel free to copy to a `playground.qmd` file.
- Canada wins gold medal in ice hockey in overtime at 2010 Vancouver Winter Olympics
    - The [Golden Goal ðŸ¥‡](https://www.youtube.com/watch?v=GBMriA6maIU&t=18s)
    - A clear [cyclical pattern](https://www.zdnet.com/a/img/resize/51e841b3331a3340a2b95219ddc1a7826fcaee31/2013/11/13/e384930f-4c5f-11e3-90a0-0291187ef9b6/epcor_edmonton_water_usage_flush_olympic_gold_game.jpg?auto=webp&fit=crop&height=1200&width=1200) in water consumption (from ðŸš½ðŸš¾ðŸ§»)


```{r eval=FALSE, echo=TRUE}
# Lec 2.2 Code
library(fpp3)

# Code block 1 ----
# Compare meta-data of data_tibble and data_tsibble
data_tibble <- tibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110)
)
data_tibble

# Set variable that indexes the data: Year
data_tsibble <- tsibble(data_tibble, index = "Year")
data_tsibble


# Code block 2 ----
# Scatterplot of relationship between two time series
# Code for Fig. 2.14:
vic_elec |>
  filter(year(Time) == 2014) |>
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")

# Code for Fig 2.14 with (simplified) notion of time
vic_elec |>
  filter(year(Time) == 2014) |>
  # Add this:
  mutate(month = factor(month(Date))) |>
  # Add color scale
  ggplot(aes(x = Temperature, y = Demand, col = month)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")
```



***



# Lec 2.1: Tue 9/12

1. Join `#questions` channel on Slack
1. Fill in your information on roster
1. ChatGPT citation mechanism: share link
1. Developing a python/jupyter notebook workflow
    1. Open a jupyter notebook
    1. Start "Manipulating Time Series Data in Python" DataCamp course, to be completed by Tue 9/19 before class
    1. Screencast recording [here](https://drive.google.com/file/d/1kHVmegmsxClToWpUjak6s2cwcRZJ8vJ8/view?usp=sharing)



***



# Lec 1.2: Thu 9/7

Install software and computing: See syllabus
