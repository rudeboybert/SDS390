---
title: "SDS390: Ecological Forecasting"
---

<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQw8rrRUpbMRdy01pwUDw4C65XK1ekgQZWUeWN66L5KrAwtXuWMuo1EEk-2qULxsYPforSIAIsEI1dY/pubhtml?gid=538580312&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="850">

</iframe>

```{=html}
<!--
Things to do next time:
1. DC:
    1. Don't do first DC class first. Maybe second?
    1. Do second DataCamp class after you've covered AR and MA process in class
    1. Do third DC class first? Data viz is important
1. PS3: Finish forecasting with decomposition data wrangling question





Things to do this semester:
1. Talk about DC grades, check-in
1. 

-->
```


# Lec 11.2: Thu 11/16

## Announcements

1. Residual analysis: on which data? train or test or both?
1. Updated `final_project.ipynb` file



------------------------------------------------------------------------




# Lec 11.1: Tue 11/14

## Announcements

1. PS3 mini-presentations today. Info in [Problem Sets](PS.html)
1. Finalized final project information posted in [Final Project](final_project.html) 



------------------------------------------------------------------------



# Lec 10.2: Thu 11/9

## Announcements

1. In-class guest lecture today with Prof. Will Hopper
1. Work on PS3. Note link to rendered output of all previous PS `.ipynb` files have been posted in [Problem Sets](PS.html)




------------------------------------------------------------------------




# Lec 10.1: Tue 11/7

## Announcements

1. Zoom lecture today starting at ~~regular class time~~ 10am (Zoom link sent on Slack)
1. Answer Poll Everywhere question in Slack `#general` on whether you prefer group or individual final projects
1. Discuss final project. Info in [Problem Sets](PS.html)





------------------------------------------------------------------------




# Lec 9.1: Tue 10/31

## Announcements

1. No lecture on Thursday from Cromwell Day
1. Zoom lecture on Tuesday 10/7
1. In-class lecture on Thursday 10/9 with Prof. Will Hopper


## Lecture

1. Start PS3
1. 




------------------------------------------------------------------------




# Lec 8.2: Thu 10/26

## Announcements

1. Fill out PS2 peer eval Google Form and slido poll

## Lecture

1. FPP 5.5: Prediction intervals
1. FPP 5.7: Forecasting with STL decomposition method
1. FPP 5.8: Assessing prediction accuracy




------------------------------------------------------------------------




# Lec 8.1: Tue 10/24

## Announcements

1. PS2 mini-presentations today. Info in [Problem Sets](PS.html)

## Lecture

1. 




------------------------------------------------------------------------




# Lec 7.2: Thu 10/19

## Announcements

1. Update PS2 in-class presentation format in [Problem Sets](PS.html)
1. PS2 mini-presentations this coming Tuesday

## Lecture

1. Code blocks below
1. [Refresher](https://moderndive.com/10-inference-for-regression.html#regression-conditions) on conditions for inference for linear regression



```{r, eval=FALSE}
library(fpp3)

# Code block 1: Fit model to data and generate forecasts ----
# Tidy:
full_data <- aus_production %>% 
  filter_index("1992 Q1" ~ "2010 Q2")

# Visualize:
full_data %>% 
  autoplot(Beer)

# Specify: Take full data and fit 3 models
beer_fit_1 <- full_data |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )

# Generate forecasts 14 time periods (quarters) into future
beer_forecast_1 <- beer_fit_1 |> 
  forecast(h = 14)

# Plot full data and forecasts
plot1 <- beer_forecast_1 |>
  autoplot(full_data, level = NULL) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))

plot1



# Code block 2: Training test set approach ----
# Define training data to be up to 2007: fit the model to this data
training_data <- full_data |>
  filter_index("1992 Q1" ~ "2006 Q4")

# Define test data to be 2007 and beyond:
test_data <- full_data |>
  filter_index("2007 Q1" ~ "2010 Q2")

# Fit 3 models only to training data
beer_fit_2 <- training_data |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )

# Generate forecasts for 14 quarters
beer_forecast_2 <- beer_fit_2 |> 
  forecast(h = 14)

# Plot training data and forecasts only
plot2 <- beer_forecast_2 |>
  autoplot(training_data, level = NULL) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
plot2
`
# Add the test data. We can now evaluate quality of predictions
plot2 +
  autolayer(test_data, colour = "black")



# Code block 3: Residual analysis ----
# Get residuals
augment(beer_fit_1) %>% 
  View()

# Focus only on seasonal naive method:
seasonal_naive_augment <- augment(beer_fit_1) %>% 
  filter(.model == "Seasonal naïve")

# Check 2 and 3: Mean 0 and constant variance
seasonal_naive_augment %>% 
  autoplot(.resid) +
  geom_hline(yintercept=0, linetype = "dashed")

# Check 1: Uncorrelated residuals
seasonal_naive_augment %>% 
  ACF(.resid) %>% 
  autoplot() +
  labs(title = "Residuals from the naïve method")

# Check 2 and 4: Zero mean and normally distributed
ggplot(seasonal_naive_augment, aes(x = .resid)) +
  geom_histogram(bins = 15)
```



------------------------------------------------------------------------






# Lec 7.1: Tue 10/17

## Announcements

1. Next DC course "Visualizing Time Series Data in Python" assigned due Tue 10/31
1. Discuss PS2 in-class presentation format on Tue 10/24 using PS1 feedback from [slido](https://admin.sli.do/event/3x3vyqpgVxaVxm6XTJLxxF/analytics)

## Lecture

```{r, eval=FALSE, echo=TRUE}
library(fpp3)
library(feasts)

# Code block 1: FPP 4.2 ACF features ----
# Reprint output from book
tourism |> 
  features(Trips, feat_acf)

# a) Focus on only top row of output from book
top_row <- tourism %>% 
  filter(Region == "Adelaide", State == "South Australia", Purpose == "Business")

top_row |> 
  features(Trips, feat_acf)

# LOOK AT YOUR DATA!!
top_row %>% 
  autoplot(Trips)

# b) ACF at k=1 is acf1 in output from book
top_row %>% 
  ACF(Trips) %>% 
  autoplot()

# c) Take differences of data using difference() from FPP 9.1
# https://otexts.com/fpp3/stationarity.html#differencing
# For example switch from size to growth
top_row %>% 
  autoplot(difference(Trips))

# ACF at k=1 is diff1_acf1 in output from book
top_row %>% 
  ACF(difference(Trips)) %>% 
  autoplot()



# Code block 2: FPP 4.3 ACF features ----
# Compare and contrast two classical decomposition features

# a) FPP 4.3 Australia trips
top_row %>% 
  model(
    classical_decomposition(Trips, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total trips")

# b) FPP 3.4 Fig 3.13 from Lec 5.1
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

us_retail_employment %>%
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

# c) compare trend_strength and seasonal_strength_year
top_row |>
  features(Trips, feat_stl) %>% 
  View()

us_retail_employment |>
  features(Employed, feat_stl) %>% 
  View()
```





------------------------------------------------------------------------



# Lec 6.2: Thu 10/12

## Announcements

1.  Still missing a few [PS1 peer evalutions](https://docs.google.com/forms/d/e/1FAIpQLSe6uHGVCgcHwBae-k00k6GXxmWKAhouNWivH-3NSim020bgww/viewform)
2.  Discuss slido [responses](https://admin.sli.do/event/3x3vyqpgVxaVxm6XTJLxxF/analytics)

## Lecture

1.  "Time Series Analysis in Python" DataCamp course Chapters 3-5 of this course focused on "autoregressive integrated moving average" (ARIMA) models. This is covered in:
    1.  [FPP Chapter 9](https://otexts.com/fpp3/arima.html)
    2.  The 4th of 5 courses in "Time Series with Python" DC skill track (see schedule above).
2.  Chalk talk
3.  Start PS2 (on new "Problem Sets" tab of webpage)

```{=html}
<!--
1. Chapter 1, Video 2, Exercise 1: Sometimes it doesn't make sense to study TS original values, but instead differences or % changes or returns of values. For example stock returns.
1. Chapter 2, Video 1, Exercise 2: If for a lag value of $k$ the autocorrelation function is statistically indistinguishable from 0, then there is no information for forecasting.
1. Chapter 3, Video 1, Exercise 1 and Video 2, Exercise 1: Purpose of a simulation study
1. Chapter 3, Video 4, Exercise 2: Model selection
1. Chapter 4, Video 2, Exercise 2: in-sample and out-of-sample forecasting
-->
```



------------------------------------------------------------------------

# Lec 5.2: Thu 10/5

## Announcements

1.  

## Lecture

1. Problem Set 01 in-class presentations

------------------------------------------------------------------------

# Lec 5.1: Tue 10/3

## Announcements

1.  PS1 mini-presentations on Thursday
2.  PS1 submission format posted [below](#problem-set-1)

## Lecture

1.  Lec 4.2 on FPP 3.4 Classical Decomposition, second attempt
    1.  What is $m$ used in example?
    2.  How is seasonal component $S_t$ computed? What is assumed seasonality?
    3.  Code over code block below
2.  FFP 3.5 Briefly discuss other decomposition methods used
3.  FFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of $m$-MA smoothing like for classical decomposition ![LOESS](https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/_images/loess.gif)

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1: Recreate Fig 3.13 ----
# 1.a) Get data
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

# Note index is 1M = monthly. This sets m=12 in m-MA method below
us_retail_employment

# 1.b) Recreate Fig 3.13
us_retail_employment %>%
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")



# Code block 2: Recreate all 4 subplots in Fig 3.13 ----
# Get data frame with full breakdown of decomposition
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() %>% 
  # Convert from tsibble to regular tibble for data wrangling
  as_tibble()

# 2.a) Top plot: Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# 2.b) 2nd plot: trend-cycle via MA average method
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# 2.c) Extra: detrended plot
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Detrended data Original data minus trend-cycle component") +
  geom_hline(yintercept = 0, col = "red")

# 2.d) 3rd plot: seasonality (values repeat every 12 months)
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "Seasonality")

# 2.e) 4th plot: Compute remainders
ggplot(full_decomposition, aes(x=Month, y = random)) +
  geom_line() +
  labs(title = "Remainder i.e. noise") +
  geom_hline(yintercept = 0, col = "red")



# Code block 3:  Compute full breakdown for one row: 1990 July ----
# 3.a) LOOK at data
# IMO most important function in RStudio: View()
# Focus on Row 7 1990 July: where do all these values come from?
View(full_decomposition)

# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA
# Compute mean of first 12 values
full_decomposition %>% 
  slice(1:12) %>% 
  summarize(mean = mean(Employed))
# Compute mean of next 12 values
full_decomposition %>% 
  slice(2:13) %>% 
  summarize(mean = mean(Employed))
# Now do second averaging of 2 values
(13186 + 13170)/2

# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625
full_decomposition <- full_decomposition %>% 
  mutate(detrended = Employed - trend)

# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661
full_decomposition %>% 
  mutate(month_num = month(Month)) %>% 
  group_by(month_num) %>% 
  summarize(St = mean(detrended, na.rm = TRUE))

# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610
# y_t - T_hat_t - S_hat_t
13170.1 - 13177.76 - (-13.311661)
```

------------------------------------------------------------------------

# Lec 4.2: Thu 9/28

## Announcements

1.  Comment on the generalizability of everything I say

## Lecture

1.  

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Modified version of code to produce FPP Fig 3.10
aus_exports <- global_economy |>
  filter(Country == "Australia") 

# Note number of rows
aus_exports

# Set m and plot
m <- 28
aus_exports |>
  mutate(
    `m-MA` = slider::slide_dbl(Exports, mean,
                .before = m, .after = m, .complete = TRUE)
  ) |>
  autoplot(Exports) +
  geom_line(aes(y = `m-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") +
  guides(colour = guide_legend(title = "series"))


# Code block 2 ----
# Classical decomposition breakdown

# Plot TS data in questions
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
us_retail_employment

# Code to create full Fig 3.13
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

# Get data frame with all the decomposition parts
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components()

full_decomposition

# Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# Step 2: Subtract trend-cycle from original data
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Original data minus trend-cycle component")

# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "For each season compute average")

# Step 4: Compute remainders. 4th plot of Fig 3.13
ggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +
  geom_line() +
  labs(title = "Remainder i.e. noise")
```

------------------------------------------------------------------------

# Lec 4.1: Tue 9/26

## Announcements

1.  The next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due ~~Tue 10/3~~ Thu 10/5 9:25am.
2.  I'm keeping up with screencasts, still need to finish Chapter 4.
3.  Slido [responses](https://admin.sli.do/event/hF9K8zvE9KGPCwexfM1YJu/analytics):
    1.  "I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I'm doing a small portion of a data analysis. I find myself"going through the motions" and feeling it is tedious because I don't think I'm really comprehending the importance of each step."
    2.  "Making stupid mistakes on the syntax and kind of confused about the difference between `[]` and `.` when calling an attribute"
    3.  "Worried I won't retain my understanding"
    4.  "I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it"
4.  Main tip: "Optimal frustration"

## Lecture

1.  FPP 3.1 Transformations. $\log10$ transformations:
    1.  What are $\log$ (base $e$) and $\log10$ (base 10) tranformations? [Example table](https://moderndive.com/A-appendixA.html#appendix-log10-transformations)
    2.  Effect on visualizations: [Example figure](https://moderndive.com/11-thinking-with-data.html#fig:log10-price-viz)
2.  FPP 3.2 Time series decompositions
3. Problem Set 1 posted

## Problem Set 1 {#problem-set-1}



------------------------------------------------------------------------

# Lec 3.2: Thu 9/21

## Announcements

1.  First problem set assigned on Tue, which will build into first mini-presentation
2.  Mountain Day recap
3.  Originally assigned course ["Manipulating Time Series Data in Python"](https://app.datacamp.com/learn/courses/manipulating-time-series-data-in-python) due next Tue 9/26 before class
4.  Go to Roster Google Sheet (top right of page) and fill DC columns

## Lecture

1.  DataCamp
    1.  Poll class on [sli.do](https://app.sli.do/event/hF9K8zvE9KGPCwexfM1YJu) about DC
    2.  DC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.
    3.  Prof. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3
    4.  Screencasts location
    5.  Continuing [Time Series with Python](https://app.datacamp.com/learn/skill-tracks/time-series-with-python) skill track, the next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due Tue 10/3. If there is an Exercise you'd like me to do in class, let me know.
2.  Finish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Lag plots: relationship of a TS variable with itself in the past
# Create regular TS plot of data in Fig 2.19 Beer production over time
recent_production <- aus_production |>
  filter(year(Quarter) >= 2000)

# Note time index meta-data = 1Q = quarter
recent_production

# Note patterns
recent_production |> autoplot(Beer)
```

------------------------------------------------------------------------

# Lec 2.2: Thu 9/14

-   Slido results from [last time](https://analytics.slido.com/share/XIJcW_05mEyupBJ2tuIT0cTYhCVNaSKt_S6IXFz9P_85KW39KIJcX_0zn_0tn_0sn_04mEwpT_cBSQUZ7NfeRoClIynj_ST2bmxub3FsZXFubP0vmHyIUAu_SVrj9mYafEeNA6EIvbcFZweiMI4O1A7kdCS5bg)
-   ChatGPT result on global temperature
-   Install `fpp3` R package
-   Chalk talk on FPP Chapter 2: Time Series Graphics. During chalk talk I will refer to "code blocks" below. Feel free to copy to a `playground.qmd` file.
-   Canada wins gold medal in ice hockey in overtime at 2010 Vancouver Winter Olympics
    -   The [Golden Goal 🥇](https://www.youtube.com/watch?v=GBMriA6maIU&t=18s)
    -   A clear [cyclical pattern](https://www.zdnet.com/a/img/resize/51e841b3331a3340a2b95219ddc1a7826fcaee31/2013/11/13/e384930f-4c5f-11e3-90a0-0291187ef9b6/epcor_edmonton_water_usage_flush_olympic_gold_game.jpg?auto=webp&fit=crop&height=1200&width=1200) in water consumption (from 🚽🚾🧻)

```{r eval=FALSE, echo=TRUE}
# Lec 2.2 Code
library(fpp3)

# Code block 1 ----
# Compare meta-data of data_tibble and data_tsibble
data_tibble <- tibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110)
)
data_tibble

# Set variable that indexes the data: Year
data_tsibble <- tsibble(data_tibble, index = "Year")
data_tsibble


# Code block 2 ----
# Scatterplot of relationship between two time series
# Code for Fig. 2.14:
vic_elec |>
  filter(year(Time) == 2014) |>
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")

# Code for Fig 2.14 with (simplified) notion of time
vic_elec |>
  filter(year(Time) == 2014) |>
  # Add this:
  mutate(month = factor(month(Date))) |>
  # Add color scale
  ggplot(aes(x = Temperature, y = Demand, col = month)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")
```

------------------------------------------------------------------------

# Lec 2.1: Tue 9/12

1.  Join `#questions` channel on Slack
2.  Fill in your information on roster
3.  ChatGPT citation mechanism: share link
4.  Developing a python/jupyter notebook workflow
    1.  Open a jupyter notebook
    2.  Start "Manipulating Time Series Data in Python" DataCamp course, to be completed by Tue 9/19 before class
    3.  Screencast recording [here](https://drive.google.com/file/d/1kHVmegmsxClToWpUjak6s2cwcRZJ8vJ8/view?usp=sharing)

------------------------------------------------------------------------

# Lec 1.2: Thu 9/7

Install software and computing: See syllabus
