---
title: "SDS390: Ecological Forecasting"
---

<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQw8rrRUpbMRdy01pwUDw4C65XK1ekgQZWUeWN66L5KrAwtXuWMuo1EEk-2qULxsYPforSIAIsEI1dY/pubhtml?gid=538580312&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="800">

</iframe>

```{=html}
<!--
Things to do next time:
1. Do second DataCamp class after you've covered AR and MA process in class
1. 
-->
```


# Lec 7.1: Tue 10/17

## Announcements

1.  Discuss PS02 in-class presentation format on Tue 10/24 using PS01 feedback from [slido](https://admin.sli.do/event/3x3vyqpgVxaVxm6XTJLxxF/analytics)



------------------------------------------------------------------------



# Lec 6.2: Thu 10/12

## Announcements

1.  Still missing a few [PS01 peer evalutions](https://docs.google.com/forms/d/e/1FAIpQLSe6uHGVCgcHwBae-k00k6GXxmWKAhouNWivH-3NSim020bgww/viewform)
2.  Discuss slido [responses](https://admin.sli.do/event/3x3vyqpgVxaVxm6XTJLxxF/analytics)

## Lecture

1.  "Time Series Analysis in Python" DataCamp course Chapters 3-5 of this course focused on "autoregressive integrated moving average" (ARIMA) models. This is covered in:
    1.  [FPP Chapter 9](https://otexts.com/fpp3/arima.html)
    2.  The 4th of 5 courses in "Time Series with Python" DC skill track (see schedule above).
2.  Chalk talk
3.  Start PS02 (on new "Problem Sets" tab of webpage)

```{=html}
<!--
1. Chapter 1, Video 2, Exercise 1: Sometimes it doesn't make sense to study TS original values, but instead differences or % changes or returns of values. For example stock returns.
1. Chapter 2, Video 1, Exercise 2: If for a lag value of $k$ the autocorrelation function is statistically indistinguishable from 0, then there is no information for forecasting.
1. Chapter 3, Video 1, Exercise 1 and Video 2, Exercise 1: Purpose of a simulation study
1. Chapter 3, Video 4, Exercise 2: Model selection
1. Chapter 4, Video 2, Exercise 2: in-sample and out-of-sample forecasting
-->
```



------------------------------------------------------------------------

# Lec 5.2: Thu 10/5

## Announcements

1.  

## Lecture

1. Problem Set 01 in-class presentations

------------------------------------------------------------------------

# Lec 5.1: Tue 10/3

## Announcements

1.  PS1 mini-presentations on Thursday
2.  PS1 submission format posted [below](#problem-set-1)

## Lecture

1.  Lec 4.2 on FPP 3.4 Classical Decomposition, second attempt
    1.  What is $m$ used in example?
    2.  How is seasonal component $S_t$ computed? What is assumed seasonality?
    3.  Code over code block below
2.  FFP 3.5 Briefly discuss other decomposition methods used
3.  FFP 3.6 STL decomposition which uses LOESS = LOcal regrESSion smoothing instead of $m$-MA smoothing like for classical decomposition ![LOESS](https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/_images/loess.gif)

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1: Recreate Fig 3.13 ----
# 1.a) Get data
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

# Note index is 1M = monthly. This sets m=12 in m-MA method below
us_retail_employment

# 1.b) Recreate Fig 3.13
us_retail_employment %>%
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")



# Code block 2: Recreate all 4 subplots in Fig 3.13 ----
# Get data frame with full breakdown of decomposition
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() %>% 
  # Convert from tsibble to regular tibble for data wrangling
  as_tibble()

# 2.a) Top plot: Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# 2.b) 2nd plot: trend-cycle via MA average method
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# 2.c) Extra: detrended plot
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Detrended data Original data minus trend-cycle component") +
  geom_hline(yintercept = 0, col = "red")

# 2.d) 3rd plot: seasonality (values repeat every 12 months)
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "Seasonality")

# 2.e) 4th plot: Compute remainders
ggplot(full_decomposition, aes(x=Month, y = random)) +
  geom_line() +
  labs(title = "Remainder i.e. noise") +
  geom_hline(yintercept = 0, col = "red")



# Code block 3:  Compute full breakdown for one row: 1990 July ----
# 3.a) LOOK at data
# IMO most important function in RStudio: View()
# Focus on Row 7 1990 July: where do all these values come from?
View(full_decomposition)

# 3.b) Step 1: Compute T_hat_t = trend = 13177.76 using 2x12-MA
# Compute mean of first 12 values
full_decomposition %>% 
  slice(1:12) %>% 
  summarize(mean = mean(Employed))
# Compute mean of next 12 values
full_decomposition %>% 
  slice(2:13) %>% 
  summarize(mean = mean(Employed))
# Now do second averaging of 2 values
(13186 + 13170)/2

# 3.c) Step 2: Compute detrended values D_hat_t = -7.6625
full_decomposition <- full_decomposition %>% 
  mutate(detrended = Employed - trend)

# 3.d) Step 3: Compute seasonal averages S_hat_t = -13.311661
full_decomposition %>% 
  mutate(month_num = month(Month)) %>% 
  group_by(month_num) %>% 
  summarize(St = mean(detrended, na.rm = TRUE))

# 3.e) Step 4: Compute remainder R_hat_t = 5.6491610
# y_t - T_hat_t - S_hat_t
13170.1 - 13177.76 - (-13.311661)
```

------------------------------------------------------------------------

# Lec 4.2: Thu 9/28

## Announcements

1.  Comment on the generalizability of everything I say

## Lecture

1.  

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Modified version of code to produce FPP Fig 3.10
aus_exports <- global_economy |>
  filter(Country == "Australia") 

# Note number of rows
aus_exports

# Set m and plot
m <- 28
aus_exports |>
  mutate(
    `m-MA` = slider::slide_dbl(Exports, mean,
                .before = m, .after = m, .complete = TRUE)
  ) |>
  autoplot(Exports) +
  geom_line(aes(y = `m-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") +
  guides(colour = guide_legend(title = "series"))


# Code block 2 ----
# Classical decomposition breakdown

# Plot TS data in questions
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
us_retail_employment

# Code to create full Fig 3.13
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

# Get data frame with all the decomposition parts
full_decomposition <- us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components()

full_decomposition

# Original TS data
ggplot(full_decomposition, aes(x=Month, y = Employed)) +
  geom_line() +
  labs(title = "Original data")

# Step 1: trend-cycle via MA average method. 2nd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = trend)) +
  geom_line() +
  labs(title = "trend-cycle component")

# Step 2: Subtract trend-cycle from original data
ggplot(full_decomposition, aes(x=Month, y = Employed - trend)) +
  geom_line() +
  labs(title = "Original data minus trend-cycle component")

# Step 3: Compute seasonal averages. 3rd plot of Fig. 3.13
ggplot(full_decomposition, aes(x=Month, y = seasonal)) +
  geom_line() +
  labs(title = "For each season compute average")

# Step 4: Compute remainders. 4th plot of Fig 3.13
ggplot(full_decomposition, aes(x=Month, y = Employed - trend - seasonal)) +
  geom_line() +
  labs(title = "Remainder i.e. noise")
```

------------------------------------------------------------------------

# Lec 4.1: Tue 9/26

## Announcements

1.  The next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due ~~Tue 10/3~~ Thu 10/5 9:25am.
2.  I'm keeping up with screencasts, still need to finish Chapter 4.
3.  Slido [responses](https://admin.sli.do/event/hF9K8zvE9KGPCwexfM1YJu/analytics):
    1.  "I wish the examples were a bit more grounded, as in, the datasets we used were a topic I found interesting. It keeps feeling like I'm doing a small portion of a data analysis. I find myself"going through the motions" and feeling it is tedious because I don't think I'm really comprehending the importance of each step."
    2.  "Making stupid mistakes on the syntax and kind of confused about the difference between `[]` and `.` when calling an attribute"
    3.  "Worried I won't retain my understanding"
    4.  "I prefer this over problems sets because it gives an instant response and allow me to improve my work before finally submitting it"
4.  Main tip: "Optimal frustration"

## Lecture

1.  FPP 3.1 Transformations. $\log10$ transformations:
    1.  What are $\log$ (base $e$) and $\log10$ (base 10) tranformations? [Example table](https://moderndive.com/A-appendixA.html#appendix-log10-transformations)
    2.  Effect on visualizations: [Example figure](https://moderndive.com/11-thinking-with-data.html#fig:log10-price-viz)
2.  FPP 3.2 Time series decompositions
3. Problem Set 1 posted

## Problem Set 1 {#problem-set-1}



------------------------------------------------------------------------

# Lec 3.2: Thu 9/21

## Announcements

1.  First problem set assigned on Tue, which will build into first mini-presentation
2.  Mountain Day recap
3.  Originally assigned course ["Manipulating Time Series Data in Python"](https://app.datacamp.com/learn/courses/manipulating-time-series-data-in-python) due next Tue 9/26 before class
4.  Go to Roster Google Sheet (top right of page) and fill DC columns

## Lecture

1.  DataCamp
    1.  Poll class on [sli.do](https://app.sli.do/event/hF9K8zvE9KGPCwexfM1YJu) about DC
    2.  DC exercise numbering system: Ex 2.1.3 = Chapter 2, Video 1, Exercise 3.
    3.  Prof. Kim gets vulnerable and does MTSD course Ex 1.3.1 and 2.1.3
    4.  Screencasts location
    5.  Continuing [Time Series with Python](https://app.datacamp.com/learn/skill-tracks/time-series-with-python) skill track, the next course ["Time Series Analysis in Python"](https://app.datacamp.com/learn/courses/time-series-analysis-in-python) due Tue 10/3. If there is an Exercise you'd like me to do in class, let me know.
2.  Finish chalk talk on FPP Chapter 2: 2.7 and 2.8. See code below

```{r eval=FALSE, echo=TRUE}
library(fpp3)

# Code block 1 ----
# Lag plots: relationship of a TS variable with itself in the past
# Create regular TS plot of data in Fig 2.19 Beer production over time
recent_production <- aus_production |>
  filter(year(Quarter) >= 2000)

# Note time index meta-data = 1Q = quarter
recent_production

# Note patterns
recent_production |> autoplot(Beer)
```

------------------------------------------------------------------------

# Lec 2.2: Thu 9/14

-   Slido results from [last time](https://analytics.slido.com/share/XIJcW_05mEyupBJ2tuIT0cTYhCVNaSKt_S6IXFz9P_85KW39KIJcX_0zn_0tn_0sn_04mEwpT_cBSQUZ7NfeRoClIynj_ST2bmxub3FsZXFubP0vmHyIUAu_SVrj9mYafEeNA6EIvbcFZweiMI4O1A7kdCS5bg)
-   ChatGPT result on global temperature
-   Install `fpp3` R package
-   Chalk talk on FPP Chapter 2: Time Series Graphics. During chalk talk I will refer to "code blocks" below. Feel free to copy to a `playground.qmd` file.
-   Canada wins gold medal in ice hockey in overtime at 2010 Vancouver Winter Olympics
    -   The [Golden Goal 🥇](https://www.youtube.com/watch?v=GBMriA6maIU&t=18s)
    -   A clear [cyclical pattern](https://www.zdnet.com/a/img/resize/51e841b3331a3340a2b95219ddc1a7826fcaee31/2013/11/13/e384930f-4c5f-11e3-90a0-0291187ef9b6/epcor_edmonton_water_usage_flush_olympic_gold_game.jpg?auto=webp&fit=crop&height=1200&width=1200) in water consumption (from 🚽🚾🧻)

```{r eval=FALSE, echo=TRUE}
# Lec 2.2 Code
library(fpp3)

# Code block 1 ----
# Compare meta-data of data_tibble and data_tsibble
data_tibble <- tibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110)
)
data_tibble

# Set variable that indexes the data: Year
data_tsibble <- tsibble(data_tibble, index = "Year")
data_tsibble


# Code block 2 ----
# Scatterplot of relationship between two time series
# Code for Fig. 2.14:
vic_elec |>
  filter(year(Time) == 2014) |>
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")

# Code for Fig 2.14 with (simplified) notion of time
vic_elec |>
  filter(year(Time) == 2014) |>
  # Add this:
  mutate(month = factor(month(Date))) |>
  # Add color scale
  ggplot(aes(x = Temperature, y = Demand, col = month)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")
```

------------------------------------------------------------------------

# Lec 2.1: Tue 9/12

1.  Join `#questions` channel on Slack
2.  Fill in your information on roster
3.  ChatGPT citation mechanism: share link
4.  Developing a python/jupyter notebook workflow
    1.  Open a jupyter notebook
    2.  Start "Manipulating Time Series Data in Python" DataCamp course, to be completed by Tue 9/19 before class
    3.  Screencast recording [here](https://drive.google.com/file/d/1kHVmegmsxClToWpUjak6s2cwcRZJ8vJ8/view?usp=sharing)

------------------------------------------------------------------------

# Lec 1.2: Thu 9/7

Install software and computing: See syllabus
